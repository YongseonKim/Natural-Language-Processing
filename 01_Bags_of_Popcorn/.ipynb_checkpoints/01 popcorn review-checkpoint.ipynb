{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Data preprocessing\\n2. word2vec\\n3. modeling\\n4. 평가\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "'''\n",
    "참고 URL\n",
    "- https://programmers.co.kr/learn/courses/21/lessons/1693\n",
    "- http://suanlab.com/assets/lectures/dpp/10.pdf\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "1. Data preprocessing\n",
    "2. word2vec\n",
    "3. modeling\n",
    "4. 평가\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledTrainData.tsv',\n",
       " 'sampleSubmission.csv',\n",
       " 'testData.tsv',\n",
       " 'unlabeledTrainData.tsv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "# -- quoting : 특수 문자가 포함된 필드를 감쌀 때 처리하는 방법, 문자를 따옴표로 묶는 방법\n",
    "import csv\n",
    "df = pd.read_csv('data/labeledTrainData.tsv', header=0, delimiter='\\t',quoting=3)\n",
    "# QUOTE_MINIMAL (0), QUOTE_ALL (1), \n",
    "# QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
    "'''\n",
    "-QUOTE_ALL(1) : Quote everything, regardless of type.(문자열처리, 모든데이터 묶음)\n",
    "-QUOTE_MINIMAL(0) :Quote fields with special characters (특수 문자가 포함된 따옴표 필드)\n",
    "(anything that would confuse a parser configured with the same dialect and options). This is the default\n",
    "-QUOTE_NONNUMERIC(2) :Quote all fields that are not integers or floats.(숫자가 아닌 경우 묶음) \n",
    "  When used with the reader, input fields that are not quoted are converted to floats.\n",
    "-QUOTE_NONE(3) : Do not quote anything on output. 데이터를 묶지 않음\n",
    " When used with the reader, quote characters are included in the field values (normally, they are treated as delimiters and stripped).\n",
    " reader와 사용하면 쌍따옴표는 필드값으로 포함된다.\n",
    "'''\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 -- HTML 태그, \\ 특수 문자 제거\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def preprocessing(x):\n",
    "    #HTML 태그 제거\n",
    "    x= BeautifulSoup(x,'html.parser').get_text()\n",
    "    # 특수문자 제거 # 영문자,숫자를 제외한 문자를 모드 변환 띄어쓰기로\n",
    "    x = re.sub(\"\\W\",\" \",x)    \n",
    "    return x\n",
    "\n",
    "df['review']=df['review'].map(lambda x: preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment ...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>The Classic War of the Worlds   by Timothy ...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager  Nicholas Bell...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised thi...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1   With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...   \n",
       "2  \"7759_3\"          0   The film starts with a manager  Nicholas Bell...   \n",
       "3  \"3630_4\"          0   It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \n",
       "0  [stuff, going, moment, mj, started, listening,...  \n",
       "1  [classic, war, worlds, timothy, hines, enterta...  \n",
       "2  [film, starts, manager, nicholas, bell, giving...  \n",
       "3  [must, assumed, praised, film, greatest, filme...  \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이징 + Stopwords 제거\n",
    "# 장점 : 노이즈를 줄일 수 있음, 단점 : 문장 구조 모델링시 정보 유실 발생\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenizing(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(words.lower())\n",
    "    words = [x for x in words if x not in stop_words]\n",
    "    return words\n",
    "\n",
    "df['words'] = df['review'].map(lambda x : tokenizing(x))\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Tokenizing 방법 리뷰\n",
    "1.  Split 함수\n",
    "2.  NLTK 활용 \n",
    "   - Tokenizing → Index로 벡터화 해야하는데 NLTK는 Tokenizing 까지만\n",
    "3.  keras.preprocessing 활용\n",
    "   - Keras는 Vector화 까지 가능, \n",
    "\n",
    "'''\n",
    "# https://inuplace.tistory.com/536\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['words']) # Fit\n",
    "df['vector'] = token.texts_to_sequences(df['words']) # vector화 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment ...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "      <td>[410, 71, 425, 8956, 511, 2484, 116, 54, 881, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>The Classic War of the Worlds   by Timothy ...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "      <td>[236, 207, 3086, 3611, 7239, 321, 2, 411, 155,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager  Nicholas Bell...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "      <td>[2, 388, 2854, 4457, 3780, 604, 2210, 18035, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised thi...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "      <td>[101, 4896, 5399, 2, 688, 670, 1272, 42, 215, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "      <td>[3409, 4193, 37747, 11135, 859, 2062, 13202, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1   With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...   \n",
       "2  \"7759_3\"          0   The film starts with a manager  Nicholas Bell...   \n",
       "3  \"3630_4\"          0   It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [stuff, going, moment, mj, started, listening,...   \n",
       "1  [classic, war, worlds, timothy, hines, enterta...   \n",
       "2  [film, starts, manager, nicholas, bell, giving...   \n",
       "3  [must, assumed, praised, film, greatest, filme...   \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...   \n",
       "\n",
       "                                              vector  \n",
       "0  [410, 71, 425, 8956, 511, 2484, 116, 54, 881, ...  \n",
       "1  [236, 207, 3086, 3611, 7239, 321, 2, 411, 155,...  \n",
       "2  [2, 388, 2854, 4457, 3780, 604, 2210, 18035, 5...  \n",
       "3  [101, 4896, 5399, 2, 688, 670, 1272, 42, 215, ...  \n",
       "4  [3409, 4193, 37747, 11135, 859, 2062, 13202, 1...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75789"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "vocab = token.word_index\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 단어수 4\n",
      "1사분위 64.0\n",
      "2사분위 90.0\n",
      "3사분위 148.0\n",
      "최대 단어수 1429\n"
     ]
    }
   ],
   "source": [
    "# 최대 문장길이 :: 3사분위에 해당하는 214개를 고정길이로 설정\n",
    "print(\"최소 단어수\", df['vector'].map(lambda x : len(x)).min())\n",
    "print(\"1사분위\", df['vector'].map(lambda x : len(x)).quantile(0.25))\n",
    "print(\"2사분위\",df['vector'].map(lambda x : len(x)).quantile(0.50))\n",
    "print(\"3사분위\",df['vector'].map(lambda x : len(x)).quantile(0.75))\n",
    "print(\"최대 단어수\",df['vector'].map(lambda x : len(x)).max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  116    54   881 ... 18947   320  1372]\n",
      " [  236   207  3086 ...     0     0     0]\n",
      " [ 4657 32515  3589 ...   707  1187  5398]\n",
      " ...\n",
      " [  118  3144    14 ...     0     0     0]\n",
      " [  831   644   521 ...     0     0     0]\n",
      " [  110     1   354 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding: 가변 길이 → 고정 길이\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_padding = 214\n",
    "\n",
    "X_train = pad_sequences(df['vector'],maxlen = max_padding, padding = 'post' )\n",
    "Y_train = df['sentiment']\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\\n2. TF-IDF (완료)\\n3. Countvectorizer -- TF를 의미하는 것\\n4. Word2vec(완료)\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector 화 \n",
    "'''\n",
    "1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\n",
    "2. TF-IDF (완료)\n",
    "3. Countvectorizer -- TF를 의미하는 것\n",
    "4. Word2vec(완료)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(25000, 5000)\n",
      "  (0, 2502)\t0.1028129880638737\n",
      "  (0, 1285)\t0.06783600784085628\n",
      "  (0, 4212)\t0.09798879691949967\n",
      "  (0, 4484)\t0.10513903894444403\n",
      "  (0, 3287)\t0.09207262080751873\n",
      "  (0, 2998)\t0.09979596337739731\n",
      "  (0, 4479)\t0.09655916967157495\n",
      "  (0, 3286)\t0.1049730103164135\n",
      "  (0, 37)\t0.09626873961802275\n",
      "  (0, 2776)\t0.09875448073816731\n",
      "  (0, 1638)\t0.17021192744889807\n",
      "  (0, 2448)\t0.10116830229528975\n",
      "  (0, 2825)\t0.171073173099871\n",
      "  (0, 2174)\t0.06334114094305214\n",
      "  (0, 4298)\t0.06275013728447221\n",
      "  (0, 1559)\t0.0683315613411591\n",
      "  (0, 1571)\t0.050147944199345866\n",
      "  (0, 1305)\t0.1025268801175609\n",
      "  (0, 815)\t0.10635206492339642\n",
      "  (0, 1204)\t0.056885940864640605\n",
      "  (0, 1280)\t0.03764404759175515\n",
      "  (0, 4304)\t0.07530453665252453\n",
      "  (0, 1875)\t0.06589300465590953\n",
      "  (0, 312)\t0.07042547866816848\n",
      "  (0, 3348)\t0.08579945561982709\n",
      "  :\t:\n",
      "  (0, 2299)\t0.08305401895465898\n",
      "  (0, 2022)\t0.160515236053926\n",
      "  (0, 2890)\t0.058087806097562915\n",
      "  (0, 2758)\t0.038533095865286734\n",
      "  (0, 1399)\t0.10653310209512738\n",
      "  (0, 950)\t0.1194356257446557\n",
      "  (0, 3606)\t0.05932910538681658\n",
      "  (0, 4490)\t0.05042535821906538\n",
      "  (0, 2027)\t0.09272701214548547\n",
      "  (0, 2303)\t0.09506771035938272\n",
      "  (0, 699)\t0.073285501336439\n",
      "  (0, 4796)\t0.049430832000672706\n",
      "  (0, 2431)\t0.0626265245897318\n",
      "  (0, 2824)\t0.11891941197539346\n",
      "  (0, 4829)\t0.09595715060955264\n",
      "  (0, 1259)\t0.07517543346217842\n",
      "  (0, 3162)\t0.07841337871168738\n",
      "  (0, 4833)\t0.04607899433847484\n",
      "  (0, 3045)\t0.09179315875001577\n",
      "  (0, 2645)\t0.09489067409146293\n",
      "  (0, 4202)\t0.06964196049374342\n",
      "  (0, 4725)\t0.07710619316689983\n",
      "  (0, 2927)\t0.06808186871502951\n",
      "  (0, 1923)\t0.1008249855090022\n",
      "  (0, 4294)\t0.06698089637298846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.08774747, 0.04602346, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.08424142, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# 입력이 텍스트여야함.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer = \"word\", sublinear_tf=True,\n",
    "                           ngram_range=(1,3), max_features=5000,stop_words = 'english')\n",
    "\n",
    "# min_df : 설정값보다 특정 토큰의 df(document Frequency)가 적으면 벡터화에서 제거\n",
    "# analyzer : word/char 2가지 : word는 단위 : 단어 / char : 단위 : char \n",
    "# sublinear_tf : term frequency에 대한 smoothing 여부\n",
    "# ngram_range = n-gram 의 범위 : 분석기에 의해 설정값을 사용하여 ngram자동 생성\n",
    "# max_features = 벡터의 최대 길이, \n",
    "\n",
    "tfidf_train = vectorizer.fit_transform(list(df['review']))\n",
    "tfidf_train\n",
    "print(type(tfidf_train))\n",
    "print(tfidf_train.shape)\n",
    "print(tfidf_train[0]) \n",
    "# 5000개의 단어 각각에 대한 tf-idf Weight를 의미함\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## TF \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000)\n",
    "\n",
    "count_train = vectorizer.fit_transform(list(df['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :  %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# Word2vec 입력은 단어로 표현된 리스트를 입력값으로 받음\n",
    "# n-gram으로 만들어서 넣을 수도 있지만 여기에서는 단순히 split만해서 넣는 것으로 함\n",
    "\n",
    "sentences = []\n",
    "for review in list(df['review']) :\n",
    "    sentences.append(review.split())\n",
    "\n",
    "# print(sentences[0])\n",
    "\n",
    "# 하이퍼파라미터\n",
    "num_features = 1000 # word2vec 특징 수\n",
    "min_word_count =20 \n",
    "num_workers = 6\n",
    "context =10 # Word2vec 수행을 위한 컨텍스트 윈도 크기\n",
    "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-13%EC%9D%BC%EC%B0%A8-word2vec-3c82ec870426\n",
    "downsampling = 1e-3 #Word2vec 빠른 학습을 위해 정답 단어 라벨에 대한 다운 샘플링, 보통 0.001이 좋은 성능\n",
    "#Downsampling of frequent words # 자주 나오는 단어에 대해서는 0.001 만큼 다운 샘플링하여 시간을 아낌\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-22 06:27:48,330 : INFO :  collecting all words and their counts\n",
      "2020-10-22 06:27:48,330 : INFO :  PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-22 06:27:48,654 : INFO :  PROGRESS: at sentence #10000, processed 2398843 words, keeping 64085 word types\n",
      "2020-10-22 06:27:48,997 : INFO :  PROGRESS: at sentence #20000, processed 4773958 words, keeping 86259 word types\n",
      "2020-10-22 06:27:49,169 : INFO :  collected 94969 word types from a corpus of 5953723 raw words and 25000 sentences\n",
      "2020-10-22 06:27:49,170 : INFO :  Loading a fresh vocabulary\n",
      "2020-10-22 06:27:49,223 : INFO :  effective_min_count=20 retains 14661 unique words (15% of original 94969, drops 80308)\n",
      "2020-10-22 06:27:49,224 : INFO :  effective_min_count=20 leaves 5672944 word corpus (95% of original 5953723, drops 280779)\n",
      "2020-10-22 06:27:49,262 : INFO :  deleting the raw counts dictionary of 94969 items\n",
      "2020-10-22 06:27:49,264 : INFO :  sample=0.001 downsamples 47 most-common words\n",
      "2020-10-22 06:27:49,264 : INFO :  downsampling leaves estimated 4294841 word corpus (75.7% of prior 5672944)\n",
      "2020-10-22 06:27:49,293 : INFO :  estimated required memory for 14661 words and 1000 dimensions: 124618500 bytes\n",
      "2020-10-22 06:27:49,293 : INFO :  resetting layer weights\n",
      "2020-10-22 06:27:51,868 : INFO :  training model with 6 workers on 14661 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-10-22 06:27:52,907 : INFO :  EPOCH 1 - PROGRESS: at 11.18% examples, 465294 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:27:53,909 : INFO :  EPOCH 1 - PROGRESS: at 22.92% examples, 489469 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:27:54,915 : INFO :  EPOCH 1 - PROGRESS: at 34.42% examples, 490079 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:27:55,914 : INFO :  EPOCH 1 - PROGRESS: at 46.15% examples, 493178 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:27:56,921 : INFO :  EPOCH 1 - PROGRESS: at 57.81% examples, 495456 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:27:57,950 : INFO :  EPOCH 1 - PROGRESS: at 69.73% examples, 495721 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:27:58,977 : INFO :  EPOCH 1 - PROGRESS: at 81.68% examples, 494756 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:27:59,999 : INFO :  EPOCH 1 - PROGRESS: at 93.86% examples, 496374 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:00,473 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:28:00,476 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:28:00,483 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:28:00,486 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:28:00,487 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:28:00,489 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:28:00,489 : INFO :  EPOCH - 1 : training on 5953723 raw words (4294032 effective words) took 8.6s, 498279 effective words/s\n",
      "2020-10-22 06:28:01,502 : INFO :  EPOCH 2 - PROGRESS: at 11.34% examples, 483966 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:02,511 : INFO :  EPOCH 2 - PROGRESS: at 22.92% examples, 493761 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:03,519 : INFO :  EPOCH 2 - PROGRESS: at 34.88% examples, 499541 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:04,521 : INFO :  EPOCH 2 - PROGRESS: at 46.81% examples, 502000 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:05,542 : INFO :  EPOCH 2 - PROGRESS: at 58.45% examples, 501285 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:06,567 : INFO :  EPOCH 2 - PROGRESS: at 70.78% examples, 503145 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:07,572 : INFO :  EPOCH 2 - PROGRESS: at 82.90% examples, 504659 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:08,578 : INFO :  EPOCH 2 - PROGRESS: at 94.96% examples, 505059 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:08,929 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:28:08,936 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:28:08,948 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:28:08,950 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:28:08,965 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:28:08,971 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:28:08,971 : INFO :  EPOCH - 2 : training on 5953723 raw words (4294931 effective words) took 8.5s, 506430 effective words/s\n",
      "2020-10-22 06:28:09,993 : INFO :  EPOCH 3 - PROGRESS: at 11.48% examples, 487239 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:11,011 : INFO :  EPOCH 3 - PROGRESS: at 22.92% examples, 490183 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:12,019 : INFO :  EPOCH 3 - PROGRESS: at 34.90% examples, 497379 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:13,027 : INFO :  EPOCH 3 - PROGRESS: at 46.65% examples, 497718 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:14,037 : INFO :  EPOCH 3 - PROGRESS: at 58.45% examples, 500315 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:15,054 : INFO :  EPOCH 3 - PROGRESS: at 70.58% examples, 501808 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:16,064 : INFO :  EPOCH 3 - PROGRESS: at 82.72% examples, 503058 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:17,076 : INFO :  EPOCH 3 - PROGRESS: at 94.47% examples, 501618 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:17,441 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:28:17,463 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:28:17,476 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:28:17,479 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:28:17,492 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:28:17,493 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:28:17,493 : INFO :  EPOCH - 3 : training on 5953723 raw words (4295110 effective words) took 8.5s, 504176 effective words/s\n",
      "2020-10-22 06:28:18,498 : INFO :  EPOCH 4 - PROGRESS: at 11.32% examples, 487463 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:19,500 : INFO :  EPOCH 4 - PROGRESS: at 23.24% examples, 505065 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:20,503 : INFO :  EPOCH 4 - PROGRESS: at 35.15% examples, 508046 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:21,511 : INFO :  EPOCH 4 - PROGRESS: at 47.14% examples, 507594 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:22,522 : INFO :  EPOCH 4 - PROGRESS: at 58.61% examples, 505271 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:23,547 : INFO :  EPOCH 4 - PROGRESS: at 70.78% examples, 505262 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:24,562 : INFO :  EPOCH 4 - PROGRESS: at 82.74% examples, 504809 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:25,563 : INFO :  EPOCH 4 - PROGRESS: at 94.96% examples, 506294 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:25,918 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:28:25,933 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:28:25,960 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:28:25,961 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:28:25,962 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:28:25,963 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:28:25,963 : INFO :  EPOCH - 4 : training on 5953723 raw words (4295170 effective words) took 8.5s, 507274 effective words/s\n",
      "2020-10-22 06:28:26,969 : INFO :  EPOCH 5 - PROGRESS: at 11.48% examples, 494320 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:27,972 : INFO :  EPOCH 5 - PROGRESS: at 22.75% examples, 494082 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:28,985 : INFO :  EPOCH 5 - PROGRESS: at 34.60% examples, 496652 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:29,986 : INFO :  EPOCH 5 - PROGRESS: at 46.16% examples, 496115 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:30,995 : INFO :  EPOCH 5 - PROGRESS: at 58.14% examples, 500522 words/s, in_qsize 11, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-22 06:28:32,022 : INFO :  EPOCH 5 - PROGRESS: at 70.24% examples, 501182 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:33,028 : INFO :  EPOCH 5 - PROGRESS: at 82.11% examples, 501006 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:34,031 : INFO :  EPOCH 5 - PROGRESS: at 94.15% examples, 502060 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:34,437 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:28:34,458 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:28:34,472 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:28:34,474 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:28:34,478 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:28:34,479 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:28:34,480 : INFO :  EPOCH - 5 : training on 5953723 raw words (4295561 effective words) took 8.5s, 504519 effective words/s\n",
      "2020-10-22 06:28:35,484 : INFO :  EPOCH 6 - PROGRESS: at 11.02% examples, 473596 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:36,509 : INFO :  EPOCH 6 - PROGRESS: at 22.56% examples, 485306 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:37,536 : INFO :  EPOCH 6 - PROGRESS: at 34.42% examples, 488516 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:38,550 : INFO :  EPOCH 6 - PROGRESS: at 46.15% examples, 490199 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:39,559 : INFO :  EPOCH 6 - PROGRESS: at 57.81% examples, 493102 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:40,561 : INFO :  EPOCH 6 - PROGRESS: at 70.08% examples, 498155 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:41,588 : INFO :  EPOCH 6 - PROGRESS: at 82.25% examples, 498910 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:42,600 : INFO :  EPOCH 6 - PROGRESS: at 94.30% examples, 499643 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:43,026 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:28:43,032 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:28:43,034 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:28:43,045 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:28:43,052 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:28:43,052 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:28:43,053 : INFO :  EPOCH - 6 : training on 5953723 raw words (4294905 effective words) took 8.6s, 501083 effective words/s\n",
      "2020-10-22 06:28:44,063 : INFO :  EPOCH 7 - PROGRESS: at 11.02% examples, 471470 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:45,064 : INFO :  EPOCH 7 - PROGRESS: at 22.56% examples, 489894 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:46,065 : INFO :  EPOCH 7 - PROGRESS: at 34.39% examples, 495837 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:47,092 : INFO :  EPOCH 7 - PROGRESS: at 46.49% examples, 497527 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:48,100 : INFO :  EPOCH 7 - PROGRESS: at 58.45% examples, 501746 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:49,103 : INFO :  EPOCH 7 - PROGRESS: at 70.24% examples, 501819 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:50,113 : INFO :  EPOCH 7 - PROGRESS: at 81.98% examples, 500232 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:51,124 : INFO :  EPOCH 7 - PROGRESS: at 94.16% examples, 501798 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:51,541 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:28:51,549 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:28:51,550 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:28:51,557 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:28:51,558 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:28:51,565 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:28:51,566 : INFO :  EPOCH - 7 : training on 5953723 raw words (4294392 effective words) took 8.5s, 504584 effective words/s\n",
      "2020-10-22 06:28:52,577 : INFO :  EPOCH 8 - PROGRESS: at 11.18% examples, 477569 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:53,587 : INFO :  EPOCH 8 - PROGRESS: at 22.75% examples, 490977 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:54,619 : INFO :  EPOCH 8 - PROGRESS: at 34.74% examples, 493983 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:55,654 : INFO :  EPOCH 8 - PROGRESS: at 46.81% examples, 495306 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:56,662 : INFO :  EPOCH 8 - PROGRESS: at 58.45% examples, 497194 words/s, in_qsize 10, out_qsize 1\n",
      "2020-10-22 06:28:57,676 : INFO :  EPOCH 8 - PROGRESS: at 70.58% examples, 499374 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:58,682 : INFO :  EPOCH 8 - PROGRESS: at 82.57% examples, 500349 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:28:59,690 : INFO :  EPOCH 8 - PROGRESS: at 94.63% examples, 501035 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:00,071 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:29:00,078 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:29:00,081 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:29:00,083 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:29:00,102 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:29:00,105 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:29:00,106 : INFO :  EPOCH - 8 : training on 5953723 raw words (4293795 effective words) took 8.5s, 502952 effective words/s\n",
      "2020-10-22 06:29:01,150 : INFO :  EPOCH 9 - PROGRESS: at 11.32% examples, 469013 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:02,175 : INFO :  EPOCH 9 - PROGRESS: at 23.24% examples, 489514 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:03,190 : INFO :  EPOCH 9 - PROGRESS: at 35.15% examples, 495662 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:04,192 : INFO :  EPOCH 9 - PROGRESS: at 47.14% examples, 498963 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:05,203 : INFO :  EPOCH 9 - PROGRESS: at 58.95% examples, 501187 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:06,206 : INFO :  EPOCH 9 - PROGRESS: at 70.58% examples, 500054 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:07,219 : INFO :  EPOCH 9 - PROGRESS: at 82.72% examples, 501441 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:08,223 : INFO :  EPOCH 9 - PROGRESS: at 94.96% examples, 503259 words/s, in_qsize 12, out_qsize 0\n",
      "2020-10-22 06:29:08,564 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:29:08,583 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:29:08,588 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:29:08,597 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:29:08,604 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:29:08,608 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:29:08,608 : INFO :  EPOCH - 9 : training on 5953723 raw words (4294585 effective words) took 8.5s, 505235 effective words/s\n",
      "2020-10-22 06:29:09,623 : INFO :  EPOCH 10 - PROGRESS: at 11.34% examples, 482886 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:10,650 : INFO :  EPOCH 10 - PROGRESS: at 23.04% examples, 492851 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:11,680 : INFO :  EPOCH 10 - PROGRESS: at 34.74% examples, 490889 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:12,683 : INFO :  EPOCH 10 - PROGRESS: at 46.81% examples, 496930 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:13,686 : INFO :  EPOCH 10 - PROGRESS: at 58.46% examples, 498910 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:14,693 : INFO :  EPOCH 10 - PROGRESS: at 70.25% examples, 499063 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:15,705 : INFO :  EPOCH 10 - PROGRESS: at 82.41% examples, 500750 words/s, in_qsize 11, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-22 06:29:16,711 : INFO :  EPOCH 10 - PROGRESS: at 94.30% examples, 500734 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-22 06:29:17,127 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-22 06:29:17,128 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-22 06:29:17,138 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-22 06:29:17,139 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-22 06:29:17,143 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-22 06:29:17,146 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-22 06:29:17,147 : INFO :  EPOCH - 10 : training on 5953723 raw words (4294792 effective words) took 8.5s, 503079 effective words/s\n",
      "2020-10-22 06:29:17,147 : INFO :  training on a 59537230 raw words (42947273 effective words) took 85.3s, 503609 effective words/s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# https://wikidocs.net/50739\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers = num_workers,\n",
    "                          size = num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window =  context,\n",
    "                          sample = downsampling,\n",
    "                          iter = 10,\n",
    "                          sg =0 # sg =0 CBOW, 1 : skip-gram\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-22 06:29:17,155 : INFO :  precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6223074197769165),\n",
       " ('lady', 0.5865253210067749),\n",
       " ('soldier', 0.5489656925201416),\n",
       " ('doctor', 0.5334686040878296),\n",
       " ('guy', 0.5075160264968872),\n",
       " ('businessman', 0.4959918260574341),\n",
       " ('priest', 0.4923453629016876),\n",
       " ('boy', 0.4903828501701355),\n",
       " ('farmer', 0.4762645959854126),\n",
       " ('scientist', 0.47407081723213196)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# man과 가장 유사한 단어 골라내기 \n",
    "model.wv.most_similar(\"man\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# word2vec 은 단어 하나하나가 벡터로 표현되어 있다.\n",
    "# Review 데이터는 단어들의 조합이기에 Review를 벡터로 표현하기 위해\n",
    "# Review에 포함된 단어 벡터들의 평균값을 만든다.\n",
    "# 다른 방법으로는 Doc2vec, average of word2vec vectors with TF-IDF\n",
    "# Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector.\n",
    " # 단어 벡터에 TF-IDF를 곱해서 평균 내는 방법\n",
    "    \n",
    "# https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence\n",
    "\n",
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype = np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘 사전\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words +=1\n",
    "            #사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            \n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            # model은 단어들에 대한 vector를 다 가지고 있음\n",
    "            # num_features 만큼 이미 학습할때 정의해서 만들어놓음 \n",
    "            \n",
    "    feature_vector = np.divide(feature_vector,num_words)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    \n",
    "    for s in reviews :\n",
    "        dataset.append(get_features(s,model,num_features))\n",
    "    \n",
    "    reviewFeaturevecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeaturevecs\n",
    "\n",
    "word2vec_train = get_dataset(sentences,model,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.84667248e-01, -1.08994767e-02, -1.13588035e-01,  1.17495187e-01,\n",
       "        2.56480053e-02,  8.04757476e-02, -1.05506063e-01,  1.20215714e-01,\n",
       "        1.46283405e-02,  1.04477229e-02,  4.12839418e-03, -1.64638460e-02,\n",
       "       -6.46725297e-02,  8.36449955e-03,  1.86226100e-01, -9.32694823e-02,\n",
       "       -1.03054129e-01,  2.75235176e-02,  1.43799812e-01, -1.12206146e-01,\n",
       "        1.62661135e-01,  9.93568376e-02, -2.88734213e-02,  5.48825972e-02,\n",
       "        1.35131255e-01,  1.42677546e-01, -1.08255811e-01,  6.69090673e-02,\n",
       "       -2.67270803e-01,  1.00531559e-02,  1.50428815e-02, -8.02390724e-02,\n",
       "        6.29948005e-02,  1.52281508e-01, -1.95384264e-01,  3.57935578e-01,\n",
       "        5.85520752e-02, -1.68395210e-02,  1.91368505e-01, -1.07441626e-01,\n",
       "       -7.34639242e-02, -7.73130506e-02,  1.45138931e-02,  4.77659442e-02,\n",
       "        1.24424910e-02,  6.51779473e-02,  2.80315448e-02, -1.30822554e-01,\n",
       "       -6.50603920e-02, -7.20745772e-02, -1.88913777e-01, -2.81763941e-01,\n",
       "       -3.56749929e-02,  3.23767364e-02, -1.07822731e-01,  2.22542495e-01,\n",
       "       -8.97190347e-03, -1.30144626e-01,  1.45544142e-01,  4.41491827e-02,\n",
       "        2.49017611e-01, -2.37817168e-01,  7.34700337e-02, -1.11323651e-02,\n",
       "       -2.15752661e-01, -5.06134070e-02, -5.10666370e-02,  1.91811621e-01,\n",
       "        6.62387386e-02,  2.99554877e-02,  2.36975942e-02, -8.01333133e-03,\n",
       "        1.60549674e-02,  1.59791857e-02, -1.71975464e-01,  1.93214417e-01,\n",
       "       -5.77831641e-03, -2.69567102e-01, -3.73627655e-02,  1.53750479e-01,\n",
       "       -1.14064291e-01,  1.90434847e-02, -1.77545957e-02, -7.00209364e-02,\n",
       "       -2.24051759e-01, -1.14036523e-01,  2.56254580e-02, -1.15213901e-01,\n",
       "        1.97314117e-02, -1.15840867e-01, -1.11550435e-01, -7.71890907e-03,\n",
       "       -1.15338014e-02, -7.48465583e-02,  1.41090900e-01, -3.33232060e-02,\n",
       "        4.78580713e-01, -2.99940050e-01,  1.81911942e-02,  3.62400971e-02,\n",
       "       -3.59188765e-01, -4.04121965e-01,  7.90601783e-03, -1.65960655e-01,\n",
       "       -1.32924644e-02, -1.64392754e-01, -2.65134513e-01,  6.20299950e-02,\n",
       "        2.67944098e-01,  4.44500595e-01, -3.24003369e-01,  2.25978971e-01,\n",
       "        1.27618313e-01,  1.96737498e-02,  3.77126515e-01, -8.81431222e-01,\n",
       "        1.06031178e-02,  1.16288245e-01,  1.02138391e-03,  1.31870866e-01,\n",
       "        2.63741892e-02, -2.03465015e-01, -1.37202814e-01, -4.18550801e-03,\n",
       "       -1.05467707e-01, -3.61469954e-01, -6.36444092e-02,  2.16048777e-01,\n",
       "       -1.16283923e-01, -2.01165169e-01,  2.97834277e-01, -1.77883223e-01,\n",
       "        2.51090955e-02, -8.65422711e-02,  3.09861209e-02, -1.95398077e-01,\n",
       "        4.96821143e-02,  6.31003175e-03, -7.12136701e-02, -1.19746432e-01,\n",
       "       -9.08130705e-02,  5.74363135e-02,  1.60544924e-02,  2.57722046e-02,\n",
       "       -8.35778043e-02, -2.21362442e-01,  1.16620928e-01,  2.48625562e-01,\n",
       "       -5.04001193e-02,  1.93233136e-02,  5.97891025e-02, -1.32534713e-01,\n",
       "       -1.42415032e-01, -2.25858055e-02, -5.67717990e-03, -1.25110019e-02,\n",
       "        5.91889769e-02,  2.04207227e-01, -1.75682809e-02,  5.95567636e-02,\n",
       "        1.64663911e-01, -1.81811959e-01,  1.80636585e-01,  1.09416072e-03,\n",
       "       -2.66306419e-02, -5.69330119e-02, -2.83780634e-01, -3.21985073e-02,\n",
       "        2.76661497e-02, -2.58811563e-02,  1.24791088e-02,  5.39624877e-02,\n",
       "        5.81936538e-02, -6.24720678e-02, -4.77919206e-02,  7.38308951e-02,\n",
       "       -1.59959435e-01,  9.10971314e-03,  1.87451288e-01,  5.62384948e-02,\n",
       "        8.35803151e-02, -6.65342137e-02, -6.43573552e-02, -2.01199781e-02,\n",
       "       -1.39243782e-01, -1.78821892e-01,  1.30168274e-01, -7.62826651e-02,\n",
       "        5.13740117e-03, -3.68710123e-02, -1.67098999e-01,  8.59286934e-02,\n",
       "        2.12649837e-01, -6.54968619e-02, -4.12837341e-02,  1.42897546e-01,\n",
       "       -1.70840602e-02,  2.79160559e-01,  2.20217124e-01,  3.07422966e-01,\n",
       "       -9.22463834e-04, -8.61271098e-03,  8.16281438e-02, -5.17601818e-02,\n",
       "       -1.26254320e-01, -1.92268088e-01, -6.46102056e-02, -6.95997030e-02,\n",
       "        7.27978572e-02,  1.22925490e-01, -1.07446723e-01, -4.78257835e-02,\n",
       "       -1.01188729e-02,  2.06068773e-02, -5.38295805e-02, -1.13425687e-01,\n",
       "        2.57521961e-02,  9.58208274e-03, -3.93330157e-02,  3.88622224e-01,\n",
       "       -3.13269854e-01, -4.96408530e-03,  4.45972621e-01,  9.57749560e-02,\n",
       "       -1.97317645e-01, -2.27982663e-02,  9.93094314e-03,  1.34441750e-02,\n",
       "       -6.74504638e-02, -2.41863325e-01, -3.97314429e-01,  2.61046380e-01,\n",
       "        1.36113495e-01, -2.30697781e-01, -1.52126923e-02,  3.79648149e-01,\n",
       "        3.31439584e-01,  1.10417366e-01,  2.41159156e-01,  1.12036124e-01,\n",
       "       -2.14693472e-01,  1.76291123e-01, -5.19767880e-01,  2.37666160e-01,\n",
       "        1.17935548e-02,  2.66572326e-01,  1.11213677e-01, -1.07990474e-01,\n",
       "        1.04652151e-01, -2.47413721e-02,  4.72927876e-02, -4.34323251e-02,\n",
       "        1.17278360e-01, -1.22036561e-01,  8.72418582e-02, -7.51456246e-02,\n",
       "       -1.40097857e-01, -2.90419813e-03,  8.26119408e-02, -1.05494075e-01,\n",
       "       -3.66264343e-01, -2.54120946e-01, -1.37484595e-01,  1.16017843e-02,\n",
       "       -1.28848329e-01,  7.65315350e-03,  7.90054575e-02,  6.34972602e-02,\n",
       "       -1.72495767e-02, -7.84036890e-02,  2.35537402e-02,  1.40092343e-01,\n",
       "        9.97654647e-02, -5.47222421e-02,  1.33104324e-01, -1.44989640e-01,\n",
       "       -2.44828165e-02,  1.43267334e-01,  6.71463385e-02, -7.67949224e-02,\n",
       "       -7.66162900e-03, -1.19461074e-01,  5.24509959e-02, -1.99234754e-01,\n",
       "       -4.49107178e-02, -1.42564416e-01,  3.14814039e-02,  8.40442479e-02,\n",
       "       -2.22209141e-01,  1.97013747e-02, -6.30903095e-02, -2.51754045e-01,\n",
       "       -8.01856443e-02,  5.76192327e-02, -2.18533546e-01,  2.53601193e-01,\n",
       "       -4.34050262e-02, -5.58217093e-02, -6.44611493e-02, -3.11171133e-02,\n",
       "        2.61336118e-02, -3.06713842e-02,  1.72395725e-03, -7.40887821e-02,\n",
       "       -2.36935578e-02,  1.32093698e-01,  6.96810707e-02, -2.80799456e-02,\n",
       "        1.31397806e-02, -1.81074202e-01, -1.24600790e-02,  2.53651924e-02,\n",
       "        1.84155747e-01, -9.44208503e-02, -1.16702188e-02, -1.66961667e-03,\n",
       "        3.07615697e-02, -1.42303593e-02, -2.60450989e-02,  6.28594607e-02,\n",
       "       -1.64428294e-01, -1.18335776e-01,  1.20708093e-01, -5.64625524e-02,\n",
       "        1.07623590e-02, -4.78596287e-03,  5.31508997e-02, -2.24440489e-02,\n",
       "       -2.19043180e-01, -7.10381020e-04, -3.45790505e-01, -1.62977919e-01,\n",
       "        3.46243948e-01, -2.40358692e-02, -1.01842977e-01,  9.99695249e-03,\n",
       "        2.49490649e-01, -3.99950653e-01,  3.28466713e-01, -1.32768210e-02,\n",
       "       -1.19238511e-01, -1.86125338e-01,  5.78804640e-03, -1.22727647e-01,\n",
       "       -1.03677604e-02, -2.67742276e-01, -1.62933946e-01,  9.56526175e-02,\n",
       "       -7.34064057e-02, -1.11001283e-01, -2.29589436e-02,  1.98178723e-01,\n",
       "        5.40189147e-02, -3.02978177e-02,  2.28657633e-01, -4.95715113e-03,\n",
       "       -3.00975680e-01,  3.02885622e-01, -1.05371557e-01,  4.80072349e-01,\n",
       "        2.18091711e-01, -1.09274171e-01, -2.27137879e-01, -1.74188279e-02,\n",
       "       -1.59164533e-01, -1.87069952e-01, -1.52092338e-01,  9.43763740e-03,\n",
       "        1.02041341e-01,  8.03293288e-02,  3.56418267e-03, -1.55564129e-01,\n",
       "       -6.60654083e-02,  7.32451007e-02,  8.60900581e-02, -4.13318098e-01,\n",
       "        6.27974123e-02, -6.59854189e-02,  6.69119554e-03, -1.52999740e-02,\n",
       "        2.03880161e-01, -7.85142258e-02, -1.37293972e-02, -5.51290549e-02,\n",
       "        1.32108822e-01, -1.46831587e-01, -3.73454630e-01, -2.20347837e-01,\n",
       "        1.33321524e-01,  1.02242857e-01,  1.82417974e-01,  1.19994888e-02,\n",
       "       -8.20412040e-02, -3.65260765e-02, -3.04247178e-02,  3.64492722e-02,\n",
       "       -1.63004115e-01, -2.78962702e-01,  6.19444624e-02,  1.50567979e-01,\n",
       "        3.38957235e-02,  2.97954857e-01, -5.47287241e-02, -3.96341905e-02,\n",
       "        1.12078220e-01, -5.01803532e-02,  4.49371934e-02,  1.88477546e-01,\n",
       "       -1.08301163e-01, -1.21125378e-01, -1.14765894e-02, -1.76274642e-01,\n",
       "       -1.98639944e-01, -1.41570777e-01, -1.40050471e-01,  3.99468355e-02,\n",
       "        2.92636808e-02,  1.39076039e-01, -2.41714373e-01,  8.96887481e-02,\n",
       "        1.10111326e-01,  8.75694156e-02,  7.94350058e-02, -4.20829542e-02,\n",
       "       -2.45623235e-02, -2.23505348e-02, -1.35014266e-01,  2.40808609e-03,\n",
       "        1.83173940e-02,  1.75768696e-02,  1.41856009e-02, -1.46952778e-01,\n",
       "       -1.18452772e-01, -6.15554454e-04, -1.12861589e-01, -7.73556083e-02,\n",
       "       -8.49358924e-03, -1.06573462e-01,  1.58843920e-02, -1.00257844e-01,\n",
       "        1.24550335e-01,  9.24223810e-02,  5.58131635e-02, -1.59422383e-02,\n",
       "       -1.13232071e-02,  8.18874091e-02, -7.78898001e-02,  4.96752001e-02,\n",
       "        8.13697726e-02, -2.32761398e-01,  8.79211426e-02,  4.67516109e-02,\n",
       "        2.78185070e-01,  6.90507144e-02,  9.25633833e-02, -7.49692097e-02,\n",
       "        1.41607478e-01, -1.45775348e-01,  7.35958740e-02,  7.04140589e-02,\n",
       "        3.18404892e-03, -1.92156821e-01,  1.23832062e-01, -6.96164221e-02,\n",
       "        5.04750833e-02, -1.49924597e-02, -1.31443050e-02,  1.38351277e-01,\n",
       "        1.30253837e-01, -1.27248973e-01, -4.32725437e-02, -1.75112441e-01,\n",
       "       -5.60009032e-02,  6.04754314e-02,  4.57695872e-02,  8.70861635e-02,\n",
       "        9.85485315e-02,  7.92463422e-02,  4.39570770e-02, -8.75445828e-03,\n",
       "        7.17017278e-02,  4.67726588e-02, -2.92969584e-01, -2.21852198e-01,\n",
       "       -3.29566121e-01, -3.74650151e-01, -1.13997780e-01,  3.21721703e-01,\n",
       "        1.04052842e-01,  1.63944177e-02,  5.42293116e-02, -7.72196427e-02,\n",
       "       -5.04987314e-02,  1.67097092e-01,  2.17452601e-01,  1.57890692e-01,\n",
       "        1.64730132e-01, -1.94519415e-01,  1.72288120e-02, -4.39945757e-02,\n",
       "       -1.33914798e-01, -6.88718334e-02, -1.83739513e-02, -7.95741081e-02,\n",
       "        6.65685833e-02, -7.70226419e-02,  5.31009287e-02, -1.15005389e-01,\n",
       "        1.56974420e-01, -1.92992464e-01, -1.27460763e-01, -2.96317264e-02,\n",
       "       -1.10531531e-01, -1.98843598e-01,  5.00287712e-02,  1.73671711e-02,\n",
       "       -3.63176242e-02, -2.04133302e-01,  2.54254162e-01, -5.61878085e-02,\n",
       "       -1.95887759e-02, -5.63311726e-02,  1.69203654e-02, -1.15985312e-02,\n",
       "        3.09231747e-02,  1.81639463e-01,  7.06479922e-02, -1.01126142e-01,\n",
       "        1.34876326e-01,  6.89135119e-02, -7.66288936e-02, -3.36204916e-02,\n",
       "       -4.45472300e-02, -2.16737092e-02,  2.38915570e-02, -7.15426654e-02,\n",
       "        1.17189400e-01,  6.93580732e-02,  1.60481602e-01,  2.93397695e-01,\n",
       "       -1.86830878e-01,  1.48986802e-01, -8.69145524e-03, -7.37105384e-02,\n",
       "        1.45423830e-01, -3.67151611e-02,  8.80848616e-03,  5.54907084e-01,\n",
       "       -2.27205172e-01, -1.76129594e-01, -1.14934541e-01,  1.32412270e-01,\n",
       "       -1.10214306e-02,  2.02251539e-01, -8.99872929e-02,  1.02939822e-01,\n",
       "        5.14155291e-02,  4.68886830e-02, -5.66818304e-02, -9.53960046e-02,\n",
       "        4.34160493e-02, -3.07366606e-02, -1.99929010e-02,  1.21866956e-01,\n",
       "       -8.61447155e-02, -9.97190401e-02,  9.22353640e-02, -9.51063707e-02,\n",
       "        3.61469239e-02, -4.90290672e-02,  2.79828515e-02, -8.67913850e-03,\n",
       "       -4.91024926e-02, -2.18496006e-02, -3.50297196e-03,  2.77142767e-02,\n",
       "        1.14739323e-02,  3.24444026e-01, -1.73231333e-01,  1.60809800e-01,\n",
       "        2.57577151e-01,  6.58987090e-02, -1.94444343e-01, -3.87131907e-02,\n",
       "       -2.20944546e-02, -1.81515738e-02,  6.50725514e-02,  2.07444251e-01,\n",
       "       -1.70605946e-02,  1.49160787e-01, -1.51514830e-02,  1.47822559e-01,\n",
       "       -3.93140465e-02,  3.50428633e-02,  6.37786537e-02, -6.82322634e-03,\n",
       "       -3.27047296e-02,  4.49129473e-03,  9.88927856e-02,  2.65549665e-04,\n",
       "       -6.93695294e-03, -1.79951295e-01, -3.66732255e-02,  1.11553289e-01,\n",
       "        1.39218671e-02, -3.10890423e-03,  1.19830795e-01, -1.15068160e-01,\n",
       "       -2.70135142e-02, -9.95833892e-04,  5.72736114e-02, -5.26539460e-02,\n",
       "        2.63468064e-02,  1.92339823e-01, -1.05120189e-01,  6.10215142e-02,\n",
       "       -1.88414216e-01, -4.30950113e-02,  2.54736301e-02,  8.26789588e-02,\n",
       "       -5.55477105e-02, -1.04049277e-02, -9.58112851e-02, -7.54729733e-02,\n",
       "        5.16359545e-02,  8.37650001e-02,  6.99695991e-03, -1.19137838e-01,\n",
       "        1.53699471e-02, -1.19549610e-01,  4.66269217e-02,  8.25314224e-03,\n",
       "       -1.03178769e-01, -8.34669173e-02, -6.47990331e-02, -4.74792309e-02,\n",
       "       -2.69935671e-02,  1.04645565e-01, -1.20437346e-01, -1.18871912e-01,\n",
       "       -1.51488274e-01,  1.89704653e-02,  2.51795910e-02, -6.71901330e-02,\n",
       "        2.16035433e-02, -6.74399212e-02, -3.24547701e-02,  1.22853972e-01,\n",
       "       -2.31940538e-01,  1.65657490e-01,  9.30362940e-02, -7.22998157e-02,\n",
       "        3.45568985e-01, -8.42212364e-02, -6.56273663e-02,  5.98036945e-02,\n",
       "       -1.60885602e-01,  2.73733716e-02,  4.10563648e-02,  9.33478586e-03,\n",
       "       -1.94441415e-02,  2.19547734e-01, -2.06244290e-02, -1.01856321e-01,\n",
       "       -2.04372242e-01,  1.57895625e-01, -9.13717076e-02,  6.17250800e-02,\n",
       "       -1.73240043e-02, -2.15630814e-01, -1.36266321e-01, -1.88966244e-01,\n",
       "       -2.99733412e-02, -3.28965709e-02,  3.33982185e-02,  8.04092214e-02,\n",
       "       -2.68512890e-02, -2.74674650e-02,  1.84655432e-02,  4.80628088e-02,\n",
       "       -2.05880597e-01, -7.10878000e-02,  1.85164034e-01,  7.57152215e-02,\n",
       "       -1.16863199e-01,  3.65949571e-02,  1.33387893e-01, -1.66628569e-01,\n",
       "       -3.07566613e-01,  3.90258571e-03,  3.96374166e-01, -4.20360081e-02,\n",
       "       -8.37943889e-03, -2.81371810e-02, -8.70595574e-02, -2.28247896e-01,\n",
       "        7.26362467e-02,  5.25084138e-02,  5.45668080e-02, -3.93405147e-02,\n",
       "       -2.06744939e-01,  1.46910930e-02,  5.52454814e-02,  2.06350088e-01,\n",
       "       -1.34469524e-01,  1.50913164e-01,  9.56536904e-02,  1.31466994e-02,\n",
       "        9.30456817e-02,  2.19971493e-01, -2.50408113e-01,  2.00371984e-02,\n",
       "        9.40394700e-02, -7.58565888e-02,  2.32604500e-02, -1.27724245e-01,\n",
       "        6.66833529e-03, -3.49120766e-01,  1.25738174e-01, -4.47079018e-02,\n",
       "        1.04160771e-01, -1.43240809e-01,  1.06501177e-01,  8.62841085e-02,\n",
       "       -5.47108017e-02, -2.26523235e-01,  5.71237085e-03,  5.37196584e-02,\n",
       "        6.34377589e-03,  3.22688706e-02,  3.89627703e-02,  5.12907133e-02,\n",
       "        1.78443223e-01, -3.60137410e-02, -1.04546666e-01,  9.14954767e-02,\n",
       "        7.26808142e-03, -3.00119892e-02,  8.97248916e-04,  1.47074372e-01,\n",
       "        9.81638357e-02, -8.94371644e-02,  4.61756065e-02,  4.79132421e-02,\n",
       "        1.47973388e-01,  1.35319298e-02,  1.31479025e-01, -2.40503196e-02,\n",
       "        7.44983628e-02, -6.57027736e-02,  2.62319949e-02,  3.20468359e-02,\n",
       "        5.86467609e-02, -3.83081706e-03, -8.08763131e-03,  1.87515598e-02,\n",
       "        6.30791485e-02, -1.47986016e-03,  6.87804297e-02, -2.73189396e-02,\n",
       "        3.58081274e-02, -1.70599204e-02,  4.22996953e-02,  1.08265780e-01,\n",
       "       -3.46058048e-02,  3.28096040e-02, -4.79873046e-02,  3.10311075e-02,\n",
       "       -1.54701993e-01,  7.45941624e-02, -1.19490679e-02,  3.52523290e-02,\n",
       "       -5.68090938e-02, -2.06962675e-02, -4.16831113e-03,  7.17800707e-02,\n",
       "       -3.09125185e-02,  1.94252193e-01,  8.15056041e-02, -3.34884301e-02,\n",
       "        1.67921174e-03, -1.47930101e-01,  1.17808118e-01, -1.69875473e-01,\n",
       "       -3.50046940e-02,  9.09301639e-02,  6.17423505e-02, -9.68796611e-02,\n",
       "       -1.80097565e-01, -1.67181209e-01,  2.21463814e-01, -1.35960132e-01,\n",
       "       -3.69543314e-01,  2.84222122e-02, -8.18266720e-02, -2.97069061e-03,\n",
       "       -8.42670649e-02,  4.93464321e-02,  6.26597032e-02, -1.66827217e-01,\n",
       "       -1.69944078e-01, -2.00448111e-01,  9.32654273e-03,  9.47270766e-02,\n",
       "        7.80306160e-02, -1.10107670e-02, -8.25662613e-02,  2.76075173e-02,\n",
       "        4.39256802e-02, -4.37953398e-02, -6.43514618e-02,  2.20605973e-02,\n",
       "       -1.78215764e-02, -2.23660916e-02,  6.46914244e-02,  8.23793039e-02,\n",
       "       -1.13021463e-01, -2.08802119e-01, -1.39253184e-01, -1.87833175e-01,\n",
       "        7.55170360e-02, -3.69868040e-01,  5.20997867e-02, -1.02995694e-01,\n",
       "        3.54827464e-01, -2.18657568e-01, -2.31494799e-01, -2.70612925e-01,\n",
       "       -2.30454966e-01,  2.28710875e-01, -7.90506229e-02,  3.62446129e-01,\n",
       "        1.93181247e-01,  2.42253467e-01, -4.55509797e-02, -1.05485748e-02,\n",
       "       -2.37742369e-03, -2.82933801e-01,  2.35141233e-01,  5.76650277e-02,\n",
       "       -5.03489189e-02,  1.69149309e-01,  1.40128732e-01, -1.84568033e-01,\n",
       "        1.82420891e-02, -1.16530284e-01,  1.62458435e-01, -2.96141922e-01,\n",
       "        1.07163735e-01, -1.50677130e-01, -1.41561776e-01,  6.32065348e-03,\n",
       "        9.21488740e-03,  6.07152879e-02,  3.11450157e-02, -9.39102992e-02,\n",
       "        1.55027944e-03, -1.52231203e-02, -6.04510605e-02, -1.34573698e-01,\n",
       "       -1.35063380e-01, -1.54088348e-01, -5.88422604e-02, -2.11855114e-01,\n",
       "        1.07419118e-01, -3.95303890e-02, -5.14967032e-02, -8.14725682e-02,\n",
       "        3.62710133e-02,  1.37716904e-01,  5.70590645e-02,  1.01485886e-02,\n",
       "        4.62863967e-02,  1.91847719e-02, -6.57523498e-02, -3.95316295e-02,\n",
       "        7.13631511e-02, -2.85872314e-02, -7.34667033e-02,  9.09034386e-02,\n",
       "        1.58687517e-01, -8.79704878e-02, -1.32563829e-01, -1.71486419e-02,\n",
       "        1.31147712e-01, -2.20209390e-01,  1.05488688e-01, -7.21108690e-02,\n",
       "        4.53139171e-02, -1.09229572e-01, -3.79458852e-02,  3.58982151e-03,\n",
       "       -2.60739867e-02, -1.88684046e-01, -7.23453015e-02, -1.32496476e-01,\n",
       "        8.33514407e-02, -9.18954462e-02, -3.16612907e-02, -2.65575014e-02,\n",
       "        8.40158463e-02,  1.24118384e-02, -8.32407176e-02,  6.76427707e-02,\n",
       "       -1.92830469e-02, -3.72877195e-02,  6.70617595e-02,  1.70562193e-02,\n",
       "        6.35192245e-02, -4.59765270e-02,  1.32260054e-01, -4.12585810e-02,\n",
       "        3.70897017e-02,  6.23463355e-02,  5.64968660e-02,  1.68582082e-01,\n",
       "       -7.91857317e-02, -1.16865799e-01, -1.59776479e-01,  1.87855698e-02,\n",
       "       -7.31628984e-02, -9.99474823e-02, -6.15190193e-02, -1.68501452e-01,\n",
       "        1.23942103e-02,  1.29191890e-01, -3.87190981e-03,  7.54098454e-03,\n",
       "       -1.58806682e-01,  5.86973913e-02, -1.51811138e-01,  1.17401563e-01,\n",
       "       -2.76492655e-01,  9.17524397e-02, -2.20363215e-01,  1.21649586e-01,\n",
       "        7.43964165e-02,  3.50103378e-01,  1.43307641e-01, -2.92113394e-01,\n",
       "       -2.72587221e-02, -2.31343266e-02,  1.79297879e-01, -2.88085550e-01,\n",
       "       -1.79037079e-01, -3.47246192e-02,  9.92357805e-02,  5.58054894e-02,\n",
       "       -1.13974959e-02,  1.36998773e-01,  3.05563956e-02, -6.77849576e-02,\n",
       "        2.61912882e-01, -1.28011475e-03,  1.08311795e-01, -1.50269447e-02,\n",
       "        7.87274726e-03, -7.60774314e-02, -8.90571997e-02,  3.08456033e-01,\n",
       "        6.57378556e-03,  2.32059211e-02, -2.14742217e-03, -1.61577046e-01,\n",
       "        1.14034757e-01,  1.03700735e-01, -9.75164236e-04, -9.80174616e-02,\n",
       "        5.15809581e-02, -7.83802867e-02, -6.52991608e-02, -5.65937795e-02,\n",
       "       -7.83203691e-02,  2.11480656e-03,  8.47422332e-03,  9.69735906e-02,\n",
       "       -6.03040271e-02, -1.40993953e-01,  4.52721566e-02,  4.38865051e-02,\n",
       "        8.11690763e-02, -3.03331494e-01,  1.05105951e-01,  2.11055234e-01,\n",
       "       -3.51397544e-01, -1.13058962e-01, -2.29896773e-02,  1.43591195e-01,\n",
       "        9.56523493e-02, -1.06793761e-01, -8.61797482e-02,  7.51460120e-02,\n",
       "       -1.36340065e-02,  3.25760432e-02,  3.00326608e-02,  1.19154327e-01,\n",
       "        1.71103254e-02, -3.55947725e-02,  8.08539316e-02, -2.22158320e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.837200\n"
     ]
    }
   ],
   "source": [
    "# Count_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(count_train.toarray(),Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.844600\n"
     ]
    }
   ],
   "source": [
    "# tfidf_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(tfidf_train.toarray(),Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.816600\n"
     ]
    }
   ],
   "source": [
    "# word2vec_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(word2vec_train,Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "BATCH_SIZE = 128\n",
    "epochs = 10\n",
    "valid = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38466725, -0.01089948, -0.11358804, ..., -0.03559477,\n",
       "         0.08085393, -0.02221583],\n",
       "       [ 0.30409044, -0.02905423, -0.01033402, ..., -0.00157314,\n",
       "         0.15747353,  0.00254611],\n",
       "       [ 0.04273553, -0.05674532,  0.13614783, ..., -0.00226927,\n",
       "         0.12489501, -0.05610893],\n",
       "       ...,\n",
       "       [ 0.4288284 ,  0.12031446, -0.04713427, ...,  0.05309478,\n",
       "         0.18899845,  0.10103627],\n",
       "       [ 0.28443012, -0.04825247, -0.07205519, ..., -0.00527934,\n",
       "         0.09853455,  0.0077378 ],\n",
       "       [ 0.5616765 ,  0.00865968, -0.08046958, ..., -0.0504946 ,\n",
       "         0.05616886,  0.09665068]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.toarray().shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tensorflow 2.0 모델 구축 방법\n",
    "1. Sequential API : tf.keras.Sequential, model.add()\n",
    "2. Functional API : Input - layers \n",
    "3. Custom layer : layers. layer 상속 : 여러 레이어를 하나로 묶은 레이어 구현 용이\n",
    "4. Subclassing : tf.keras.Model  : 자유도가 가장 높아서 자주 사용. \n",
    "\n",
    "'''\n",
    "\n",
    "# tf.keras.Model을 학습받아 클래스로 구현 \n",
    "# Input은 \n",
    "\n",
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, emb_shape,lstm_shape  ):\n",
    "        super(RNN,self).__init__() #  부모 클래스에 있는__init__ 함수 호출 \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim= input_shape, # 이전에 임베딩된 5000개 Feature 를 넣겠다\n",
    "                                          output_dim= emb_shape # Dense 임베딩 결과shape                                         \n",
    "                                         )\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(lstm_shape, #  lstm_shape : units = dimensionality of the output shape 의미\n",
    "                                          return_sequences = True)\n",
    "        self.lstm_2 = tf.keras.layers.LSTM(lstm_shape)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc1 = tf.keras.layers.Dense(units = 150, activation = tf.keras.activations.tanh)\n",
    "        self.fc2 = tf.keras.layers.Dense(units = 1, activation = tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm_1(x)\n",
    "        x = self.lstm_2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tfidf_train.toarray().shape[1]\n",
    "emb_shape =500\n",
    "lstm_shape = 150\n",
    "\n",
    "model = RNN(input_shape, emb_shape, lstm_shape)\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(1e-4),\n",
    "             loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics= [tf.keras.metrics.BinaryAccuracy(name = \"accuracy\")]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "   64/25000 [..............................] - ETA: 37:31 - loss: 0.6919 - accuracy: 0.5625  "
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 500, 150, 1, 5000, 32, 150] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[StatefulPartitionedCall_1]]\n\t [[Reshape_14/_38]] [Op:__inference_distributed_function_8024]\n\nFunction call stack:\ndistributed_function -> distributed_function -> distributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-81d90dd7746b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 500, 150, 1, 5000, 32, 150] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[StatefulPartitionedCall_1]]\n\t [[Reshape_14/_38]] [Op:__inference_distributed_function_8024]\n\nFunction call stack:\ndistributed_function -> distributed_function -> distributed_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(tfidf_train.toarray(),df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popcorn",
   "language": "python",
   "name": "popcorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
