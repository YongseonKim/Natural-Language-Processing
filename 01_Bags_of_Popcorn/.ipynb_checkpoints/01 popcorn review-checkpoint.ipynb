{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "'''\n",
    "참고 URL\n",
    "- https://programmers.co.kr/learn/courses/21/lessons/1693\n",
    "- http://suanlab.com/assets/lectures/dpp/10.pdf\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "1. Data preprocessing\n",
    "2. word2vec\n",
    "3. modeling\n",
    "4. 평가\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledTrainData.tsv',\n",
       " 'sampleSubmission.csv',\n",
       " 'testData.tsv',\n",
       " 'unlabeledTrainData.tsv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "# -- quoting : 특수 문자가 포함된 필드를 감쌀 때 처리하는 방법, 문자를 따옴표로 묶는 방법\n",
    "import csv\n",
    "df = pd.read_csv('data/labeledTrainData.tsv', header=0, delimiter='\\t',quoting=3)\n",
    "# QUOTE_MINIMAL (0), QUOTE_ALL (1), \n",
    "# QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
    "'''\n",
    "-QUOTE_ALL(1) : Quote everything, regardless of type.(문자열처리, 모든데이터 묶음)\n",
    "-QUOTE_MINIMAL(0) :Quote fields with special characters (특수 문자가 포함된 따옴표 필드)\n",
    "(anything that would confuse a parser configured with the same dialect and options). This is the default\n",
    "-QUOTE_NONNUMERIC(2) :Quote all fields that are not integers or floats.(숫자가 아닌 경우 묶음) \n",
    "  When used with the reader, input fields that are not quoted are converted to floats.\n",
    "-QUOTE_NONE(3) : Do not quote anything on output. 데이터를 묶지 않음\n",
    " When used with the reader, quote characters are included in the field values (normally, they are treated as delimiters and stripped).\n",
    " reader와 사용하면 쌍따옴표는 필드값으로 포함된다.\n",
    "'''\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         With all this stuff going down at the moment ...\n",
       "1           The Classic War of the Worlds   by Timothy ...\n",
       "2         The film starts with a manager  Nicholas Bell...\n",
       "3         It must be assumed that those who praised thi...\n",
       "4         Superbly trashy and wondrously unpretentious ...\n",
       "                               ...                        \n",
       "24995     It seems like more consideration has gone int...\n",
       "24996     I don t believe they made this film  Complete...\n",
       "24997     Guy is a loser  Can t get girls  needs to bui...\n",
       "24998     This 30 minute documentary Buñuel made in the...\n",
       "24999     I saw this movie as a child and it broke my h...\n",
       "Name: review, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 -- HTML 태그, \\ 특수 문자 제거\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def preprocessing(x):\n",
    "    #HTML 태그 제거\n",
    "    x= BeautifulSoup(x,'html.parser').get_text()\n",
    "    # 특수문자 제거 # 영문자,숫자를 제외한 문자를 모드 변환 띄어쓰기로\n",
    "    x = re.sub(\"\\W\",\" \",x)    \n",
    "    return x\n",
    "\n",
    "df['review']=df['review'].map(lambda x: preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review2</th>\n",
       "      <th>words</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>[``, stuff, going, moment, mj, 've, started, l...</td>\n",
       "      <td>[16, 432, 94, 448, 8944, 70, 523, 2440, 142, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>[``, \\, '', classic, war, worlds\\, '', timothy...</td>\n",
       "      <td>[16, 11, 7, 265, 248, 14492, 7, 3528, 7181, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>[``, film, starts, manager, (, nicholas, bell,...</td>\n",
       "      <td>[16, 10, 399, 2930, 13, 4346, 3834, 12, 610, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>[``, must, assumed, praised, film, (, \\, '', g...</td>\n",
       "      <td>[16, 131, 4773, 5646, 10, 13, 11, 7, 686, 703,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[``, superbly, trashy, wondrously, unpretentio...</td>\n",
       "      <td>[16, 3370, 4155, 44772, 11262, 986, 8, 2153, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                             review2  \\\n",
       "0  \"With all this stuff going down at the moment ...   \n",
       "1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"It must be assumed that those who praised thi...   \n",
       "4  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [``, stuff, going, moment, mj, 've, started, l...   \n",
       "1  [``, \\, '', classic, war, worlds\\, '', timothy...   \n",
       "2  [``, film, starts, manager, (, nicholas, bell,...   \n",
       "3  [``, must, assumed, praised, film, (, \\, '', g...   \n",
       "4  [``, superbly, trashy, wondrously, unpretentio...   \n",
       "\n",
       "                                              vector  \n",
       "0  [16, 432, 94, 448, 8944, 70, 523, 2440, 142, 1...  \n",
       "1  [16, 11, 7, 265, 248, 14492, 7, 3528, 7181, 34...  \n",
       "2  [16, 10, 399, 2930, 13, 4346, 3834, 12, 610, 2...  \n",
       "3  [16, 131, 4773, 5646, 10, 13, 11, 7, 686, 703,...  \n",
       "4  [16, 3370, 4155, 44772, 11262, 986, 8, 2153, 1...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이징 + Stopwords 제거\n",
    "# 장점 : 노이즈를 줄일 수 있음, 단점 : 문장 구조 모델링시 정보 유실 발생\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenizing(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(words.lower())\n",
    "    words = [x for x in words if x not in stop_words]\n",
    "    return words\n",
    "\n",
    "df['words'] = df['review'].map(lambda x : tokenizing(x))\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Tokenizing 방법 리뷰\n",
    "1.  Split 함수\n",
    "2.  NLTK 활용 \n",
    "   - Tokenizing → Index로 벡터화 해야하는데 NLTK는 Tokenizing 까지만\n",
    "3.  keras.preprocessing 활용\n",
    "   - Keras는 Vector화 까지 가능, \n",
    "\n",
    "'''\n",
    "# https://inuplace.tistory.com/536\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['words']) # Fit\n",
    "df['vector'] = token.texts_to_sequences(df['words']) # vector화 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review2</th>\n",
       "      <th>words</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>[``, stuff, going, moment, mj, 've, started, l...</td>\n",
       "      <td>[16, 432, 94, 448, 8944, 70, 523, 2440, 142, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>[``, \\, '', classic, war, worlds\\, '', timothy...</td>\n",
       "      <td>[16, 11, 7, 265, 248, 14492, 7, 3528, 7181, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>[``, film, starts, manager, (, nicholas, bell,...</td>\n",
       "      <td>[16, 10, 399, 2930, 13, 4346, 3834, 12, 610, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>[``, must, assumed, praised, film, (, \\, '', g...</td>\n",
       "      <td>[16, 131, 4773, 5646, 10, 13, 11, 7, 686, 703,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[``, superbly, trashy, wondrously, unpretentio...</td>\n",
       "      <td>[16, 3370, 4155, 44772, 11262, 986, 8, 2153, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                             review2  \\\n",
       "0  \"With all this stuff going down at the moment ...   \n",
       "1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"It must be assumed that those who praised thi...   \n",
       "4  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [``, stuff, going, moment, mj, 've, started, l...   \n",
       "1  [``, \\, '', classic, war, worlds\\, '', timothy...   \n",
       "2  [``, film, starts, manager, (, nicholas, bell,...   \n",
       "3  [``, must, assumed, praised, film, (, \\, '', g...   \n",
       "4  [``, superbly, trashy, wondrously, unpretentio...   \n",
       "\n",
       "                                              vector  \n",
       "0  [16, 432, 94, 448, 8944, 70, 523, 2440, 142, 1...  \n",
       "1  [16, 11, 7, 265, 248, 14492, 7, 3528, 7181, 34...  \n",
       "2  [16, 10, 399, 2930, 13, 4346, 3834, 12, 610, 2...  \n",
       "3  [16, 131, 4773, 5646, 10, 13, 11, 7, 686, 703,...  \n",
       "4  [16, 3370, 4155, 44772, 11262, 986, 8, 2153, 1...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120745"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "vocab = token.word_index\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 단어수 7\n",
      "1사분위 92.0\n",
      "2사분위 130.0\n",
      "3사분위 214.0\n",
      "최대 단어수 1790\n"
     ]
    }
   ],
   "source": [
    "# 최대 문장길이 :: 3사분위에 해당하는 214개를 고정길이로 설정\n",
    "print(\"최소 단어수\", df['vector'].map(lambda x : len(x)).min())\n",
    "print(\"1사분위\", df['vector'].map(lambda x : len(x)).quantile(0.25))\n",
    "print(\"2사분위\",df['vector'].map(lambda x : len(x)).quantile(0.50))\n",
    "print(\"3사분위\",df['vector'].map(lambda x : len(x)).quantile(0.75))\n",
    "print(\"최대 단어수\",df['vector'].map(lambda x : len(x)).max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  756  2365    18 ...  1412     2     7]\n",
      " [   16    11     7 ...     0     0     0]\n",
      " [  826 24276 59374 ...  6154     2     7]\n",
      " ...\n",
      " [   16   146  3353 ...     0     0     0]\n",
      " [   16   974   792 ...     0     0     0]\n",
      " [   16   128     9 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding: 가변 길이 → 고정 길이\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_padding = 214\n",
    "\n",
    "X_train = pad_sequences(df['vector'],maxlen = max_padding, padding = 'post' )\n",
    "Y_train = df['sentiment']\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\\n2. TF-IDF\\n3. Countvectorizer\\n4. Word2vec\\n'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector 화 \n",
    "'''\n",
    "1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\n",
    "2. TF-IDF (완료)\n",
    "3. Countvectorizer -- TF를 의미하는 것\n",
    "4. Word2vec\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(25000, 5000)\n",
      "  (0, 3883)\t0.09933834228705851\n",
      "  (0, 2559)\t0.10246383707774773\n",
      "  (0, 1376)\t0.06760563802592813\n",
      "  (0, 4225)\t0.09765602879634694\n",
      "  (0, 3320)\t0.09183369558685252\n",
      "  (0, 3039)\t0.10031374229998055\n",
      "  (0, 3882)\t0.09933834228705851\n",
      "  (0, 4487)\t0.09642717568231131\n",
      "  (0, 3319)\t0.10461652393508376\n",
      "  (0, 34)\t0.09594181277732189\n",
      "  (0, 2825)\t0.09864400636809899\n",
      "  (0, 519)\t0.058425705309277265\n",
      "  (0, 1721)\t0.16963389092415215\n",
      "  (0, 2505)\t0.1008247366312985\n",
      "  (0, 2872)\t0.17049221180098909\n",
      "  (0, 2243)\t0.06312603561211018\n",
      "  (0, 4309)\t0.06253703899091029\n",
      "  (0, 1644)\t0.06809950863580468\n",
      "  (0, 1656)\t0.04997226926678856\n",
      "  (0, 1396)\t0.10217870074867391\n",
      "  (0, 1297)\t0.05669275727253205\n",
      "  (0, 1371)\t0.037508823151526205\n",
      "  (0, 4315)\t0.07504880385332162\n",
      "  (0, 1956)\t0.06565488069410813\n",
      "  (0, 299)\t0.07018631505870204\n",
      "  :\t:\n",
      "  (0, 2364)\t0.0827719690583781\n",
      "  (0, 2094)\t0.1599701293119485\n",
      "  (0, 2936)\t0.05789054099358346\n",
      "  (0, 2809)\t0.038399635337451506\n",
      "  (0, 1487)\t0.10617131767127166\n",
      "  (0, 1054)\t0.11903002459160476\n",
      "  (0, 3629)\t0.05912762485364786\n",
      "  (0, 4498)\t0.05025411464488773\n",
      "  (0, 2098)\t0.0923999760933632\n",
      "  (0, 2368)\t0.09474486219159972\n",
      "  (0, 818)\t0.07301386361412548\n",
      "  (0, 4794)\t0.049262965818947305\n",
      "  (0, 2488)\t0.062404320621754474\n",
      "  (0, 2871)\t0.11848169931053008\n",
      "  (0, 4827)\t0.09563128191930166\n",
      "  (0, 1350)\t0.0749201390949973\n",
      "  (0, 3200)\t0.0781470883429487\n",
      "  (0, 4831)\t0.04591417514234447\n",
      "  (0, 3085)\t0.09146970466225947\n",
      "  (0, 2699)\t0.09456842713553738\n",
      "  (0, 4215)\t0.06940545769735518\n",
      "  (0, 4728)\t0.07684434197584294\n",
      "  (0, 2973)\t0.06785066396116726\n",
      "  (0, 2000)\t0.10048258574240136\n",
      "  (0, 4305)\t0.06675343050062513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0897721 , 0.04699259, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.0837525 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# 입력이 텍스트여야함.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer = \"word\", sublinear_tf=True,\n",
    "                           ngram_range=(1,3), max_features=5000,stop_words = 'english')\n",
    "\n",
    "# min_df : 설정값보다 특정 토큰의 df(document Frequency)가 적으면 벡터화에서 제거\n",
    "# analyzer : word/char 2가지 : word는 단위 : 단어 / char : 단위 : char \n",
    "# sublinear_tf : term frequency에 대한 smoothing 여부\n",
    "# ngram_range = n-gram 의 범위 : 분석기에 의해 설정값을 사용하여 ngram자동 생성\n",
    "# max_features = 벡터의 최대 길이, \n",
    "\n",
    "tfidf_train = vectorizer.fit_transform(list(df['review']))\n",
    "tfidf_train\n",
    "print(type(tfidf_train))\n",
    "print(tfidf_train.shape)\n",
    "print(tfidf_train[0]) \n",
    "# 5000개의 단어 각각에 대한 tf-idf Weight를 의미함\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## TF \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000)\n",
    "\n",
    "count_train = vectorizer.fit_transform(list(df['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"With', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'MJ', \"i've\", 'started', 'listening', 'to', 'his', 'music,', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there,', 'watched', 'The', 'Wiz', 'and', 'watched', 'Moonwalker', 'again.', 'Maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent.', 'Moonwalker', 'is', 'part', 'biography,', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released.', 'Some', 'of', 'it', 'has', 'subtle', 'messages', 'about', \"MJ's\", 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', \"m'kay.<br\", '/><br', '/>Visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'Michael', 'Jackson', 'so', 'unless', 'you', 'remotely', 'like', 'MJ', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring.', 'Some', 'may', 'call', 'MJ', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'BUT', 'MJ', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him.<br', '/><br', '/>The', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', '20', 'minutes', 'or', 'so', 'excluding', 'the', 'Smooth', 'Criminal', 'sequence', 'and', 'Joe', 'Pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord.', 'Why', 'he', 'wants', 'MJ', 'dead', 'so', 'bad', 'is', 'beyond', 'me.', 'Because', 'MJ', 'overheard', 'his', 'plans?', 'Nah,', 'Joe', \"Pesci's\", 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno,', 'maybe', 'he', 'just', 'hates', \"MJ's\", 'music.<br', '/><br', '/>Lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'MJ', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'Speed', 'Demon', 'sequence.', 'Also,', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'Bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene.<br', '/><br', '/>Bottom', 'line,', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'MJ', 'on', 'one', 'level', 'or', 'another', '(which', 'i', 'think', 'is', 'most', 'people).', 'If', 'not,', 'then', 'stay', 'away.', 'It', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', \"MJ's\", 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl!', 'Michael', 'Jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty?', 'Well,', 'with', 'all', 'the', 'attention', \"i've\", 'gave', 'this', 'subject....hmmm', 'well', 'i', \"don't\", 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors,', 'i', 'know', 'this', 'for', 'a', 'fact.', 'He', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars.', 'I', 'hope', 'he', 'is', 'not', 'the', 'latter.\"']\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :  %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# Word2vec 입력은 단어로 표현된 리스트를 입력값으로 받음\n",
    "# n-gram으로 만들어서 넣을 수도 있지만 여기에서는 단순히 split만해서 넣는 것으로 함\n",
    "\n",
    "sentences = []\n",
    "for review in list(df['review']) :\n",
    "    sentences.append(review.split())\n",
    "\n",
    "# print(sentences[0])\n",
    "\n",
    "# 하이퍼파라미터\n",
    "num_features = 300 # word2vec 특징 수\n",
    "min_word_count =20 \n",
    "num_workers = 6\n",
    "context =10 # Word2vec 수행을 위한 컨텍스트 윈도 크기\n",
    "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-13%EC%9D%BC%EC%B0%A8-word2vec-3c82ec870426\n",
    "downsampling = 1e-3 #Word2vec 빠른 학습을 위해 정답 단어 라벨에 대한 다운 샘플링, 보통 0.001이 좋은 성능\n",
    "#Downsampling of frequent words # 자주 나오는 단어에 대해서는 0.001 만큼 다운 샘플링하여 시간을 아낌\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-20 07:20:53,861 : INFO :  collecting all words and their counts\n",
      "2020-10-20 07:20:53,862 : INFO :  PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-20 07:20:54,158 : INFO :  PROGRESS: at sentence #10000, processed 2354780 words, keeping 163178 word types\n",
      "2020-10-20 07:20:54,484 : INFO :  PROGRESS: at sentence #20000, processed 4686268 words, keeping 251892 word types\n",
      "2020-10-20 07:20:54,640 : INFO :  collected 289705 word types from a corpus of 5844706 raw words and 25000 sentences\n",
      "2020-10-20 07:20:54,641 : INFO :  Loading a fresh vocabulary\n",
      "2020-10-20 07:20:54,743 : INFO :  effective_min_count=20 retains 16736 unique words (5% of original 289705, drops 272969)\n",
      "2020-10-20 07:20:54,743 : INFO :  effective_min_count=20 leaves 5206111 word corpus (89% of original 5844706, drops 638595)\n",
      "2020-10-20 07:20:54,786 : INFO :  deleting the raw counts dictionary of 289705 items\n",
      "2020-10-20 07:20:54,790 : INFO :  sample=0.001 downsamples 45 most-common words\n",
      "2020-10-20 07:20:54,790 : INFO :  downsampling leaves estimated 3929600 word corpus (75.5% of prior 5206111)\n",
      "2020-10-20 07:20:54,817 : INFO :  estimated required memory for 16736 words and 300 dimensions: 48534400 bytes\n",
      "2020-10-20 07:20:54,817 : INFO :  resetting layer weights\n",
      "2020-10-20 07:20:57,674 : INFO :  training model with 6 workers on 16736 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-10-20 07:20:58,683 : INFO :  EPOCH 1 - PROGRESS: at 31.63% examples, 1245536 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:20:59,686 : INFO :  EPOCH 1 - PROGRESS: at 64.30% examples, 1264491 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:00,692 : INFO :  EPOCH 1 - PROGRESS: at 97.47% examples, 1271915 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:00,737 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:00,741 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:00,744 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:00,747 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:00,750 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:00,753 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:00,754 : INFO :  EPOCH - 1 : training on 5844706 raw words (3929283 effective words) took 3.1s, 1277271 effective words/s\n",
      "2020-10-20 07:21:01,759 : INFO :  EPOCH 2 - PROGRESS: at 32.22% examples, 1276069 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:02,774 : INFO :  EPOCH 2 - PROGRESS: at 66.30% examples, 1298329 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:03,765 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:03,768 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:03,769 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:03,771 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:03,779 : INFO :  EPOCH 2 - PROGRESS: at 99.83% examples, 1297769 words/s, in_qsize 1, out_qsize 1\n",
      "2020-10-20 07:21:03,780 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:03,782 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:03,782 : INFO :  EPOCH - 2 : training on 5844706 raw words (3928815 effective words) took 3.0s, 1298644 effective words/s\n",
      "2020-10-20 07:21:04,787 : INFO :  EPOCH 3 - PROGRESS: at 31.90% examples, 1262810 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:05,798 : INFO :  EPOCH 3 - PROGRESS: at 65.46% examples, 1285144 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:06,782 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:06,789 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:06,792 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:06,795 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:06,798 : INFO :  EPOCH 3 - PROGRESS: at 99.83% examples, 1301775 words/s, in_qsize 1, out_qsize 1\n",
      "2020-10-20 07:21:06,799 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:06,799 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:06,799 : INFO :  EPOCH - 3 : training on 5844706 raw words (3930331 effective words) took 3.0s, 1303402 effective words/s\n",
      "2020-10-20 07:21:07,803 : INFO :  EPOCH 4 - PROGRESS: at 32.40% examples, 1284588 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:08,804 : INFO :  EPOCH 4 - PROGRESS: at 65.97% examples, 1301859 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:09,810 : INFO :  EPOCH 4 - PROGRESS: at 98.72% examples, 1290029 words/s, in_qsize 8, out_qsize 0\n",
      "2020-10-20 07:21:09,822 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:09,828 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:09,831 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:09,832 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:09,834 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:09,841 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:09,841 : INFO :  EPOCH - 4 : training on 5844706 raw words (3929365 effective words) took 3.0s, 1292803 effective words/s\n",
      "2020-10-20 07:21:10,847 : INFO :  EPOCH 5 - PROGRESS: at 32.25% examples, 1274990 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:11,853 : INFO :  EPOCH 5 - PROGRESS: at 65.97% examples, 1297672 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:12,827 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:12,830 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:12,833 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:12,837 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:12,838 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:12,843 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:12,843 : INFO :  EPOCH - 5 : training on 5844706 raw words (3929120 effective words) took 3.0s, 1309961 effective words/s\n",
      "2020-10-20 07:21:13,849 : INFO :  EPOCH 6 - PROGRESS: at 32.90% examples, 1301240 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:14,854 : INFO :  EPOCH 6 - PROGRESS: at 65.70% examples, 1291170 words/s, in_qsize 10, out_qsize 1\n",
      "2020-10-20 07:21:15,856 : INFO :  EPOCH 6 - PROGRESS: at 98.72% examples, 1289311 words/s, in_qsize 8, out_qsize 0\n",
      "2020-10-20 07:21:15,865 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:15,872 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:15,876 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:15,878 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:15,882 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:15,885 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:15,885 : INFO :  EPOCH - 6 : training on 5844706 raw words (3929447 effective words) took 3.0s, 1292898 effective words/s\n",
      "2020-10-20 07:21:16,892 : INFO :  EPOCH 7 - PROGRESS: at 32.25% examples, 1274032 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:17,900 : INFO :  EPOCH 7 - PROGRESS: at 65.82% examples, 1291985 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:18,886 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:18,892 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:18,893 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:18,894 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:18,895 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:18,897 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:18,898 : INFO :  EPOCH - 7 : training on 5844706 raw words (3929471 effective words) took 3.0s, 1305213 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-20 07:21:19,907 : INFO :  EPOCH 8 - PROGRESS: at 32.90% examples, 1295812 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:20,923 : INFO :  EPOCH 8 - PROGRESS: at 65.82% examples, 1284986 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:21,916 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:21,917 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:21,919 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:21,921 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:21,928 : INFO :  EPOCH 8 - PROGRESS: at 99.82% examples, 1295463 words/s, in_qsize 1, out_qsize 1\n",
      "2020-10-20 07:21:21,928 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:21,928 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:21,929 : INFO :  EPOCH - 8 : training on 5844706 raw words (3928085 effective words) took 3.0s, 1297198 effective words/s\n",
      "2020-10-20 07:21:22,934 : INFO :  EPOCH 9 - PROGRESS: at 32.25% examples, 1275144 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:23,935 : INFO :  EPOCH 9 - PROGRESS: at 65.52% examples, 1290562 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:24,943 : INFO :  EPOCH 9 - PROGRESS: at 99.22% examples, 1294710 words/s, in_qsize 5, out_qsize 1\n",
      "2020-10-20 07:21:24,944 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:24,945 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:24,950 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:24,953 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:24,955 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:24,959 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:24,960 : INFO :  EPOCH - 9 : training on 5844706 raw words (3928079 effective words) took 3.0s, 1297157 effective words/s\n",
      "2020-10-20 07:21:25,963 : INFO :  EPOCH 10 - PROGRESS: at 31.46% examples, 1245112 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:26,968 : INFO :  EPOCH 10 - PROGRESS: at 65.34% examples, 1286680 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-20 07:21:27,965 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-20 07:21:27,966 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-20 07:21:27,971 : INFO :  EPOCH 10 - PROGRESS: at 99.60% examples, 1301068 words/s, in_qsize 3, out_qsize 1\n",
      "2020-10-20 07:21:27,972 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-20 07:21:27,973 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-20 07:21:27,981 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-20 07:21:27,986 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-20 07:21:27,987 : INFO :  EPOCH - 10 : training on 5844706 raw words (3929945 effective words) took 3.0s, 1299403 effective words/s\n",
      "2020-10-20 07:21:27,987 : INFO :  training on a 58447060 raw words (39291941 effective words) took 30.3s, 1296245 effective words/s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# https://wikidocs.net/50739\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers = num_workers,\n",
    "                          size = num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window =  context,\n",
    "                          sample = downsampling,\n",
    "                          iter = 10,\n",
    "                          sg =0 # sg =0 CBOW, 1 : skip-gram\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man,', 0.7132534980773926),\n",
       " ('woman', 0.6852004528045654),\n",
       " ('doctor', 0.6443697810173035),\n",
       " ('lady', 0.6242107152938843),\n",
       " ('boy', 0.608852744102478),\n",
       " ('soldier', 0.6023517847061157),\n",
       " ('man.', 0.5866681337356567),\n",
       " ('businessman', 0.5810524821281433),\n",
       " ('priest', 0.5781041979789734),\n",
       " ('person', 0.5658349990844727)]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# man과 가장 유사한 단어 골라내기 \n",
    "model.wv.most_similar(\"man\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 은 단어 하나하나가 벡터로 표현되어 있다.\n",
    "# Review 데이터는 단어들의 조합이기에 Review를 벡터로 표현하기 위해\n",
    "# Review에 포함된 단어 벡터들의 평균값을 만든다.\n",
    "# 다른 방법으로는 Doc2vec, average of word2vec vectors with TF-IDF\n",
    "# Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector.\n",
    " # 단어 벡터에 TF-IDF를 곱해서 평균 내는 방법\n",
    "    \n",
    "# https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence\n",
    "\n",
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype = np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘 사전\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words +=1\n",
    "            #사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            \n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "    feature_vector = np.divide(feature_vector,num_words)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    \n",
    "    for s in reviews :\n",
    "        dataset.append(get_features(s,model,num_features))\n",
    "    \n",
    "    reviewFeaturevecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs\n",
    "\n",
    "word2vec_train = get_dataset(sentences,model,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x_train,x_val = train_test_split(X_train,test_size =0.2, random_state =99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "map not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-201-f8a0a59f8cc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: map not found"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(X_train.map(split('\\t')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0897721 , 0.04699259, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.0837525 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popcorn",
   "language": "python",
   "name": "popcorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
