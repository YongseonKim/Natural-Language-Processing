{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "'''\n",
    "참고 URL\n",
    "- https://programmers.co.kr/learn/courses/21/lessons/1693\n",
    "- http://suanlab.com/assets/lectures/dpp/10.pdf\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "1. Data preprocessing\n",
    "2. word2vec\n",
    "3. modeling\n",
    "4. 평가\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledTrainData.tsv',\n",
       " 'sampleSubmission.csv',\n",
       " 'testData.tsv',\n",
       " 'unlabeledTrainData.tsv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "# -- quoting : 특수 문자가 포함된 필드를 감쌀 때 처리하는 방법, 문자를 따옴표로 묶는 방법\n",
    "import csv\n",
    "df = pd.read_csv('data/labeledTrainData.tsv', header=0, delimiter='\\t',quoting=3)\n",
    "# QUOTE_MINIMAL (0), QUOTE_ALL (1), \n",
    "# QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
    "'''\n",
    "-QUOTE_ALL(1) : Quote everything, regardless of type.(문자열처리, 모든데이터 묶음)\n",
    "-QUOTE_MINIMAL(0) :Quote fields with special characters (특수 문자가 포함된 따옴표 필드)\n",
    "(anything that would confuse a parser configured with the same dialect and options). This is the default\n",
    "-QUOTE_NONNUMERIC(2) :Quote all fields that are not integers or floats.(숫자가 아닌 경우 묶음) \n",
    "  When used with the reader, input fields that are not quoted are converted to floats.\n",
    "-QUOTE_NONE(3) : Do not quote anything on output. 데이터를 묶지 않음\n",
    " When used with the reader, quote characters are included in the field values (normally, they are treated as delimiters and stripped).\n",
    " reader와 사용하면 쌍따옴표는 필드값으로 포함된다.\n",
    "'''\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         With all this stuff going down at the moment ...\n",
       "1           The Classic War of the Worlds   by Timothy ...\n",
       "2         The film starts with a manager  Nicholas Bell...\n",
       "3         It must be assumed that those who praised thi...\n",
       "4         Superbly trashy and wondrously unpretentious ...\n",
       "                               ...                        \n",
       "24995     It seems like more consideration has gone int...\n",
       "24996     I don t believe they made this film  Complete...\n",
       "24997     Guy is a loser  Can t get girls  needs to bui...\n",
       "24998     This 30 minute documentary Buñuel made in the...\n",
       "24999     I saw this movie as a child and it broke my h...\n",
       "Name: review, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 -- HTML 태그, \\ 특수 문자 제거\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def preprocessing(x):\n",
    "    #HTML 태그 제거\n",
    "    x= BeautifulSoup(x,'html.parser').get_text()\n",
    "    # 특수문자 제거 # 영문자,숫자를 제외한 문자를 모드 변환 띄어쓰기로\n",
    "    x = re.sub(\"\\W\",\" \",x)    \n",
    "    return x\n",
    "\n",
    "df['review']=df['review'].map(lambda x: preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review2</th>\n",
       "      <th>words</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>[``, stuff, going, moment, mj, 've, started, l...</td>\n",
       "      <td>[16, 432, 94, 448, 8944, 70, 523, 2440, 142, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>[``, \\, '', classic, war, worlds\\, '', timothy...</td>\n",
       "      <td>[16, 11, 7, 265, 248, 14492, 7, 3528, 7181, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>[``, film, starts, manager, (, nicholas, bell,...</td>\n",
       "      <td>[16, 10, 399, 2930, 13, 4346, 3834, 12, 610, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>[``, must, assumed, praised, film, (, \\, '', g...</td>\n",
       "      <td>[16, 131, 4773, 5646, 10, 13, 11, 7, 686, 703,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[``, superbly, trashy, wondrously, unpretentio...</td>\n",
       "      <td>[16, 3370, 4155, 44772, 11262, 986, 8, 2153, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                             review2  \\\n",
       "0  \"With all this stuff going down at the moment ...   \n",
       "1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"It must be assumed that those who praised thi...   \n",
       "4  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [``, stuff, going, moment, mj, 've, started, l...   \n",
       "1  [``, \\, '', classic, war, worlds\\, '', timothy...   \n",
       "2  [``, film, starts, manager, (, nicholas, bell,...   \n",
       "3  [``, must, assumed, praised, film, (, \\, '', g...   \n",
       "4  [``, superbly, trashy, wondrously, unpretentio...   \n",
       "\n",
       "                                              vector  \n",
       "0  [16, 432, 94, 448, 8944, 70, 523, 2440, 142, 1...  \n",
       "1  [16, 11, 7, 265, 248, 14492, 7, 3528, 7181, 34...  \n",
       "2  [16, 10, 399, 2930, 13, 4346, 3834, 12, 610, 2...  \n",
       "3  [16, 131, 4773, 5646, 10, 13, 11, 7, 686, 703,...  \n",
       "4  [16, 3370, 4155, 44772, 11262, 986, 8, 2153, 1...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이징 + Stopwords 제거\n",
    "# 장점 : 노이즈를 줄일 수 있음, 단점 : 문장 구조 모델링시 정보 유실 발생\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenizing(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(words.lower())\n",
    "    words = [x for x in words if x not in stop_words]\n",
    "    return words\n",
    "\n",
    "df['words'] = df['review'].map(lambda x : tokenizing(x))\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Tokenizing 방법 리뷰\n",
    "1.  Split 함수\n",
    "2.  NLTK 활용 \n",
    "   - Tokenizing → Index로 벡터화 해야하는데 NLTK는 Tokenizing 까지만\n",
    "3.  keras.preprocessing 활용\n",
    "   - Keras는 Vector화 까지 가능, \n",
    "\n",
    "'''\n",
    "# https://inuplace.tistory.com/536\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['words']) # Fit\n",
    "df['vector'] = token.texts_to_sequences(df['words']) # vector화 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review2</th>\n",
       "      <th>words</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>[``, stuff, going, moment, mj, 've, started, l...</td>\n",
       "      <td>[16, 432, 94, 448, 8944, 70, 523, 2440, 142, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>[``, \\, '', classic, war, worlds\\, '', timothy...</td>\n",
       "      <td>[16, 11, 7, 265, 248, 14492, 7, 3528, 7181, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>[``, film, starts, manager, (, nicholas, bell,...</td>\n",
       "      <td>[16, 10, 399, 2930, 13, 4346, 3834, 12, 610, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>[``, must, assumed, praised, film, (, \\, '', g...</td>\n",
       "      <td>[16, 131, 4773, 5646, 10, 13, 11, 7, 686, 703,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[``, superbly, trashy, wondrously, unpretentio...</td>\n",
       "      <td>[16, 3370, 4155, 44772, 11262, 986, 8, 2153, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                             review2  \\\n",
       "0  \"With all this stuff going down at the moment ...   \n",
       "1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"It must be assumed that those who praised thi...   \n",
       "4  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [``, stuff, going, moment, mj, 've, started, l...   \n",
       "1  [``, \\, '', classic, war, worlds\\, '', timothy...   \n",
       "2  [``, film, starts, manager, (, nicholas, bell,...   \n",
       "3  [``, must, assumed, praised, film, (, \\, '', g...   \n",
       "4  [``, superbly, trashy, wondrously, unpretentio...   \n",
       "\n",
       "                                              vector  \n",
       "0  [16, 432, 94, 448, 8944, 70, 523, 2440, 142, 1...  \n",
       "1  [16, 11, 7, 265, 248, 14492, 7, 3528, 7181, 34...  \n",
       "2  [16, 10, 399, 2930, 13, 4346, 3834, 12, 610, 2...  \n",
       "3  [16, 131, 4773, 5646, 10, 13, 11, 7, 686, 703,...  \n",
       "4  [16, 3370, 4155, 44772, 11262, 986, 8, 2153, 1...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120745"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "vocab = token.word_index\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 단어수 7\n",
      "1사분위 92.0\n",
      "2사분위 130.0\n",
      "3사분위 214.0\n",
      "최대 단어수 1790\n"
     ]
    }
   ],
   "source": [
    "# 최대 문장길이 :: 3사분위에 해당하는 214개를 고정길이로 설정\n",
    "print(\"최소 단어수\", df['vector'].map(lambda x : len(x)).min())\n",
    "print(\"1사분위\", df['vector'].map(lambda x : len(x)).quantile(0.25))\n",
    "print(\"2사분위\",df['vector'].map(lambda x : len(x)).quantile(0.50))\n",
    "print(\"3사분위\",df['vector'].map(lambda x : len(x)).quantile(0.75))\n",
    "print(\"최대 단어수\",df['vector'].map(lambda x : len(x)).max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  756  2365    18 ...  1412     2     7]\n",
      " [   16    11     7 ...     0     0     0]\n",
      " [  826 24276 59374 ...  6154     2     7]\n",
      " ...\n",
      " [   16   146  3353 ...     0     0     0]\n",
      " [   16   974   792 ...     0     0     0]\n",
      " [   16   128     9 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding: 가변 길이 → 고정 길이\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_padding = 214\n",
    "\n",
    "X_train = pad_sequences(df['vector'],maxlen = max_padding, padding = 'post' )\n",
    "Y_train = df['sentiment']\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\\n2. TF-IDF\\n3. Countvectorizer\\n4. Word2vec\\n'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector 화 \n",
    "'''\n",
    "1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\n",
    "2. TF-IDF (완료)\n",
    "3. Countvectorizer -- TF를 의미하는 것\n",
    "4. Word2vec(완료)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(25000, 5000)\n",
      "  (0, 3883)\t0.09933834228705851\n",
      "  (0, 2559)\t0.10246383707774773\n",
      "  (0, 1376)\t0.06760563802592813\n",
      "  (0, 4225)\t0.09765602879634694\n",
      "  (0, 3320)\t0.09183369558685252\n",
      "  (0, 3039)\t0.10031374229998055\n",
      "  (0, 3882)\t0.09933834228705851\n",
      "  (0, 4487)\t0.09642717568231131\n",
      "  (0, 3319)\t0.10461652393508376\n",
      "  (0, 34)\t0.09594181277732189\n",
      "  (0, 2825)\t0.09864400636809899\n",
      "  (0, 519)\t0.058425705309277265\n",
      "  (0, 1721)\t0.16963389092415215\n",
      "  (0, 2505)\t0.1008247366312985\n",
      "  (0, 2872)\t0.17049221180098909\n",
      "  (0, 2243)\t0.06312603561211018\n",
      "  (0, 4309)\t0.06253703899091029\n",
      "  (0, 1644)\t0.06809950863580468\n",
      "  (0, 1656)\t0.04997226926678856\n",
      "  (0, 1396)\t0.10217870074867391\n",
      "  (0, 1297)\t0.05669275727253205\n",
      "  (0, 1371)\t0.037508823151526205\n",
      "  (0, 4315)\t0.07504880385332162\n",
      "  (0, 1956)\t0.06565488069410813\n",
      "  (0, 299)\t0.07018631505870204\n",
      "  :\t:\n",
      "  (0, 2364)\t0.0827719690583781\n",
      "  (0, 2094)\t0.1599701293119485\n",
      "  (0, 2936)\t0.05789054099358346\n",
      "  (0, 2809)\t0.038399635337451506\n",
      "  (0, 1487)\t0.10617131767127166\n",
      "  (0, 1054)\t0.11903002459160476\n",
      "  (0, 3629)\t0.05912762485364786\n",
      "  (0, 4498)\t0.05025411464488773\n",
      "  (0, 2098)\t0.0923999760933632\n",
      "  (0, 2368)\t0.09474486219159972\n",
      "  (0, 818)\t0.07301386361412548\n",
      "  (0, 4794)\t0.049262965818947305\n",
      "  (0, 2488)\t0.062404320621754474\n",
      "  (0, 2871)\t0.11848169931053008\n",
      "  (0, 4827)\t0.09563128191930166\n",
      "  (0, 1350)\t0.0749201390949973\n",
      "  (0, 3200)\t0.0781470883429487\n",
      "  (0, 4831)\t0.04591417514234447\n",
      "  (0, 3085)\t0.09146970466225947\n",
      "  (0, 2699)\t0.09456842713553738\n",
      "  (0, 4215)\t0.06940545769735518\n",
      "  (0, 4728)\t0.07684434197584294\n",
      "  (0, 2973)\t0.06785066396116726\n",
      "  (0, 2000)\t0.10048258574240136\n",
      "  (0, 4305)\t0.06675343050062513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0897721 , 0.04699259, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.0837525 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# 입력이 텍스트여야함.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer = \"word\", sublinear_tf=True,\n",
    "                           ngram_range=(1,3), max_features=5000,stop_words = 'english')\n",
    "\n",
    "# min_df : 설정값보다 특정 토큰의 df(document Frequency)가 적으면 벡터화에서 제거\n",
    "# analyzer : word/char 2가지 : word는 단위 : 단어 / char : 단위 : char \n",
    "# sublinear_tf : term frequency에 대한 smoothing 여부\n",
    "# ngram_range = n-gram 의 범위 : 분석기에 의해 설정값을 사용하여 ngram자동 생성\n",
    "# max_features = 벡터의 최대 길이, \n",
    "\n",
    "tfidf_train = vectorizer.fit_transform(list(df['review']))\n",
    "tfidf_train\n",
    "print(type(tfidf_train))\n",
    "print(tfidf_train.shape)\n",
    "print(tfidf_train[0]) \n",
    "# 5000개의 단어 각각에 대한 tf-idf Weight를 의미함\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## TF \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000)\n",
    "\n",
    "count_train = vectorizer.fit_transform(list(df['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :  %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# Word2vec 입력은 단어로 표현된 리스트를 입력값으로 받음\n",
    "# n-gram으로 만들어서 넣을 수도 있지만 여기에서는 단순히 split만해서 넣는 것으로 함\n",
    "\n",
    "sentences = []\n",
    "for review in list(df['review']) :\n",
    "    sentences.append(review.split())\n",
    "\n",
    "# print(sentences[0])\n",
    "\n",
    "# 하이퍼파라미터\n",
    "num_features = 1000 # word2vec 특징 수\n",
    "min_word_count =20 \n",
    "num_workers = 6\n",
    "context =10 # Word2vec 수행을 위한 컨텍스트 윈도 크기\n",
    "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-13%EC%9D%BC%EC%B0%A8-word2vec-3c82ec870426\n",
    "downsampling = 1e-3 #Word2vec 빠른 학습을 위해 정답 단어 라벨에 대한 다운 샘플링, 보통 0.001이 좋은 성능\n",
    "#Downsampling of frequent words # 자주 나오는 단어에 대해서는 0.001 만큼 다운 샘플링하여 시간을 아낌\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 07:12:34,544 : INFO :  collecting all words and their counts\n",
      "2020-10-21 07:12:34,544 : INFO :  PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 07:12:34,883 : INFO :  PROGRESS: at sentence #10000, processed 2354780 words, keeping 163178 word types\n",
      "2020-10-21 07:12:35,244 : INFO :  PROGRESS: at sentence #20000, processed 4686268 words, keeping 251892 word types\n",
      "2020-10-21 07:12:35,416 : INFO :  collected 289705 word types from a corpus of 5844706 raw words and 25000 sentences\n",
      "2020-10-21 07:12:35,417 : INFO :  Loading a fresh vocabulary\n",
      "2020-10-21 07:12:35,541 : INFO :  effective_min_count=20 retains 16736 unique words (5% of original 289705, drops 272969)\n",
      "2020-10-21 07:12:35,541 : INFO :  effective_min_count=20 leaves 5206111 word corpus (89% of original 5844706, drops 638595)\n",
      "2020-10-21 07:12:35,583 : INFO :  deleting the raw counts dictionary of 289705 items\n",
      "2020-10-21 07:12:35,587 : INFO :  sample=0.001 downsamples 45 most-common words\n",
      "2020-10-21 07:12:35,587 : INFO :  downsampling leaves estimated 3929600 word corpus (75.5% of prior 5206111)\n",
      "2020-10-21 07:12:35,620 : INFO :  estimated required memory for 16736 words and 1000 dimensions: 142256000 bytes\n",
      "2020-10-21 07:12:35,620 : INFO :  resetting layer weights\n",
      "2020-10-21 07:12:38,575 : INFO :  training model with 6 workers on 16736 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-10-21 07:12:39,582 : INFO :  EPOCH 1 - PROGRESS: at 12.70% examples, 499924 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:40,600 : INFO :  EPOCH 1 - PROGRESS: at 26.91% examples, 528240 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:41,608 : INFO :  EPOCH 1 - PROGRESS: at 41.02% examples, 535942 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:42,643 : INFO :  EPOCH 1 - PROGRESS: at 54.88% examples, 535672 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:43,659 : INFO :  EPOCH 1 - PROGRESS: at 69.45% examples, 540559 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:44,662 : INFO :  EPOCH 1 - PROGRESS: at 83.16% examples, 539136 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:45,662 : INFO :  EPOCH 1 - PROGRESS: at 97.12% examples, 539459 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:45,792 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:12:45,811 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:12:45,822 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:12:45,826 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:12:45,838 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:12:45,842 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:12:45,843 : INFO :  EPOCH - 1 : training on 5844706 raw words (3929888 effective words) took 7.3s, 540952 effective words/s\n",
      "2020-10-21 07:12:46,858 : INFO :  EPOCH 2 - PROGRESS: at 13.36% examples, 522057 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:47,860 : INFO :  EPOCH 2 - PROGRESS: at 26.91% examples, 530099 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:48,884 : INFO :  EPOCH 2 - PROGRESS: at 40.85% examples, 532035 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:49,886 : INFO :  EPOCH 2 - PROGRESS: at 54.19% examples, 532114 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:50,892 : INFO :  EPOCH 2 - PROGRESS: at 67.82% examples, 531142 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:51,901 : INFO :  EPOCH 2 - PROGRESS: at 81.74% examples, 531914 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:52,923 : INFO :  EPOCH 2 - PROGRESS: at 95.94% examples, 533426 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:53,150 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:12:53,154 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:12:53,155 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:12:53,166 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:12:53,174 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:12:53,177 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:12:53,177 : INFO :  EPOCH - 2 : training on 5844706 raw words (3929167 effective words) took 7.3s, 535907 effective words/s\n",
      "2020-10-21 07:12:54,184 : INFO :  EPOCH 3 - PROGRESS: at 13.16% examples, 519948 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:55,195 : INFO :  EPOCH 3 - PROGRESS: at 26.72% examples, 526659 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:56,202 : INFO :  EPOCH 3 - PROGRESS: at 40.36% examples, 528535 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:57,219 : INFO :  EPOCH 3 - PROGRESS: at 53.85% examples, 529108 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:58,233 : INFO :  EPOCH 3 - PROGRESS: at 67.98% examples, 531702 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:12:59,234 : INFO :  EPOCH 3 - PROGRESS: at 81.89% examples, 533095 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:00,265 : INFO :  EPOCH 3 - PROGRESS: at 96.07% examples, 533660 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:00,472 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:13:00,493 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:13:00,496 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:13:00,499 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:13:00,500 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:13:00,501 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:13:00,502 : INFO :  EPOCH - 3 : training on 5844706 raw words (3928189 effective words) took 7.3s, 536535 effective words/s\n",
      "2020-10-21 07:13:01,509 : INFO :  EPOCH 4 - PROGRESS: at 12.54% examples, 492655 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:02,518 : INFO :  EPOCH 4 - PROGRESS: at 26.42% examples, 520324 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:03,525 : INFO :  EPOCH 4 - PROGRESS: at 40.53% examples, 530832 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:04,530 : INFO :  EPOCH 4 - PROGRESS: at 54.03% examples, 532420 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:05,536 : INFO :  EPOCH 4 - PROGRESS: at 68.15% examples, 535271 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:06,556 : INFO :  EPOCH 4 - PROGRESS: at 81.74% examples, 532353 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:07,559 : INFO :  EPOCH 4 - PROGRESS: at 95.78% examples, 534385 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:07,793 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:13:07,796 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:13:07,808 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:13:07,822 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:13:07,828 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:13:07,840 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:13:07,841 : INFO :  EPOCH - 4 : training on 5844706 raw words (3930365 effective words) took 7.3s, 535675 effective words/s\n",
      "2020-10-21 07:13:08,847 : INFO :  EPOCH 5 - PROGRESS: at 13.22% examples, 520351 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:09,849 : INFO :  EPOCH 5 - PROGRESS: at 27.25% examples, 538909 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:10,851 : INFO :  EPOCH 5 - PROGRESS: at 40.70% examples, 535427 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:11,866 : INFO :  EPOCH 5 - PROGRESS: at 54.19% examples, 534464 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:12,876 : INFO :  EPOCH 5 - PROGRESS: at 67.82% examples, 532668 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:13,893 : INFO :  EPOCH 5 - PROGRESS: at 81.74% examples, 532515 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:14,898 : INFO :  EPOCH 5 - PROGRESS: at 94.79% examples, 528853 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:15,250 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:13:15,255 : INFO :  worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 07:13:15,259 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:13:15,273 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:13:15,282 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:13:15,291 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:13:15,291 : INFO :  EPOCH - 5 : training on 5844706 raw words (3929917 effective words) took 7.4s, 527657 effective words/s\n",
      "2020-10-21 07:13:16,305 : INFO :  EPOCH 6 - PROGRESS: at 11.37% examples, 443637 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:17,322 : INFO :  EPOCH 6 - PROGRESS: at 23.16% examples, 455128 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:18,335 : INFO :  EPOCH 6 - PROGRESS: at 35.68% examples, 466662 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:19,336 : INFO :  EPOCH 6 - PROGRESS: at 48.50% examples, 474931 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:20,342 : INFO :  EPOCH 6 - PROGRESS: at 60.70% examples, 475882 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:21,357 : INFO :  EPOCH 6 - PROGRESS: at 73.04% examples, 475815 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:22,363 : INFO :  EPOCH 6 - PROGRESS: at 85.80% examples, 478950 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:23,366 : INFO :  EPOCH 6 - PROGRESS: at 99.22% examples, 483321 words/s, in_qsize 5, out_qsize 1\n",
      "2020-10-21 07:13:23,367 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:13:23,368 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:13:23,375 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:13:23,380 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:13:23,387 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:13:23,389 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:13:23,390 : INFO :  EPOCH - 6 : training on 5844706 raw words (3929801 effective words) took 8.1s, 485448 effective words/s\n",
      "2020-10-21 07:13:24,396 : INFO :  EPOCH 7 - PROGRESS: at 12.86% examples, 506387 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:25,397 : INFO :  EPOCH 7 - PROGRESS: at 26.09% examples, 515806 words/s, in_qsize 12, out_qsize 0\n",
      "2020-10-21 07:13:26,421 : INFO :  EPOCH 7 - PROGRESS: at 38.76% examples, 507498 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:27,428 : INFO :  EPOCH 7 - PROGRESS: at 51.15% examples, 501678 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:28,433 : INFO :  EPOCH 7 - PROGRESS: at 64.51% examples, 505131 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:29,434 : INFO :  EPOCH 7 - PROGRESS: at 77.95% examples, 508896 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:30,435 : INFO :  EPOCH 7 - PROGRESS: at 91.62% examples, 512480 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:30,955 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:13:30,975 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:13:30,977 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:13:30,986 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:13:30,998 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:13:31,001 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:13:31,002 : INFO :  EPOCH - 7 : training on 5844706 raw words (3928891 effective words) took 7.6s, 516246 effective words/s\n",
      "2020-10-21 07:13:32,010 : INFO :  EPOCH 8 - PROGRESS: at 13.02% examples, 512324 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:33,021 : INFO :  EPOCH 8 - PROGRESS: at 26.42% examples, 519511 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:34,053 : INFO :  EPOCH 8 - PROGRESS: at 38.96% examples, 506699 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:35,062 : INFO :  EPOCH 8 - PROGRESS: at 51.98% examples, 507461 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:36,079 : INFO :  EPOCH 8 - PROGRESS: at 64.67% examples, 503504 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:37,095 : INFO :  EPOCH 8 - PROGRESS: at 77.95% examples, 505207 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:38,096 : INFO :  EPOCH 8 - PROGRESS: at 91.28% examples, 507407 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:38,661 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:13:38,678 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:13:38,692 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:13:38,712 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:13:38,713 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:13:38,713 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:13:38,713 : INFO :  EPOCH - 8 : training on 5844706 raw words (3930650 effective words) took 7.7s, 509883 effective words/s\n",
      "2020-10-21 07:13:39,721 : INFO :  EPOCH 9 - PROGRESS: at 11.87% examples, 466022 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:40,727 : INFO :  EPOCH 9 - PROGRESS: at 24.97% examples, 491637 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:41,730 : INFO :  EPOCH 9 - PROGRESS: at 37.40% examples, 492686 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:42,736 : INFO :  EPOCH 9 - PROGRESS: at 51.15% examples, 503687 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:43,751 : INFO :  EPOCH 9 - PROGRESS: at 64.69% examples, 507375 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:44,772 : INFO :  EPOCH 9 - PROGRESS: at 77.95% examples, 508015 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:45,776 : INFO :  EPOCH 9 - PROGRESS: at 90.40% examples, 504891 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:46,403 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:13:46,427 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:13:46,435 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:13:46,439 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:13:46,444 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:13:46,450 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:13:46,451 : INFO :  EPOCH - 9 : training on 5844706 raw words (3930185 effective words) took 7.7s, 508145 effective words/s\n",
      "2020-10-21 07:13:47,462 : INFO :  EPOCH 10 - PROGRESS: at 12.86% examples, 504676 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:48,480 : INFO :  EPOCH 10 - PROGRESS: at 26.38% examples, 517164 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:49,485 : INFO :  EPOCH 10 - PROGRESS: at 40.36% examples, 527100 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:50,486 : INFO :  EPOCH 10 - PROGRESS: at 53.19% examples, 523638 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:51,501 : INFO :  EPOCH 10 - PROGRESS: at 66.97% examples, 524484 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:52,510 : INFO :  EPOCH 10 - PROGRESS: at 80.92% examples, 526455 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:53,536 : INFO :  EPOCH 10 - PROGRESS: at 94.79% examples, 526825 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-21 07:13:53,833 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-21 07:13:53,839 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-21 07:13:53,842 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-21 07:13:53,850 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-21 07:13:53,862 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-21 07:13:53,871 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-21 07:13:53,872 : INFO :  EPOCH - 10 : training on 5844706 raw words (3930548 effective words) took 7.4s, 529790 effective words/s\n",
      "2020-10-21 07:13:53,872 : INFO :  training on a 58447060 raw words (39297601 effective words) took 75.3s, 521902 effective words/s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# https://wikidocs.net/50739\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers = num_workers,\n",
    "                          size = num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window =  context,\n",
    "                          sample = downsampling,\n",
    "                          iter = 10,\n",
    "                          sg =0 # sg =0 CBOW, 1 : skip-gram\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 07:13:53,883 : INFO :  precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('man,', 0.716978907585144),\n",
       " ('woman', 0.6902350187301636),\n",
       " ('doctor', 0.647634744644165),\n",
       " ('soldier', 0.6302933692932129),\n",
       " ('boy', 0.6176154017448425),\n",
       " ('lady', 0.6128588914871216),\n",
       " ('man.', 0.5945290327072144),\n",
       " ('patient', 0.5821107625961304),\n",
       " ('woman,', 0.5750539302825928),\n",
       " (\"man's\", 0.5717611312866211)]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# man과 가장 유사한 단어 골라내기 \n",
    "model.wv.most_similar(\"man\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# word2vec 은 단어 하나하나가 벡터로 표현되어 있다.\n",
    "# Review 데이터는 단어들의 조합이기에 Review를 벡터로 표현하기 위해\n",
    "# Review에 포함된 단어 벡터들의 평균값을 만든다.\n",
    "# 다른 방법으로는 Doc2vec, average of word2vec vectors with TF-IDF\n",
    "# Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector.\n",
    " # 단어 벡터에 TF-IDF를 곱해서 평균 내는 방법\n",
    "    \n",
    "# https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence\n",
    "\n",
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype = np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘 사전\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words +=1\n",
    "            #사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            \n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            # model은 단어들에 대한 vector를 다 가지고 있음\n",
    "            # num_features 만큼 이미 학습할때 정의해서 만들어놓음 \n",
    "            \n",
    "    feature_vector = np.divide(feature_vector,num_words)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    \n",
    "    for s in reviews :\n",
    "        dataset.append(get_features(s,model,num_features))\n",
    "    \n",
    "    reviewFeaturevecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeaturevecs\n",
    "\n",
    "word2vec_train = get_dataset(sentences,model,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.18139751e-01,  6.28871098e-02,  1.49180561e-01, -1.26731306e-01,\n",
       "        5.12885191e-02,  1.32029548e-01, -3.20467204e-02, -1.41882464e-01,\n",
       "       -1.59057543e-01, -1.76634882e-02,  2.67821364e-02,  1.29582128e-02,\n",
       "        1.50080062e-02,  5.48530258e-02, -3.05359736e-02, -7.36697763e-02,\n",
       "       -2.04020426e-01, -3.95200215e-02, -1.54978139e-02,  5.88751631e-03,\n",
       "        8.21877643e-02,  1.17347911e-01,  3.05978972e-02, -8.75424780e-03,\n",
       "       -3.54981497e-02, -1.51894391e-01, -7.06436113e-02,  1.31319031e-01,\n",
       "       -4.01055254e-02, -3.94649893e-01, -1.29856095e-02,  5.32002859e-02,\n",
       "       -6.21244013e-02, -7.38168210e-02, -1.01046986e-03,  1.91170737e-01,\n",
       "       -2.61195868e-01, -1.95050910e-01,  5.21635175e-01, -1.45756811e-01,\n",
       "       -2.58524239e-01,  1.35558635e-01, -1.34425178e-01,  9.70060900e-02,\n",
       "       -2.55309343e-01,  2.10041985e-01, -7.42806494e-02, -1.94924381e-02,\n",
       "       -2.27575526e-02,  7.24768415e-02,  4.58092019e-02, -2.66585886e-01,\n",
       "       -2.17871964e-01, -5.41810282e-02, -8.27785432e-02,  2.09950924e-01,\n",
       "        4.58673835e-02, -1.16571203e-01,  2.09350511e-02,  1.07247651e-01,\n",
       "       -8.10470246e-03,  1.94456195e-03,  8.31535384e-02, -1.29018039e-01,\n",
       "       -1.59806106e-02, -1.65734477e-02,  5.06074466e-02,  1.15954317e-01,\n",
       "       -4.45237011e-02, -3.07026822e-02, -7.74073079e-02,  5.67464158e-03,\n",
       "        6.37790712e-04, -1.35428309e-01, -6.26993254e-02,  2.96816137e-02,\n",
       "        1.72259212e-01,  1.47827938e-02,  1.71460453e-02,  3.39633413e-02,\n",
       "       -2.55055204e-02,  1.09444767e-01,  4.80365288e-03, -6.13430515e-03,\n",
       "       -5.99827990e-03,  1.71255082e-01,  2.20813632e-01, -7.64669105e-02,\n",
       "        5.37310578e-02,  6.11457862e-02,  2.81814653e-02,  8.64059329e-02,\n",
       "       -2.71509439e-02,  1.04100592e-01,  4.14787009e-02,  1.15637323e-02,\n",
       "       -1.32612512e-02,  1.14744036e-02, -1.53297454e-01,  9.37027037e-02,\n",
       "        3.89847048e-02,  1.65969809e-03, -3.92607935e-02,  1.58370053e-03,\n",
       "       -7.31502920e-02,  8.56427997e-02,  1.04882903e-01,  9.78371203e-02,\n",
       "       -9.00720730e-02,  4.87672202e-02, -1.38837293e-01, -1.71774030e-01,\n",
       "        1.11009754e-01,  1.75507769e-01, -5.01149520e-02, -1.19048804e-01,\n",
       "        5.39332554e-02,  5.40702268e-02,  2.28318319e-01,  2.82127466e-02,\n",
       "       -4.78286520e-02, -1.32308066e-01,  4.35655788e-02, -2.31109206e-02,\n",
       "        6.91473037e-02,  1.52271301e-01,  4.20387648e-02,  5.53817898e-02,\n",
       "       -1.30688757e-01, -3.97818647e-02, -1.31353540e-02,  1.17777891e-01,\n",
       "        8.49183202e-02,  6.57182112e-02,  8.67167041e-02, -3.76492366e-02,\n",
       "       -2.78118048e-02,  7.28911310e-02, -3.21368547e-03,  7.46088028e-02,\n",
       "       -1.28299981e-01,  4.17304710e-02, -1.59718841e-01, -2.26813212e-01,\n",
       "       -1.83522806e-01,  9.07303300e-03, -5.88025637e-02, -5.69890402e-02,\n",
       "        9.77279395e-02, -7.53569975e-02, -3.04197753e-03,  1.57754302e-01,\n",
       "        5.01189195e-03,  1.46487102e-01,  3.84707041e-02,  4.06134455e-03,\n",
       "        4.44930308e-02,  3.40173654e-02,  7.60189891e-02,  3.44042592e-02,\n",
       "       -1.82432935e-01, -1.70982804e-03, -3.01153585e-02, -4.05949503e-02,\n",
       "       -1.18437856e-02, -2.61756703e-02,  1.22849308e-02, -9.46601778e-02,\n",
       "        7.85357505e-02, -5.82247488e-02, -1.09393112e-02, -7.04373121e-02,\n",
       "        8.33741799e-02,  2.09585205e-02, -2.52012879e-01, -1.34436801e-01,\n",
       "       -4.78213876e-02,  3.83104794e-02, -8.87345895e-02, -1.89629719e-01,\n",
       "       -4.07735594e-02,  6.47359043e-02,  1.77192539e-02, -3.39710526e-02,\n",
       "        2.13706240e-01, -4.86821570e-02, -7.24052787e-02, -5.97184617e-03,\n",
       "        6.03090376e-02, -5.56377023e-02,  4.14836891e-02, -3.01478580e-02,\n",
       "       -1.79206401e-01, -2.04214618e-01, -1.80522829e-01, -9.80010703e-02,\n",
       "        3.31320837e-02, -1.19912550e-01, -4.98869866e-02,  1.13088332e-01,\n",
       "       -6.09689951e-02,  3.21569555e-02,  2.76044682e-02, -1.54710010e-01,\n",
       "        6.38868287e-02,  1.06465677e-02, -4.50008214e-02, -1.53944209e-01,\n",
       "        1.67412534e-01,  8.79186094e-02, -2.22492397e-01,  1.85264144e-02,\n",
       "        1.08715147e-01,  2.23904494e-02,  3.03978063e-02, -2.09111437e-01,\n",
       "        7.45220929e-02, -1.50240973e-01, -4.06820104e-02, -6.76103830e-02,\n",
       "        4.34351228e-02, -6.39077723e-02, -3.90658043e-02, -1.82177871e-02,\n",
       "        3.49371098e-02,  5.61677143e-02,  1.08407661e-01,  3.34148109e-02,\n",
       "       -2.67913379e-03, -5.95358200e-03, -1.67206749e-01, -6.49970993e-02,\n",
       "       -1.29189745e-01,  2.86834724e-02,  5.07525094e-02, -8.20703581e-02,\n",
       "        6.07158467e-02,  1.29673779e-01,  1.49054294e-02, -7.67228082e-02,\n",
       "        2.70808842e-02,  3.04452553e-02, -5.53988405e-02, -9.18590501e-02,\n",
       "        2.87616868e-02,  1.69632360e-02,  1.03833666e-03, -5.72413765e-02,\n",
       "        8.67995061e-03, -2.91422814e-01, -2.15322331e-01, -2.58838028e-01,\n",
       "       -1.88075490e-02, -7.55417496e-02, -4.33261842e-02, -2.80386239e-01,\n",
       "       -1.32872924e-01,  2.65843160e-02,  1.29547834e-01, -5.57539379e-03,\n",
       "        4.16669343e-03, -2.61621904e-02, -4.02405970e-02, -1.30531555e-02,\n",
       "       -5.03172539e-03, -4.17123064e-02, -5.69778830e-02, -3.89224961e-02,\n",
       "        5.63693978e-02, -5.84104322e-02,  2.91649587e-02,  9.69307497e-02,\n",
       "        7.79907107e-02,  1.15624465e-01, -1.43852562e-01, -1.51046868e-02,\n",
       "        1.70025472e-02, -1.06556434e-02, -1.94397382e-02, -5.62854297e-02,\n",
       "        9.44724455e-02,  1.52845711e-01, -8.96007121e-02,  1.54978223e-02,\n",
       "        7.73989931e-02,  2.50303835e-01,  8.97641182e-02,  5.32002039e-02,\n",
       "       -1.07074417e-01, -1.11477166e-01,  4.08990711e-01,  1.78552698e-02,\n",
       "       -3.02735642e-02, -1.41273543e-01,  5.35139777e-02,  1.69617206e-01,\n",
       "       -1.54117001e-02, -4.72950153e-02,  2.90731937e-02,  1.38488319e-02,\n",
       "        2.14535724e-02, -1.36814207e-01,  6.31136149e-02, -9.66744870e-02,\n",
       "        6.80122823e-02,  4.05051671e-02,  1.19924068e-03, -1.18750058e-01,\n",
       "       -1.17079400e-01, -5.92293479e-02,  2.43000593e-02, -5.23420796e-03,\n",
       "       -1.92758933e-01, -1.68687448e-01, -1.42199159e-01,  9.88979451e-03,\n",
       "        9.68158916e-02, -5.60951419e-02,  9.24074650e-02,  7.38357231e-02,\n",
       "       -2.11842135e-01, -6.50239810e-02,  1.36769563e-01, -3.27619582e-01,\n",
       "       -5.17669357e-02,  1.36889309e-01,  1.85850635e-01, -2.70664468e-02,\n",
       "       -3.77453446e-01,  1.45140424e-01, -7.18524531e-02, -1.24106534e-01,\n",
       "       -4.11320180e-02,  2.06445113e-01, -3.45367342e-01, -2.54448771e-01,\n",
       "        3.23644094e-02, -1.20104760e-01, -2.35677883e-01,  1.87695161e-01,\n",
       "        1.30887464e-01,  1.35783806e-01,  8.40048715e-02, -1.63396150e-01,\n",
       "       -6.63373843e-02, -4.35652968e-04,  6.23995028e-02, -4.71864305e-02,\n",
       "        9.27439425e-03,  5.37737757e-02, -8.78980458e-02,  5.16843162e-02,\n",
       "        7.60688707e-02, -3.61156859e-03,  1.58455886e-03,  1.63864210e-01,\n",
       "       -1.01582892e-02, -8.91377777e-02, -1.49371147e-01, -8.14263970e-02,\n",
       "       -1.27440151e-02, -2.22982410e-02,  9.19369143e-03, -4.62605059e-02,\n",
       "        3.46909203e-02,  3.71821672e-02,  2.19101422e-02, -1.89382471e-02,\n",
       "        3.07677370e-02, -4.58794273e-02, -2.53987256e-02,  4.31007054e-03,\n",
       "        7.06791878e-02, -4.72281463e-02, -1.75891258e-02,  1.09360136e-01,\n",
       "        7.05803633e-02, -2.23452915e-02, -1.17491253e-01, -8.03592801e-02,\n",
       "        4.69785295e-02,  7.51019567e-02, -7.24377930e-02,  5.85697219e-02,\n",
       "        2.29493249e-02,  3.75185609e-02, -1.15713820e-01,  2.96503138e-02,\n",
       "       -9.45095792e-02,  2.28955038e-02,  1.18752331e-01, -5.74920550e-02,\n",
       "       -2.13415459e-01,  1.89643800e-01, -2.76801120e-02,  2.52968039e-05,\n",
       "       -1.08075507e-01,  1.80234775e-01,  1.43031418e-01,  2.23897700e-03,\n",
       "       -3.57331559e-02, -5.07948212e-02,  3.37441042e-02,  2.16734614e-02,\n",
       "       -1.03833629e-02,  6.46913648e-02, -1.58581197e-01,  1.50497913e-01,\n",
       "       -1.88799396e-01, -4.86179581e-03, -2.25473251e-02,  3.98977362e-02,\n",
       "       -7.31329769e-02, -4.65873666e-02, -4.10198085e-02,  2.08778754e-01,\n",
       "        3.27177942e-02, -1.66435614e-01,  7.35085383e-02, -1.18636161e-01,\n",
       "        1.51636019e-01, -1.23352893e-01,  6.77148551e-02, -6.18767971e-03,\n",
       "        1.45917386e-01, -1.60338119e-01,  4.14682664e-02, -2.28964686e-02,\n",
       "        6.08525798e-02, -2.82965433e-02, -5.82854263e-02, -4.61237319e-02,\n",
       "        1.44810900e-01,  8.42389241e-02,  1.12822428e-01, -1.61795206e-02,\n",
       "       -1.29842058e-01,  1.03229787e-02, -6.98090717e-02, -8.44129026e-02,\n",
       "        9.73225615e-05, -9.55937654e-02, -4.51275893e-02, -1.42751530e-01,\n",
       "       -3.18761989e-02,  6.75764540e-03,  1.87719986e-01,  4.61839885e-02,\n",
       "        1.33039773e-01, -3.77384782e-01,  1.11949958e-01, -2.63373554e-03,\n",
       "        1.55118972e-01, -2.30418891e-03,  9.25245695e-03,  9.38912705e-02,\n",
       "       -1.66332144e-02, -9.27296132e-02,  2.06349820e-01,  6.98752925e-02,\n",
       "        7.58565217e-02, -1.75172895e-01,  2.95599122e-02,  1.13383628e-01,\n",
       "        2.77329125e-02,  3.06165777e-04, -1.11082373e-02, -6.50575608e-02,\n",
       "        1.78842954e-02,  2.02183668e-02, -6.86840266e-02,  1.12102576e-01,\n",
       "        4.66273800e-02,  5.61243519e-02,  1.00027107e-01,  9.66025889e-02,\n",
       "       -4.57961410e-02, -2.72446983e-02,  2.67953407e-02, -8.17586780e-02,\n",
       "       -1.32850572e-01,  1.90660674e-02, -3.85583453e-02,  1.78927854e-01,\n",
       "        5.14264703e-02, -2.02879399e-01,  3.22190523e-02, -1.99294999e-01,\n",
       "       -5.41615672e-02,  2.34253164e-02,  6.37695268e-02, -8.36855918e-02,\n",
       "        7.34657869e-02, -7.28370100e-02,  2.98203086e-04,  1.09681394e-02,\n",
       "        1.16902284e-01,  2.88543440e-02, -6.20579720e-03, -7.55064040e-02,\n",
       "        1.34127289e-01,  1.26139343e-01, -2.75581516e-02, -5.61734065e-02,\n",
       "        1.13186650e-01, -6.28593937e-02,  8.96125361e-02, -9.37986597e-02,\n",
       "        4.31877002e-02, -9.93405879e-02,  2.46774778e-01,  1.16808265e-02,\n",
       "       -1.35694537e-02,  1.28516406e-01,  4.28975932e-02, -7.14008734e-02,\n",
       "       -2.82520875e-02, -9.20202117e-03,  6.45950586e-02,  4.66711521e-02,\n",
       "       -1.23894718e-02, -4.20506522e-02,  8.04234296e-02, -2.54250746e-02,\n",
       "       -6.61708936e-02, -6.89828768e-02,  2.24700104e-02,  8.40778947e-02,\n",
       "        8.93401727e-02,  1.25260577e-01, -2.09980041e-01, -1.46730334e-01,\n",
       "        2.17174692e-03,  2.97221970e-02,  8.28437433e-02, -1.41071439e-01,\n",
       "        8.97790119e-02,  7.05835298e-02,  2.14409940e-02,  2.83385366e-01,\n",
       "       -7.59786135e-03,  1.71445414e-01,  2.48616368e-01, -8.53813291e-02,\n",
       "        1.40113384e-01,  6.68939063e-03,  1.75293192e-01, -4.12198514e-01,\n",
       "        2.00640380e-01,  1.40588805e-01,  1.08300693e-01,  7.82692507e-02,\n",
       "        1.25613296e-02, -3.98738593e-01, -1.78296953e-01,  5.50824478e-02,\n",
       "        3.18659879e-02, -2.37083957e-01,  4.87871394e-02, -1.42658740e-01,\n",
       "        1.78330373e-02,  1.01497285e-01, -9.22951847e-02, -3.41047645e-02,\n",
       "       -7.94245973e-02,  1.15898393e-01, -9.65559557e-02,  1.83978304e-02,\n",
       "       -3.60733010e-02,  4.89383899e-02,  3.08213890e-01, -1.44667000e-01,\n",
       "       -1.33771151e-01,  2.55047232e-01, -1.27726033e-01,  5.15897274e-02,\n",
       "        4.75711785e-02,  4.58147377e-03,  4.08667848e-02, -8.29702392e-02,\n",
       "        1.12672336e-01, -7.74761960e-02, -1.37855252e-02, -9.34068672e-03,\n",
       "        1.40306950e-01,  8.25562105e-02, -6.38187677e-02,  5.11261858e-02,\n",
       "        6.38634786e-02, -2.73321159e-02,  9.21034813e-02,  7.83675909e-02,\n",
       "       -3.25331502e-02,  5.14406487e-02,  8.63899142e-02, -8.49827677e-02,\n",
       "       -1.74890980e-01,  9.71274301e-02,  1.02524862e-01, -4.33213897e-02,\n",
       "       -5.72618023e-02,  8.83836416e-04, -9.76316035e-02, -7.98234567e-02,\n",
       "       -1.76283158e-02,  1.21763803e-01, -1.22707166e-01, -4.48792726e-02,\n",
       "        1.35382280e-01, -9.75892097e-02, -8.66304245e-03,  6.84020594e-02,\n",
       "       -3.31881158e-02, -2.32136212e-02, -3.10339052e-02, -2.29662042e-02,\n",
       "        1.94527745e-01, -4.01744582e-02,  1.46117033e-02, -3.37737687e-02,\n",
       "       -5.19918501e-02,  7.59357363e-02, -6.42411411e-02,  1.49710048e-02,\n",
       "        1.35269672e-01, -5.05066887e-02,  2.21285354e-02,  1.35266721e-01,\n",
       "       -2.95625962e-02,  2.46039465e-01,  6.88086972e-02, -2.10964218e-01,\n",
       "        2.15669170e-01,  9.33328122e-02,  1.48197617e-02, -1.09701902e-01,\n",
       "        5.97747006e-02,  3.69879931e-01,  1.21171504e-01,  1.41033530e-01,\n",
       "        3.06166768e-01, -6.53695688e-02,  2.21469849e-02,  1.30712658e-01,\n",
       "       -1.34723663e-01,  1.51186973e-01, -2.82192439e-01,  1.13188490e-01,\n",
       "        7.51877297e-03,  2.45179951e-01, -3.80718894e-02, -9.66342092e-02,\n",
       "       -6.87339827e-02, -5.73507883e-02,  1.43169751e-03, -8.32999067e-04,\n",
       "       -2.47480981e-02,  8.17526598e-03,  7.60905594e-02,  1.53099652e-02,\n",
       "        1.53464368e-02,  4.81040254e-02, -9.10531208e-02, -1.16202384e-01,\n",
       "       -5.09245358e-02,  1.80561870e-01, -1.76925305e-02, -1.99334055e-01,\n",
       "       -8.29821974e-02,  4.47767861e-02, -1.82743371e-01, -5.27747646e-02,\n",
       "       -8.74895081e-02, -1.25589564e-01, -7.70254061e-02, -8.95119756e-02,\n",
       "        3.64855267e-02,  4.75469455e-02, -1.47966102e-01, -3.25738378e-02,\n",
       "        5.03645055e-02, -1.22593557e-02, -1.38495892e-01,  1.68399662e-01,\n",
       "        7.87091404e-02, -5.20055965e-02,  1.56887248e-01, -1.31873965e-01,\n",
       "        2.05763131e-01, -2.41413817e-01,  1.76792610e-02,  1.30224019e-01,\n",
       "       -3.75708230e-02, -2.07659245e-01,  7.39851445e-02,  5.34299575e-03,\n",
       "       -7.92369694e-02,  9.78683680e-02,  2.17785202e-02, -9.27346572e-02,\n",
       "       -8.03756341e-02, -9.66871753e-02,  2.22319718e-02,  6.20981157e-02,\n",
       "       -1.55936494e-01, -8.74699056e-02,  3.67150903e-02,  1.07849441e-01,\n",
       "        5.69210872e-02, -7.97134340e-02, -1.75631896e-01,  2.53815558e-02,\n",
       "        1.40445828e-01,  1.33925781e-01,  3.11210863e-02, -1.95935011e-01,\n",
       "        1.11201830e-01,  7.36090839e-02, -7.90715292e-02,  1.57936271e-02,\n",
       "        1.46037564e-01,  9.13779661e-02,  2.57790446e-01, -1.59511846e-02,\n",
       "        2.34668963e-02,  3.20313275e-02, -1.31137401e-01,  9.21932384e-02,\n",
       "        1.41407892e-01, -2.09008217e-01, -2.71571904e-01, -1.70318678e-01,\n",
       "        2.89930943e-02,  6.75648972e-02,  1.29589602e-01, -7.89619684e-02,\n",
       "        8.43878314e-02,  1.24044724e-01,  9.66942161e-02,  1.30781025e-01,\n",
       "        7.95367290e-04,  6.95614144e-02, -1.37151763e-01, -7.83547685e-02,\n",
       "        2.85252389e-02, -1.54068381e-01, -7.61099532e-03,  5.49977869e-02,\n",
       "       -6.64143488e-02,  5.88993393e-02,  5.02678528e-02,  3.18699260e-03,\n",
       "        7.88980350e-02, -1.73920363e-01, -5.39520420e-02,  2.80861575e-02,\n",
       "       -2.10445583e-01,  9.13370103e-02, -3.73701565e-02, -7.29499608e-02,\n",
       "       -1.32626981e-01,  1.58199832e-01, -9.67241004e-02,  1.48831725e-01,\n",
       "        1.36321234e-02, -2.31256746e-02,  2.39934683e-01,  7.49785006e-02,\n",
       "       -1.10848755e-01, -8.37839916e-02, -1.66798517e-01,  1.84964508e-01,\n",
       "       -3.47021699e-01,  2.49237958e-02,  1.62937388e-01, -8.88500363e-02,\n",
       "        1.46348655e-01, -1.86662808e-01,  1.76255882e-01, -3.15361582e-02,\n",
       "        5.14444783e-02, -6.73367754e-02,  4.67782021e-02, -6.78530633e-02,\n",
       "        8.54808930e-03,  4.16822592e-03,  1.57695711e-02, -2.60016951e-03,\n",
       "       -2.73914449e-02,  2.54572481e-02,  3.58565571e-03, -1.51182294e-01,\n",
       "        2.42561594e-01,  2.46293638e-02, -1.28265629e-02,  7.90418312e-02,\n",
       "        1.78977445e-01, -1.57999650e-01,  8.20461735e-02, -1.21657372e-01,\n",
       "       -7.12183788e-02,  2.45236536e-03, -3.27672929e-01, -3.75281781e-01,\n",
       "       -3.02072316e-01, -1.90487295e-01,  1.85198978e-01,  1.51427686e-02,\n",
       "        7.89392814e-02, -1.92087337e-01, -6.13470338e-02, -3.73244099e-02,\n",
       "        4.50795107e-02,  3.66627984e-02,  3.13179903e-02, -1.14959560e-01,\n",
       "        1.03290817e-02, -1.39554769e-01,  4.85852137e-02,  1.87051587e-03,\n",
       "       -1.52147459e-02,  1.80002689e-01, -5.57100438e-02,  1.87728442e-02,\n",
       "        2.69923843e-02, -6.35401756e-02,  3.40375341e-02, -9.16458741e-02,\n",
       "       -1.85326242e-03,  1.34904936e-01, -5.31002767e-02,  1.69875681e-01,\n",
       "        1.12798728e-01,  6.59195408e-02,  1.30982362e-02,  5.25785983e-02,\n",
       "       -1.36454487e-02, -1.25328586e-01, -1.79845482e-01, -5.01737222e-02,\n",
       "        1.02647906e-02,  1.16428278e-01,  4.91873547e-02,  9.16216001e-02,\n",
       "        1.11975968e-01, -1.47190839e-02, -3.59395258e-02,  7.84236845e-03,\n",
       "       -9.45626721e-02, -1.14768287e-02,  2.88256910e-02, -9.85124409e-02,\n",
       "       -1.13270231e-01,  5.71063310e-02, -4.60140556e-02,  2.88388450e-02,\n",
       "       -1.23342231e-01,  7.00633004e-02,  1.41462535e-01,  6.26410618e-02,\n",
       "        7.93616399e-02, -6.57516997e-03,  1.70428399e-02,  1.07217409e-01,\n",
       "        7.55436867e-02, -9.28258225e-02, -1.19034111e-01,  3.13727511e-03,\n",
       "       -3.66248451e-02, -1.26144126e-01, -9.04978812e-02,  6.64626956e-02,\n",
       "       -1.03375830e-01,  2.01268401e-02, -9.73517373e-02,  1.34730607e-01,\n",
       "       -9.14453715e-02,  3.43082137e-02, -5.90038411e-02, -3.20870839e-02,\n",
       "        7.18446076e-02, -1.73532509e-03, -1.14654079e-01,  2.60455996e-01,\n",
       "       -1.71136763e-02, -6.02888688e-02, -4.01345752e-02,  2.19862103e-01,\n",
       "        9.30582881e-02,  1.71365485e-01, -5.55896200e-02,  1.36366025e-01,\n",
       "        6.63373023e-02, -8.53873193e-02,  1.19931705e-01, -8.45746249e-02,\n",
       "       -1.01314776e-01, -2.10943922e-01, -2.99619157e-02,  1.71741664e-01,\n",
       "        6.99572787e-02,  2.88623631e-01,  1.25814468e-01, -3.31766978e-02,\n",
       "       -3.10551282e-02,  1.84746966e-01, -1.16873570e-01,  6.91612959e-02,\n",
       "        3.96750681e-02, -7.37590194e-02, -1.25321168e-02,  7.17826653e-03,\n",
       "        8.93107802e-02,  1.24887191e-01, -1.28574014e-01, -3.35445851e-02,\n",
       "       -8.77845138e-02, -1.43670902e-01, -1.97097138e-01,  9.81717259e-02,\n",
       "        2.02536672e-01,  2.37418711e-03,  6.23438768e-02, -7.79982135e-02,\n",
       "       -1.61053419e-01,  1.40258878e-01, -1.97160199e-01,  6.49530664e-02,\n",
       "       -1.13522373e-01,  2.27970943e-01, -1.19329095e-02, -1.77401714e-02,\n",
       "        1.79793656e-01, -5.30431047e-02, -2.31265262e-01, -4.98957150e-02,\n",
       "       -3.05673063e-01, -2.58225530e-01, -9.63019021e-03, -6.99902475e-02,\n",
       "       -5.88825196e-02,  1.13523312e-01, -1.76218227e-01,  4.16503800e-03,\n",
       "       -1.71433210e-01,  2.34467864e-01,  7.69823566e-02, -1.24605618e-01,\n",
       "        1.80668086e-01,  2.84288526e-01, -1.07453115e-01,  4.86976616e-02,\n",
       "        1.51847914e-01,  4.36029509e-02,  5.82251400e-02, -6.33187667e-02,\n",
       "        7.79354945e-02,  4.04631570e-02, -7.03534260e-02, -6.61412552e-02,\n",
       "        1.99959666e-01, -7.62452781e-02,  8.65124017e-02, -3.65361534e-02,\n",
       "       -5.74810728e-02,  9.05933231e-02,  3.86384651e-02,  8.61045197e-02,\n",
       "       -1.71405315e-01, -2.74300188e-01, -1.72457367e-01,  1.60608456e-01,\n",
       "       -6.44102618e-02,  1.39445132e-02,  5.04152365e-02,  1.18782409e-01,\n",
       "       -2.22230718e-01, -1.42896071e-01, -1.49250887e-02,  1.04953289e-01,\n",
       "       -2.62299746e-01, -1.46573529e-01, -6.91911057e-02,  6.21866472e-02,\n",
       "       -6.34805560e-02, -3.42597999e-02,  8.53086337e-02,  2.64558103e-03,\n",
       "       -3.55856121e-02, -2.16037929e-02,  3.21481675e-02,  5.58045059e-02,\n",
       "       -8.46478343e-02, -1.20076194e-01, -9.37650576e-02, -1.53899729e-01,\n",
       "        7.94564486e-02,  4.53535980e-03, -6.02457933e-02,  1.14558607e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.837200\n"
     ]
    }
   ],
   "source": [
    "# Count_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(count_train.toarray(),Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.844600\n"
     ]
    }
   ],
   "source": [
    "# tfidf_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(tfidf_train.toarray(),Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.816600\n"
     ]
    }
   ],
   "source": [
    "# word2vec_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(word2vec_train,Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "BATCH_SIZE = 128\n",
    "epochs = 10\n",
    "valid = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popcorn",
   "language": "python",
   "name": "popcorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
