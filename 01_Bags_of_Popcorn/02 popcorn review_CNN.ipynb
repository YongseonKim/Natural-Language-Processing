{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "tf.random.set_seed(99)\n",
    "BATCH_SIZE = 128\n",
    "epochs = 10\n",
    "valid = 0.2\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "# -- quoting : 특수 문자가 포함된 필드를 감쌀 때 처리하는 방법, 문자를 따옴표로 묶는 방법\n",
    "import csv\n",
    "df = pd.read_csv('data/labeledTrainData.tsv', header=0, delimiter='\\t',quoting=3)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts 20 minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/labeledTrainData.tsv',header =0, delimiter = '\\t', quoting =3)\n",
    "\n",
    "def preprocessing(review, remove_stopwords = False):\n",
    "    review_text = BeautifulSoup(review,'html.parser').get_text()\n",
    "    \n",
    "    # 특수문자 제거 # 영문자,숫자를 제외한 문자를 모드 변환 띄어쓰기로\n",
    "    review_text = re.sub(\"\\W\",\" \",review_text)    \n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        clean_review = ' '.join(words)\n",
    "        \n",
    "    else :\n",
    "        clean_review = ' '.join(words)\n",
    "    \n",
    "    return clean_review\n",
    "\n",
    "clean_train_reviews = []\n",
    "\n",
    "for review in train_data['review']:\n",
    "    clean_train_reviews.append(preprocessing(review, remove_stopwords=True))\n",
    "\n",
    "clean_train_reviews[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[410, 71, 425, 8957, 512, 2483, 116, 54, 882, 522, 179, 18946, 179, 11409, 167, 79, 14, 669, 2484, 118, 93, 10, 505, 4131, 167, 22, 213, 589, 2361, 1205, 11409, 72, 4896, 72, 641, 2, 257, 71, 11, 306, 1683, 492, 1157, 3309, 8957, 417, 800, 3387, 17, 447, 607, 1516, 15, 4528, 1875, 1010, 148, 348, 1455, 750, 2452, 4, 8957, 424, 71, 643, 70, 241, 95, 547, 8957, 26374, 26375, 121, 1, 8957, 327, 8, 47, 20, 327, 169, 10, 210, 638, 641, 2, 117, 295, 388, 733, 124, 15761, 3361, 1517, 582, 741, 10167, 934, 11746, 829, 1251, 1423, 366, 8957, 225, 15, 584, 8957, 22505, 2300, 13630, 741, 10167, 27, 28950, 346, 16, 41, 18947, 1516, 394, 11410, 167, 4018, 8957, 116, 633, 505, 80, 4, 8957, 1444, 386, 2193, 115, 1943, 2529, 582, 17, 60, 101, 4947, 5239, 264, 1280, 26376, 15, 582, 498, 751, 643, 637, 3, 400, 166, 452, 115, 622, 3310, 1172, 690, 48, 1190, 228, 1, 16, 4, 8957, 3, 513, 62, 25, 16, 646, 135, 235, 96, 7553, 607, 3486, 8957, 37737, 1888, 1, 130, 348, 1455, 251, 3, 874, 16, 42, 1502, 1009, 2361, 12, 555, 392, 723, 7037, 12, 41, 16, 160, 368, 4456, 3432, 41, 88, 229, 444, 210, 258, 118, 3, 18948, 18949, 320, 1372]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_reviews)\n",
    "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
    "print(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = tokenizer.word_index\n",
    "word_vocab[\"<PAD>\"]=0 \n",
    "# print(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75684"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_configs = {}\n",
    "data_configs['vocab']= word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab)+1\n",
    "data_configs['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 174)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 174\n",
    "\n",
    "train_inputs =  pad_sequences(text_sequences,maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print(train_inputs.shape)\n",
    "train_labels = np.array(train_data['sentiment'])\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kargs = {\n",
    "    'vocab_size' : data_configs['vocab_size'],\n",
    "    'embedding_size' : 128,\n",
    "    'num_filters' : 100,\n",
    "    'dropout_rate' : 0.5,\n",
    "    'hidden_dimension' : 250,\n",
    "    'output_dimension' : 1\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,**kargs):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = kargs['vocab_size'],\n",
    "                                            output_dim = kargs['embedding_size'])\n",
    "        \n",
    "        self.conv_list = [tf.keras.layers.Conv1D(filters = kargs['num_filters'],\n",
    "                                       kernel_size = kernel_size,\n",
    "                                       padding ='valid',\n",
    "                                    activation = tf.keras.activations.relu,\n",
    "                                        kernel_constraint = tf.keras.constraints.MaxNorm(max_value=3.)                                       \n",
    "                                       )\n",
    "                             for kernel_size in [3,4,5]]\n",
    "        self.pooling = tf.keras.layers.GlobalMaxPool1D()\n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['dropout_rate'])\n",
    "        self.fc1 = tf.keras.layers.Dense(units = kargs['hidden_dimension'],\n",
    "                                        activation = tf.keras.activations.relu,\n",
    "                                         # 가중치가 너무 커지는것을 방지\n",
    "                                         kernel_constraint = tf.keras.constraints.MaxNorm(max_value = 3.)                                        \n",
    "                                        )\n",
    "        self.fc2 = tf.keras.layers.Dense(units=kargs['output_dimension'],\n",
    "                                        activation = tf.keras.activations.sigmoid,\n",
    "                                         kernel_constraint = tf.keras.constraints.MaxNorm(max_value=3.)\n",
    "                                        )\n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list] , axis =1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(**kargs)\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(1e-4),\n",
    "             loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics= [tf.keras.metrics.BinaryAccuracy(name = \"accuracy\")]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_out/rnn_classifier - exists\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# earlystopping \n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy', min_delta=0.0001, patience=2)\n",
    "model_name = 'rnn_classifier'\n",
    "checkpoint_path = './data_out/{}/weight2.h5'.format(model_name)\n",
    "checkpoint_dir =  os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} - exists\".format(checkpoint_dir))\n",
    "else :\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} - complete\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback= ModelCheckpoint(\n",
    "    checkpoint_path, monitor ='val_accuracy', verbose =1, save_best_only =True, save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_inputs, train_labels, batch_size = 512, epochs =  10,\n",
    "                    validation_split = 0.2 , callbacks = [earlystop, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popcorn",
   "language": "python",
   "name": "popcorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
