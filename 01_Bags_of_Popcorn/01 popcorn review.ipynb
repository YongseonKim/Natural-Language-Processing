{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Data preprocessing\\n2. word2vec\\n3. modeling\\n4. 평가\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "'''\n",
    "참고 URL\n",
    "- https://programmers.co.kr/learn/courses/21/lessons/1693\n",
    "- http://suanlab.com/assets/lectures/dpp/10.pdf\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "1. Data preprocessing\n",
    "2. word2vec\n",
    "3. modeling\n",
    "4. 평가\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledTrainData.tsv',\n",
       " 'sampleSubmission.csv',\n",
       " 'testData.tsv',\n",
       " 'unlabeledTrainData.tsv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "# -- quoting : 특수 문자가 포함된 필드를 감쌀 때 처리하는 방법, 문자를 따옴표로 묶는 방법\n",
    "import csv\n",
    "df = pd.read_csv('data/labeledTrainData.tsv', header=0, delimiter='\\t',quoting=3)\n",
    "# QUOTE_MINIMAL (0), QUOTE_ALL (1), \n",
    "# QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
    "'''\n",
    "-QUOTE_ALL(1) : Quote everything, regardless of type.(문자열처리, 모든데이터 묶음)\n",
    "-QUOTE_MINIMAL(0) :Quote fields with special characters (특수 문자가 포함된 따옴표 필드)\n",
    "(anything that would confuse a parser configured with the same dialect and options). This is the default\n",
    "-QUOTE_NONNUMERIC(2) :Quote all fields that are not integers or floats.(숫자가 아닌 경우 묶음) \n",
    "  When used with the reader, input fields that are not quoted are converted to floats.\n",
    "-QUOTE_NONE(3) : Do not quote anything on output. 데이터를 묶지 않음\n",
    " When used with the reader, quote characters are included in the field values (normally, they are treated as delimiters and stripped).\n",
    " reader와 사용하면 쌍따옴표는 필드값으로 포함된다.\n",
    "'''\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 -- HTML 태그, \\ 특수 문자 제거\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def preprocessing(x):\n",
    "    #HTML 태그 제거\n",
    "    x= BeautifulSoup(x,'html.parser').get_text()\n",
    "    # 특수문자 제거 # 영문자,숫자를 제외한 문자를 모드 변환 띄어쓰기로\n",
    "    x = re.sub(\"\\W\",\" \",x)    \n",
    "    return x\n",
    "\n",
    "df['review']=df['review'].map(lambda x: preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment ...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>The Classic War of the Worlds   by Timothy ...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager  Nicholas Bell...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised thi...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1   With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...   \n",
       "2  \"7759_3\"          0   The film starts with a manager  Nicholas Bell...   \n",
       "3  \"3630_4\"          0   It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \n",
       "0  [stuff, going, moment, mj, started, listening,...  \n",
       "1  [classic, war, worlds, timothy, hines, enterta...  \n",
       "2  [film, starts, manager, nicholas, bell, giving...  \n",
       "3  [must, assumed, praised, film, greatest, filme...  \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이징 + Stopwords 제거\n",
    "# 장점 : 노이즈를 줄일 수 있음, 단점 : 문장 구조 모델링시 정보 유실 발생\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenizing(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(words.lower())\n",
    "    words = [x for x in words if x not in stop_words]\n",
    "    return words\n",
    "\n",
    "df['words'] = df['review'].map(lambda x : tokenizing(x))\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Tokenizing 방법 리뷰\n",
    "1.  Split 함수\n",
    "2.  NLTK 활용 \n",
    "   - Tokenizing → Index로 벡터화 해야하는데 NLTK는 Tokenizing 까지만\n",
    "3.  keras.preprocessing 활용\n",
    "   - Keras는 Vector화 까지 가능, \n",
    "\n",
    "'''\n",
    "# https://inuplace.tistory.com/536\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['words']) # Fit\n",
    "df['vector'] = token.texts_to_sequences(df['words']) # vector화 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment ...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "      <td>[410, 71, 425, 8956, 511, 2484, 116, 54, 881, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>The Classic War of the Worlds   by Timothy ...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "      <td>[236, 207, 3086, 3611, 7239, 321, 2, 411, 155,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager  Nicholas Bell...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "      <td>[2, 388, 2854, 4457, 3780, 604, 2210, 18035, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised thi...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "      <td>[101, 4896, 5399, 2, 688, 670, 1272, 42, 215, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "      <td>[3409, 4193, 37747, 11135, 859, 2062, 13202, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1   With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...   \n",
       "2  \"7759_3\"          0   The film starts with a manager  Nicholas Bell...   \n",
       "3  \"3630_4\"          0   It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [stuff, going, moment, mj, started, listening,...   \n",
       "1  [classic, war, worlds, timothy, hines, enterta...   \n",
       "2  [film, starts, manager, nicholas, bell, giving...   \n",
       "3  [must, assumed, praised, film, greatest, filme...   \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...   \n",
       "\n",
       "                                              vector  \n",
       "0  [410, 71, 425, 8956, 511, 2484, 116, 54, 881, ...  \n",
       "1  [236, 207, 3086, 3611, 7239, 321, 2, 411, 155,...  \n",
       "2  [2, 388, 2854, 4457, 3780, 604, 2210, 18035, 5...  \n",
       "3  [101, 4896, 5399, 2, 688, 670, 1272, 42, 215, ...  \n",
       "4  [3409, 4193, 37747, 11135, 859, 2062, 13202, 1...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75789"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "vocab = token.word_index\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 단어수 4\n",
      "1사분위 64.0\n",
      "2사분위 90.0\n",
      "3사분위 148.0\n",
      "최대 단어수 1429\n"
     ]
    }
   ],
   "source": [
    "# 최대 문장길이 :: 3사분위에 해당하는 214개를 고정길이로 설정\n",
    "print(\"최소 단어수\", df['vector'].map(lambda x : len(x)).min())\n",
    "print(\"1사분위\", df['vector'].map(lambda x : len(x)).quantile(0.25))\n",
    "print(\"2사분위\",df['vector'].map(lambda x : len(x)).quantile(0.50))\n",
    "print(\"3사분위\",df['vector'].map(lambda x : len(x)).quantile(0.75))\n",
    "print(\"최대 단어수\",df['vector'].map(lambda x : len(x)).max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  116    54   881 ... 18947   320  1372]\n",
      " [  236   207  3086 ...     0     0     0]\n",
      " [ 4657 32515  3589 ...   707  1187  5398]\n",
      " ...\n",
      " [  118  3144    14 ...     0     0     0]\n",
      " [  831   644   521 ...     0     0     0]\n",
      " [  110     1   354 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding: 가변 길이 → 고정 길이\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_padding = 214\n",
    "\n",
    "X_train = pad_sequences(df['vector'],maxlen = max_padding, padding = 'post' )\n",
    "Y_train = df['sentiment']\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\\n2. TF-IDF (완료)\\n3. Countvectorizer -- TF를 의미하는 것\\n4. Word2vec(완료)\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector 화 \n",
    "'''\n",
    "1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\n",
    "2. TF-IDF (완료)\n",
    "3. Countvectorizer -- TF를 의미하는 것\n",
    "4. Word2vec(완료)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(25000, 1000)\n",
      "  (0, 229)\t0.0942788152451277\n",
      "  (0, 407)\t0.08803182726193616\n",
      "  (0, 837)\t0.08721044748871645\n",
      "  (0, 280)\t0.09496753792186204\n",
      "  (0, 284)\t0.06969585794589742\n",
      "  (0, 212)\t0.07906035864322487\n",
      "  (0, 228)\t0.05231788132798058\n",
      "  (0, 839)\t0.10465861308995464\n",
      "  (0, 345)\t0.09157841992227735\n",
      "  (0, 47)\t0.09787767445694262\n",
      "  (0, 636)\t0.11924450276453806\n",
      "  (0, 858)\t0.10762165447123535\n",
      "  (0, 897)\t0.08444378828088646\n",
      "  (0, 355)\t0.077168582057444\n",
      "  (0, 899)\t0.08261680650536574\n",
      "  (0, 224)\t0.06018549026941358\n",
      "  (0, 52)\t0.07430033101736666\n",
      "  (0, 822)\t0.1010110104655137\n",
      "  (0, 873)\t0.056218845322282135\n",
      "  (0, 488)\t0.09699208939260177\n",
      "  (0, 495)\t0.08318639349213663\n",
      "  (0, 737)\t0.06406989962574486\n",
      "  (0, 186)\t0.10911092450265579\n",
      "  (0, 100)\t0.1008889996138849\n",
      "  (0, 487)\t0.07837482258819749\n",
      "  :\t:\n",
      "  (0, 702)\t0.09631309046208494\n",
      "  (0, 133)\t0.08993827764399456\n",
      "  (0, 704)\t0.08568406507165809\n",
      "  (0, 316)\t0.059719045764548194\n",
      "  (0, 303)\t0.17239761070304283\n",
      "  (0, 555)\t0.0807307168180463\n",
      "  (0, 525)\t0.05355348496375057\n",
      "  (0, 164)\t0.16599221640051742\n",
      "  (0, 692)\t0.08245588063709065\n",
      "  (0, 875)\t0.07008140930636224\n",
      "  (0, 374)\t0.1288724546822711\n",
      "  (0, 117)\t0.10185254793964123\n",
      "  (0, 940)\t0.06869921190730176\n",
      "  (0, 449)\t0.08703864996141024\n",
      "  (0, 539)\t0.16527477998100643\n",
      "  (0, 950)\t0.13336171690690563\n",
      "  (0, 223)\t0.10447918484502089\n",
      "  (0, 597)\t0.10897929697821805\n",
      "  (0, 951)\t0.06404081154230137\n",
      "  (0, 576)\t0.12757458066904687\n",
      "  (0, 819)\t0.09678873706868833\n",
      "  (0, 924)\t0.10716256411921418\n",
      "  (0, 562)\t0.09462051388969658\n",
      "  (0, 363)\t0.1401270576416619\n",
      "  (0, 836)\t0.09309037714773506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.10381025, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0796076 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.14477071, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# 입력이 텍스트여야함.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer = \"word\", sublinear_tf=True,\n",
    "                           ngram_range=(1,3), max_features=1000,stop_words = 'english')\n",
    "\n",
    "# min_df : 설정값보다 특정 토큰의 df(document Frequency)가 적으면 벡터화에서 제거\n",
    "# analyzer : word/char 2가지 : word는 단위 : 단어 / char : 단위 : char \n",
    "# sublinear_tf : term frequency에 대한 smoothing 여부\n",
    "# ngram_range = n-gram 의 범위 : 분석기에 의해 설정값을 사용하여 ngram자동 생성\n",
    "# max_features = 벡터의 최대 길이, \n",
    "\n",
    "tfidf_train = vectorizer.fit_transform(list(df['review']))\n",
    "tfidf_train\n",
    "print(type(tfidf_train))\n",
    "print(tfidf_train.shape)\n",
    "print(tfidf_train[0]) \n",
    "# 5000개의 단어 각각에 대한 tf-idf Weight를 의미함\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## TF \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000,ngram_range=(1,3))\n",
    "\n",
    "count_train = vectorizer.fit_transform(list(df['review']))\n",
    "count_train.toarray()#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :  %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# Word2vec 입력은 단어로 표현된 리스트를 입력값으로 받음\n",
    "# n-gram으로 만들어서 넣을 수도 있지만 여기에서는 단순히 split만해서 넣는 것으로 함\n",
    "\n",
    "sentences = []\n",
    "for review in list(df['review']) :\n",
    "    sentences.append(review.split())\n",
    "\n",
    "# print(sentences[0])\n",
    "\n",
    "# 하이퍼파라미터\n",
    "num_features = 1000 # word2vec 특징 수\n",
    "min_word_count =20 \n",
    "num_workers = 6\n",
    "context =10 # Word2vec 수행을 위한 컨텍스트 윈도 크기\n",
    "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-13%EC%9D%BC%EC%B0%A8-word2vec-3c82ec870426\n",
    "downsampling = 1e-3 #Word2vec 빠른 학습을 위해 정답 단어 라벨에 대한 다운 샘플링, 보통 0.001이 좋은 성능\n",
    "#Downsampling of frequent words # 자주 나오는 단어에 대해서는 0.001 만큼 다운 샘플링하여 시간을 아낌\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 09:19:35,606 : INFO :  collecting all words and their counts\n",
      "2020-10-25 09:19:35,607 : INFO :  PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 09:19:35,928 : INFO :  PROGRESS: at sentence #10000, processed 2398843 words, keeping 64085 word types\n",
      "2020-10-25 09:19:36,247 : INFO :  PROGRESS: at sentence #20000, processed 4773958 words, keeping 86259 word types\n",
      "2020-10-25 09:19:36,404 : INFO :  collected 94969 word types from a corpus of 5953723 raw words and 25000 sentences\n",
      "2020-10-25 09:19:36,405 : INFO :  Loading a fresh vocabulary\n",
      "2020-10-25 09:19:36,450 : INFO :  effective_min_count=20 retains 14661 unique words (15% of original 94969, drops 80308)\n",
      "2020-10-25 09:19:36,451 : INFO :  effective_min_count=20 leaves 5672944 word corpus (95% of original 5953723, drops 280779)\n",
      "2020-10-25 09:19:36,488 : INFO :  deleting the raw counts dictionary of 94969 items\n",
      "2020-10-25 09:19:36,490 : INFO :  sample=0.001 downsamples 47 most-common words\n",
      "2020-10-25 09:19:36,491 : INFO :  downsampling leaves estimated 4294841 word corpus (75.7% of prior 5672944)\n",
      "2020-10-25 09:19:36,529 : INFO :  estimated required memory for 14661 words and 1000 dimensions: 124618500 bytes\n",
      "2020-10-25 09:19:36,530 : INFO :  resetting layer weights\n",
      "2020-10-25 09:19:39,086 : INFO :  training model with 6 workers on 14661 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-10-25 09:19:40,091 : INFO :  EPOCH 1 - PROGRESS: at 11.99% examples, 516722 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:41,091 : INFO :  EPOCH 1 - PROGRESS: at 24.01% examples, 520258 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:42,093 : INFO :  EPOCH 1 - PROGRESS: at 36.17% examples, 523153 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:43,098 : INFO :  EPOCH 1 - PROGRESS: at 48.40% examples, 522631 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:44,135 : INFO :  EPOCH 1 - PROGRESS: at 61.32% examples, 526098 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:45,149 : INFO :  EPOCH 1 - PROGRESS: at 74.43% examples, 530503 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:46,159 : INFO :  EPOCH 1 - PROGRESS: at 87.54% examples, 534940 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:47,030 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:19:47,048 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:19:47,051 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:19:47,057 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:19:47,070 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:19:47,076 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:19:47,076 : INFO :  EPOCH - 1 : training on 5953723 raw words (4296424 effective words) took 8.0s, 537944 effective words/s\n",
      "2020-10-25 09:19:48,089 : INFO :  EPOCH 2 - PROGRESS: at 11.66% examples, 498472 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:49,105 : INFO :  EPOCH 2 - PROGRESS: at 24.01% examples, 513748 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:50,108 : INFO :  EPOCH 2 - PROGRESS: at 36.30% examples, 520811 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:51,109 : INFO :  EPOCH 2 - PROGRESS: at 48.58% examples, 521230 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:52,115 : INFO :  EPOCH 2 - PROGRESS: at 61.14% examples, 525174 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:53,133 : INFO :  EPOCH 2 - PROGRESS: at 73.88% examples, 527184 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:54,154 : INFO :  EPOCH 2 - PROGRESS: at 86.60% examples, 528158 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:55,089 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:19:55,113 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:19:55,121 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:19:55,122 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:19:55,127 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:19:55,132 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:19:55,132 : INFO :  EPOCH - 2 : training on 5953723 raw words (4294759 effective words) took 8.1s, 533267 effective words/s\n",
      "2020-10-25 09:19:56,169 : INFO :  EPOCH 3 - PROGRESS: at 12.33% examples, 514037 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:57,171 : INFO :  EPOCH 3 - PROGRESS: at 25.02% examples, 532472 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:58,175 : INFO :  EPOCH 3 - PROGRESS: at 37.69% examples, 537265 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:19:59,187 : INFO :  EPOCH 3 - PROGRESS: at 49.65% examples, 530549 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:00,210 : INFO :  EPOCH 3 - PROGRESS: at 62.50% examples, 532272 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:01,211 : INFO :  EPOCH 3 - PROGRESS: at 75.21% examples, 534402 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:02,214 : INFO :  EPOCH 3 - PROGRESS: at 87.73% examples, 534792 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:03,071 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:20:03,088 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:20:03,094 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:20:03,100 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:20:03,103 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:20:03,113 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:20:03,114 : INFO :  EPOCH - 3 : training on 5953723 raw words (4294196 effective words) took 8.0s, 538133 effective words/s\n",
      "2020-10-25 09:20:04,141 : INFO :  EPOCH 4 - PROGRESS: at 11.99% examples, 504998 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:05,153 : INFO :  EPOCH 4 - PROGRESS: at 24.69% examples, 525209 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:06,162 : INFO :  EPOCH 4 - PROGRESS: at 37.11% examples, 529532 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:07,172 : INFO :  EPOCH 4 - PROGRESS: at 49.65% examples, 530290 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:08,186 : INFO :  EPOCH 4 - PROGRESS: at 62.35% examples, 531703 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:09,199 : INFO :  EPOCH 4 - PROGRESS: at 75.04% examples, 532970 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:10,211 : INFO :  EPOCH 4 - PROGRESS: at 87.20% examples, 530820 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:11,102 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:20:11,109 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:20:11,119 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:20:11,124 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:20:11,141 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:20:11,154 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:20:11,155 : INFO :  EPOCH - 4 : training on 5953723 raw words (4294875 effective words) took 8.0s, 534351 effective words/s\n",
      "2020-10-25 09:20:12,157 : INFO :  EPOCH 5 - PROGRESS: at 12.16% examples, 523931 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:13,163 : INFO :  EPOCH 5 - PROGRESS: at 24.88% examples, 536518 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:14,180 : INFO :  EPOCH 5 - PROGRESS: at 37.64% examples, 540274 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:15,188 : INFO :  EPOCH 5 - PROGRESS: at 49.65% examples, 533470 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:16,216 : INFO :  EPOCH 5 - PROGRESS: at 62.05% examples, 530074 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:17,221 : INFO :  EPOCH 5 - PROGRESS: at 74.40% examples, 529958 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:18,222 : INFO :  EPOCH 5 - PROGRESS: at 86.60% examples, 529109 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:19,203 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:20:19,214 : INFO :  worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 09:20:19,223 : INFO :  EPOCH 5 - PROGRESS: at 99.54% examples, 530257 words/s, in_qsize 3, out_qsize 1\n",
      "2020-10-25 09:20:19,224 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:20:19,227 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:20:19,235 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:20:19,237 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:20:19,238 : INFO :  EPOCH - 5 : training on 5953723 raw words (4295970 effective words) took 8.1s, 531614 effective words/s\n",
      "2020-10-25 09:20:20,244 : INFO :  EPOCH 6 - PROGRESS: at 11.99% examples, 515306 words/s, in_qsize 12, out_qsize 0\n",
      "2020-10-25 09:20:21,244 : INFO :  EPOCH 6 - PROGRESS: at 24.01% examples, 519395 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:22,258 : INFO :  EPOCH 6 - PROGRESS: at 36.58% examples, 527287 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:23,275 : INFO :  EPOCH 6 - PROGRESS: at 49.32% examples, 529458 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:24,276 : INFO :  EPOCH 6 - PROGRESS: at 61.65% examples, 529551 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:25,293 : INFO :  EPOCH 6 - PROGRESS: at 74.26% examples, 529615 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:26,293 : INFO :  EPOCH 6 - PROGRESS: at 86.33% examples, 527769 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:27,296 : INFO :  EPOCH 6 - PROGRESS: at 98.67% examples, 526355 words/s, in_qsize 8, out_qsize 0\n",
      "2020-10-25 09:20:27,331 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:20:27,342 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:20:27,355 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:20:27,366 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:20:27,366 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:20:27,370 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:20:27,371 : INFO :  EPOCH - 6 : training on 5953723 raw words (4294342 effective words) took 8.1s, 528168 effective words/s\n",
      "2020-10-25 09:20:28,378 : INFO :  EPOCH 7 - PROGRESS: at 11.82% examples, 507950 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:29,397 : INFO :  EPOCH 7 - PROGRESS: at 24.01% examples, 514308 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:30,420 : INFO :  EPOCH 7 - PROGRESS: at 36.78% examples, 524393 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:31,427 : INFO :  EPOCH 7 - PROGRESS: at 49.65% examples, 530227 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:32,434 : INFO :  EPOCH 7 - PROGRESS: at 62.51% examples, 533732 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:33,456 : INFO :  EPOCH 7 - PROGRESS: at 75.75% examples, 537275 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:34,460 : INFO :  EPOCH 7 - PROGRESS: at 88.46% examples, 538283 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:35,263 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:20:35,265 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:20:35,287 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:20:35,290 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:20:35,297 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:20:35,301 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:20:35,301 : INFO :  EPOCH - 7 : training on 5953723 raw words (4293480 effective words) took 7.9s, 541624 effective words/s\n",
      "2020-10-25 09:20:36,310 : INFO :  EPOCH 8 - PROGRESS: at 12.33% examples, 527669 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:37,312 : INFO :  EPOCH 8 - PROGRESS: at 25.02% examples, 539503 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:38,318 : INFO :  EPOCH 8 - PROGRESS: at 37.48% examples, 539376 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:39,319 : INFO :  EPOCH 8 - PROGRESS: at 50.27% examples, 542381 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:40,334 : INFO :  EPOCH 8 - PROGRESS: at 63.84% examples, 548438 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:41,335 : INFO :  EPOCH 8 - PROGRESS: at 76.98% examples, 550203 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:42,338 : INFO :  EPOCH 8 - PROGRESS: at 89.89% examples, 550511 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:43,047 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:20:43,059 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:20:43,082 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:20:43,083 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:20:43,083 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:20:43,094 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:20:43,095 : INFO :  EPOCH - 8 : training on 5953723 raw words (4294532 effective words) took 7.8s, 551200 effective words/s\n",
      "2020-10-25 09:20:44,105 : INFO :  EPOCH 9 - PROGRESS: at 12.48% examples, 534950 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:45,121 : INFO :  EPOCH 9 - PROGRESS: at 25.50% examples, 545745 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:46,130 : INFO :  EPOCH 9 - PROGRESS: at 38.66% examples, 552422 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:47,148 : INFO :  EPOCH 9 - PROGRESS: at 51.46% examples, 549785 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:48,153 : INFO :  EPOCH 9 - PROGRESS: at 64.01% examples, 547160 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:49,158 : INFO :  EPOCH 9 - PROGRESS: at 77.26% examples, 549885 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:50,162 : INFO :  EPOCH 9 - PROGRESS: at 90.40% examples, 551199 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:50,809 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:20:50,810 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:20:50,825 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:20:50,843 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:20:50,848 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:20:50,864 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:20:50,865 : INFO :  EPOCH - 9 : training on 5953723 raw words (4295302 effective words) took 7.8s, 552941 effective words/s\n",
      "2020-10-25 09:20:51,869 : INFO :  EPOCH 10 - PROGRESS: at 12.48% examples, 538031 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:52,869 : INFO :  EPOCH 10 - PROGRESS: at 25.36% examples, 548202 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:53,880 : INFO :  EPOCH 10 - PROGRESS: at 37.48% examples, 539526 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:54,882 : INFO :  EPOCH 10 - PROGRESS: at 50.30% examples, 542452 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:55,922 : INFO :  EPOCH 10 - PROGRESS: at 63.84% examples, 545854 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:56,926 : INFO :  EPOCH 10 - PROGRESS: at 76.98% examples, 547714 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:57,931 : INFO :  EPOCH 10 - PROGRESS: at 89.89% examples, 548188 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-25 09:20:58,629 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 09:20:58,653 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 09:20:58,655 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 09:20:58,660 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 09:20:58,663 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 09:20:58,664 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 09:20:58,665 : INFO :  EPOCH - 10 : training on 5953723 raw words (4294868 effective words) took 7.8s, 550802 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 09:20:58,665 : INFO :  training on a 59537230 raw words (42948748 effective words) took 79.6s, 539708 effective words/s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# https://wikidocs.net/50739\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers = num_workers,\n",
    "                          size = num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window =  context,\n",
    "                          sample = downsampling,\n",
    "                          iter = 10,\n",
    "                          sg =0 # sg =0 CBOW, 1 : skip-gram\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 09:20:58,668 : INFO :  precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6091312170028687),\n",
       " ('lady', 0.5978107452392578),\n",
       " ('soldier', 0.530846118927002),\n",
       " ('doctor', 0.5244083404541016),\n",
       " ('priest', 0.5158758163452148),\n",
       " ('boy', 0.5143296718597412),\n",
       " ('businessman', 0.5103845596313477),\n",
       " ('farmer', 0.5048384666442871),\n",
       " ('journalist', 0.49860942363739014),\n",
       " ('guy', 0.4938508868217468)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# man과 가장 유사한 단어 골라내기 \n",
    "model.wv.most_similar(\"man\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# word2vec 은 단어 하나하나가 벡터로 표현되어 있다.\n",
    "# Review 데이터는 단어들의 조합이기에 Review를 벡터로 표현하기 위해\n",
    "# Review에 포함된 단어 벡터들의 평균값을 만든다.\n",
    "# 다른 방법으로는 Doc2vec, average of word2vec vectors with TF-IDF\n",
    "# Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector.\n",
    " # 단어 벡터에 TF-IDF를 곱해서 평균 내는 방법\n",
    "    \n",
    "# https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence\n",
    "\n",
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype = np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘 사전\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words +=1\n",
    "            #사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            \n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            # model은 단어들에 대한 vector를 다 가지고 있음\n",
    "            # num_features 만큼 이미 학습할때 정의해서 만들어놓음 \n",
    "            \n",
    "    feature_vector = np.divide(feature_vector,num_words)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    \n",
    "    for s in reviews :\n",
    "        dataset.append(get_features(s,model,num_features))\n",
    "    \n",
    "    reviewFeaturevecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeaturevecs\n",
    "\n",
    "word2vec_train = get_dataset(sentences,model,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.840400\n"
     ]
    }
   ],
   "source": [
    "# Count_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(count_train.toarray(),Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.829000\n"
     ]
    }
   ],
   "source": [
    "# tfidf_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(tfidf_train.toarray(),Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.832400\n"
     ]
    }
   ],
   "source": [
    "# word2vec_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(word2vec_train,Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "tf.random.set_seed(99)\n",
    "BATCH_SIZE = 128\n",
    "epochs = 10\n",
    "valid = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts 20 minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/labeledTrainData.tsv',header =0, delimiter = '\\t', quoting =3)\n",
    "\n",
    "def preprocessing(review, remove_stopwords = False):\n",
    "    review_text = BeautifulSoup(review,'html.parser').get_text()\n",
    "    \n",
    "    # 특수문자 제거 # 영문자,숫자를 제외한 문자를 모드 변환 띄어쓰기로\n",
    "    review_text = re.sub(\"\\W\",\" \",review_text)    \n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        clean_review = ' '.join(words)\n",
    "        \n",
    "    else :\n",
    "        clean_review = ' '.join(words)\n",
    "    \n",
    "    return clean_review\n",
    "\n",
    "clean_train_reviews = []\n",
    "\n",
    "for review in train_data['review']:\n",
    "    clean_train_reviews.append(preprocessing(review, remove_stopwords=True))\n",
    "\n",
    "clean_train_reviews[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[410, 71, 425, 8957, 512, 2483, 116, 54, 882, 522, 179, 18946, 179, 11409, 167, 79, 14, 669, 2484, 118, 93, 10, 505, 4131, 167, 22, 213, 589, 2361, 1205, 11409, 72, 4896, 72, 641, 2, 257, 71, 11, 306, 1683, 492, 1157, 3309, 8957, 417, 800, 3387, 17, 447, 607, 1516, 15, 4528, 1875, 1010, 148, 348, 1455, 750, 2452, 4, 8957, 424, 71, 643, 70, 241, 95, 547, 8957, 26374, 26375, 121, 1, 8957, 327, 8, 47, 20, 327, 169, 10, 210, 638, 641, 2, 117, 295, 388, 733, 124, 15761, 3361, 1517, 582, 741, 10167, 934, 11746, 829, 1251, 1423, 366, 8957, 225, 15, 584, 8957, 22505, 2300, 13630, 741, 10167, 27, 28950, 346, 16, 41, 18947, 1516, 394, 11410, 167, 4018, 8957, 116, 633, 505, 80, 4, 8957, 1444, 386, 2193, 115, 1943, 2529, 582, 17, 60, 101, 4947, 5239, 264, 1280, 26376, 15, 582, 498, 751, 643, 637, 3, 400, 166, 452, 115, 622, 3310, 1172, 690, 48, 1190, 228, 1, 16, 4, 8957, 3, 513, 62, 25, 16, 646, 135, 235, 96, 7553, 607, 3486, 8957, 37737, 1888, 1, 130, 348, 1455, 251, 3, 874, 16, 42, 1502, 1009, 2361, 12, 555, 392, 723, 7037, 12, 41, 16, 160, 368, 4456, 3432, 41, 88, 229, 444, 210, 258, 118, 3, 18948, 18949, 320, 1372]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_reviews)\n",
    "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
    "print(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = tokenizer.word_index\n",
    "word_vocab[\"<PAD>\"]=0 \n",
    "# print(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75684"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_configs = {}\n",
    "data_configs['vocab']= word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab)+1\n",
    "data_configs['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 174)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 174\n",
    "\n",
    "train_inputs =  pad_sequences(text_sequences,maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print(train_inputs.shape)\n",
    "train_labels = np.array(train_data['sentiment'])\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(text_sequences[2]).shape\n",
    "# text_sequences[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75684"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_configs['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  800,  3387,    17, ..., 18949,   320,  1372],\n",
       "       [  236,   206,  3085, ...,     0,     0,     0],\n",
       "       [37741,  3511,  1410, ...,   708,  1190,  5398],\n",
       "       ...,\n",
       "       [  118,  3143,    14, ...,     0,     0,     0],\n",
       "       [  832,   645,   522, ...,     0,     0,     0],\n",
       "       [  110,     1,   354, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 174)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.toarray().shape\n",
    "data_configs['vocab_size']\n",
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tensorflow 2.0 모델 구축 방법\n",
    "1. Sequential API : tf.keras.Sequential, model.add()\n",
    "2. Functional API : Input - layers \n",
    "3. Custom layer : layers. layer 상속 : 여러 레이어를 하나로 묶은 레이어 구현 용이\n",
    "4. Subclassing : tf.keras.Model  : 자유도가 가장 높아서 자주 사용. \n",
    "\n",
    "'''\n",
    "\n",
    "# tf.keras.Model을 학습받아 클래스로 구현 \n",
    "# Input은 \n",
    "# Word2 vec!!!!\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__() #  부모 클래스에 있는__init__ 함수 호출 \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim= data_configs['vocab_size'], # 이전에 임베딩된 5000개 Feature 를 넣겠다\n",
    "                                          output_dim= 300 # Dense 임베딩 결과shape                                         \n",
    "                                         )\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(150, #  lstm_shape : units = dimensionality of the output shape 의미\n",
    "                                          return_sequences = True)\n",
    "        self.lstm_2 = tf.keras.layers.LSTM(lstm_shape)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc1 = tf.keras.layers.Dense(units = 150, activation = tf.keras.activations.tanh)\n",
    "        self.fc2 = tf.keras.layers.Dense(units = 1, activation = tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "#         print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm_1(x)\n",
    "        x = self.lstm_2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "# input_shape = (batch_size,tfidf_train.toarray().shape[1])\n",
    "input_shape = (batch_size, train_inputs.shape[1])\n",
    "emb_shape =500\n",
    "lstm_shape = 150\n",
    "\n",
    "model = RNN()\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(1e-4),\n",
    "             loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics= [tf.keras.metrics.BinaryAccuracy(name = \"accuracy\")]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_out/rnn_classifier - exists\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# earlystopping \n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy', min_delta=0.0001, patience=2)\n",
    "model_name = 'rnn_classifier'\n",
    "checkpoint_path = './data_out/{}/weight.h5'.format(model_name)\n",
    "checkpoint_dir =  os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} - exists\".format(checkpoint_dir))\n",
    "else :\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} - complete\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback= ModelCheckpoint(\n",
    "    checkpoint_path, monitor ='val_accuracy', verbose =1, save_best_only =True, save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5042\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50560, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 31s 2ms/sample - loss: 0.6931 - accuracy: 0.5046 - val_loss: 0.6922 - val_accuracy: 0.5056\n",
      "Epoch 2/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5339\n",
      "Epoch 00002: val_accuracy improved from 0.50560 to 0.67900, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.6798 - accuracy: 0.5347 - val_loss: 0.5901 - val_accuracy: 0.6790\n",
      "Epoch 3/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.4811 - accuracy: 0.7838\n",
      "Epoch 00003: val_accuracy improved from 0.67900 to 0.84260, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.4804 - accuracy: 0.7843 - val_loss: 0.3979 - val_accuracy: 0.8426\n",
      "Epoch 4/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.2711 - accuracy: 0.8918\n",
      "Epoch 00004: val_accuracy improved from 0.84260 to 0.85760, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.2711 - accuracy: 0.8916 - val_loss: 0.3183 - val_accuracy: 0.8576\n",
      "Epoch 5/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9382\n",
      "Epoch 00005: val_accuracy improved from 0.85760 to 0.87060, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.1747 - accuracy: 0.9384 - val_loss: 0.3178 - val_accuracy: 0.8706\n",
      "Epoch 6/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.1122 - accuracy: 0.9652\n",
      "Epoch 00006: val_accuracy improved from 0.87060 to 0.88320, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.1122 - accuracy: 0.9652 - val_loss: 0.3427 - val_accuracy: 0.8832\n",
      "Epoch 7/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9809\n",
      "Epoch 00007: val_accuracy did not improve from 0.88320\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.0723 - accuracy: 0.9809 - val_loss: 0.4474 - val_accuracy: 0.8722\n",
      "Epoch 8/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.0520 - accuracy: 0.9868\n",
      "Epoch 00008: val_accuracy did not improve from 0.88320\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.0518 - accuracy: 0.9869 - val_loss: 0.4994 - val_accuracy: 0.8726\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_inputs, train_labels, batch_size = batch_size, epochs =  epochs,\n",
    "                    validation_split = 0.2 , callbacks = [earlystop, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_graphs' from 'tensorflow.keras.utils' (C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\keras\\api\\_v2\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-f09b396cd587>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_graphs' from 'tensorflow.keras.utils' (C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\keras\\api\\_v2\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_graphs\n",
    "plot_graphs(history,'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  22705200  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  270600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  180600    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  22650     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  151       \n",
      "=================================================================\n",
      "Total params: 23,179,201\n",
      "Trainable params: 23,179,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4350000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25000*174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popcorn",
   "language": "python",
   "name": "popcorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
