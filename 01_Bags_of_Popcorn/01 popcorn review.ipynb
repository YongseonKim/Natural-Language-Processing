{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Data preprocessing\\n2. word2vec\\n3. modeling\\n4. 평가\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "'''\n",
    "참고 URL\n",
    "- https://programmers.co.kr/learn/courses/21/lessons/1693\n",
    "- http://suanlab.com/assets/lectures/dpp/10.pdf\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "1. Data preprocessing\n",
    "2. word2vec\n",
    "3. modeling\n",
    "4. 평가\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledTrainData.tsv',\n",
       " 'sampleSubmission.csv',\n",
       " 'testData.tsv',\n",
       " 'unlabeledTrainData.tsv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "# -- quoting : 특수 문자가 포함된 필드를 감쌀 때 처리하는 방법, 문자를 따옴표로 묶는 방법\n",
    "import csv\n",
    "df = pd.read_csv('data/labeledTrainData.tsv', header=0, delimiter='\\t',quoting=3)\n",
    "# QUOTE_MINIMAL (0), QUOTE_ALL (1), \n",
    "# QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
    "'''\n",
    "-QUOTE_ALL(1) : Quote everything, regardless of type.(문자열처리, 모든데이터 묶음)\n",
    "-QUOTE_MINIMAL(0) :Quote fields with special characters (특수 문자가 포함된 따옴표 필드)\n",
    "(anything that would confuse a parser configured with the same dialect and options). This is the default\n",
    "-QUOTE_NONNUMERIC(2) :Quote all fields that are not integers or floats.(숫자가 아닌 경우 묶음) \n",
    "  When used with the reader, input fields that are not quoted are converted to floats.\n",
    "-QUOTE_NONE(3) : Do not quote anything on output. 데이터를 묶지 않음\n",
    " When used with the reader, quote characters are included in the field values (normally, they are treated as delimiters and stripped).\n",
    " reader와 사용하면 쌍따옴표는 필드값으로 포함된다.\n",
    "'''\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 -- HTML 태그, \\ 특수 문자 제거\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def preprocessing(x):\n",
    "    #HTML 태그 제거\n",
    "    x= BeautifulSoup(x,'html.parser').get_text()\n",
    "    # 특수문자 제거 # 영문자,숫자를 제외한 문자를 모드 변환 띄어쓰기로\n",
    "    x = re.sub(\"\\W\",\" \",x)    \n",
    "    return x\n",
    "\n",
    "df['review']=df['review'].map(lambda x: preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment ...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>The Classic War of the Worlds   by Timothy ...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager  Nicholas Bell...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised thi...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1   With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...   \n",
       "2  \"7759_3\"          0   The film starts with a manager  Nicholas Bell...   \n",
       "3  \"3630_4\"          0   It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \n",
       "0  [stuff, going, moment, mj, started, listening,...  \n",
       "1  [classic, war, worlds, timothy, hines, enterta...  \n",
       "2  [film, starts, manager, nicholas, bell, giving...  \n",
       "3  [must, assumed, praised, film, greatest, filme...  \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이징 + Stopwords 제거\n",
    "# 장점 : 노이즈를 줄일 수 있음, 단점 : 문장 구조 모델링시 정보 유실 발생\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenizing(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(words.lower())\n",
    "    words = [x for x in words if x not in stop_words]\n",
    "    return words\n",
    "\n",
    "df['words'] = df['review'].map(lambda x : tokenizing(x))\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Tokenizing 방법 리뷰\n",
    "1.  Split 함수\n",
    "2.  NLTK 활용 \n",
    "   - Tokenizing → Index로 벡터화 해야하는데 NLTK는 Tokenizing 까지만\n",
    "3.  keras.preprocessing 활용\n",
    "   - Keras는 Vector화 까지 가능, \n",
    "\n",
    "'''\n",
    "# https://inuplace.tistory.com/536\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['words']) # Fit\n",
    "df['vector'] = token.texts_to_sequences(df['words']) # vector화 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment ...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "      <td>[410, 71, 425, 8956, 511, 2484, 116, 54, 881, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>The Classic War of the Worlds   by Timothy ...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "      <td>[236, 207, 3086, 3611, 7239, 321, 2, 411, 155,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager  Nicholas Bell...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "      <td>[2, 388, 2854, 4457, 3780, 604, 2210, 18035, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised thi...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "      <td>[101, 4896, 5399, 2, 688, 670, 1272, 42, 215, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "      <td>[3409, 4193, 37747, 11135, 859, 2062, 13202, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1   With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...   \n",
       "2  \"7759_3\"          0   The film starts with a manager  Nicholas Bell...   \n",
       "3  \"3630_4\"          0   It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [stuff, going, moment, mj, started, listening,...   \n",
       "1  [classic, war, worlds, timothy, hines, enterta...   \n",
       "2  [film, starts, manager, nicholas, bell, giving...   \n",
       "3  [must, assumed, praised, film, greatest, filme...   \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...   \n",
       "\n",
       "                                              vector  \n",
       "0  [410, 71, 425, 8956, 511, 2484, 116, 54, 881, ...  \n",
       "1  [236, 207, 3086, 3611, 7239, 321, 2, 411, 155,...  \n",
       "2  [2, 388, 2854, 4457, 3780, 604, 2210, 18035, 5...  \n",
       "3  [101, 4896, 5399, 2, 688, 670, 1272, 42, 215, ...  \n",
       "4  [3409, 4193, 37747, 11135, 859, 2062, 13202, 1...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75789"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "vocab = token.word_index\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 단어수 4\n",
      "1사분위 64.0\n",
      "2사분위 90.0\n",
      "3사분위 148.0\n",
      "최대 단어수 1429\n"
     ]
    }
   ],
   "source": [
    "# 최대 문장길이 :: 3사분위에 해당하는 214개를 고정길이로 설정\n",
    "print(\"최소 단어수\", df['vector'].map(lambda x : len(x)).min())\n",
    "print(\"1사분위\", df['vector'].map(lambda x : len(x)).quantile(0.25))\n",
    "print(\"2사분위\",df['vector'].map(lambda x : len(x)).quantile(0.50))\n",
    "print(\"3사분위\",df['vector'].map(lambda x : len(x)).quantile(0.75))\n",
    "print(\"최대 단어수\",df['vector'].map(lambda x : len(x)).max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  116    54   881 ... 18947   320  1372]\n",
      " [  236   207  3086 ...     0     0     0]\n",
      " [ 4657 32515  3589 ...   707  1187  5398]\n",
      " ...\n",
      " [  118  3144    14 ...     0     0     0]\n",
      " [  831   644   521 ...     0     0     0]\n",
      " [  110     1   354 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding: 가변 길이 → 고정 길이\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_padding = 214\n",
    "\n",
    "X_train = pad_sequences(df['vector'],maxlen = max_padding, padding = 'post' )\n",
    "Y_train = df['sentiment']\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\\n2. TF-IDF (완료)\\n3. Countvectorizer -- TF를 의미하는 것\\n4. Word2vec(완료)\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector 화 \n",
    "'''\n",
    "1. BOW를 이용해서 Vector Sequence로 변환(위에 완료)\n",
    "2. TF-IDF (완료)\n",
    "3. Countvectorizer -- TF를 의미하는 것\n",
    "4. Word2vec(완료)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(25000, 5000)\n",
      "  (0, 2502)\t0.1028129880638737\n",
      "  (0, 1285)\t0.06783600784085628\n",
      "  (0, 4212)\t0.09798879691949967\n",
      "  (0, 4484)\t0.10513903894444403\n",
      "  (0, 3287)\t0.09207262080751873\n",
      "  (0, 2998)\t0.09979596337739731\n",
      "  (0, 4479)\t0.09655916967157495\n",
      "  (0, 3286)\t0.1049730103164135\n",
      "  (0, 37)\t0.09626873961802275\n",
      "  (0, 2776)\t0.09875448073816731\n",
      "  (0, 1638)\t0.17021192744889807\n",
      "  (0, 2448)\t0.10116830229528975\n",
      "  (0, 2825)\t0.171073173099871\n",
      "  (0, 2174)\t0.06334114094305214\n",
      "  (0, 4298)\t0.06275013728447221\n",
      "  (0, 1559)\t0.0683315613411591\n",
      "  (0, 1571)\t0.050147944199345866\n",
      "  (0, 1305)\t0.1025268801175609\n",
      "  (0, 815)\t0.10635206492339642\n",
      "  (0, 1204)\t0.056885940864640605\n",
      "  (0, 1280)\t0.03764404759175515\n",
      "  (0, 4304)\t0.07530453665252453\n",
      "  (0, 1875)\t0.06589300465590953\n",
      "  (0, 312)\t0.07042547866816848\n",
      "  (0, 3348)\t0.08579945561982709\n",
      "  :\t:\n",
      "  (0, 2299)\t0.08305401895465898\n",
      "  (0, 2022)\t0.160515236053926\n",
      "  (0, 2890)\t0.058087806097562915\n",
      "  (0, 2758)\t0.038533095865286734\n",
      "  (0, 1399)\t0.10653310209512738\n",
      "  (0, 950)\t0.1194356257446557\n",
      "  (0, 3606)\t0.05932910538681658\n",
      "  (0, 4490)\t0.05042535821906538\n",
      "  (0, 2027)\t0.09272701214548547\n",
      "  (0, 2303)\t0.09506771035938272\n",
      "  (0, 699)\t0.073285501336439\n",
      "  (0, 4796)\t0.049430832000672706\n",
      "  (0, 2431)\t0.0626265245897318\n",
      "  (0, 2824)\t0.11891941197539346\n",
      "  (0, 4829)\t0.09595715060955264\n",
      "  (0, 1259)\t0.07517543346217842\n",
      "  (0, 3162)\t0.07841337871168738\n",
      "  (0, 4833)\t0.04607899433847484\n",
      "  (0, 3045)\t0.09179315875001577\n",
      "  (0, 2645)\t0.09489067409146293\n",
      "  (0, 4202)\t0.06964196049374342\n",
      "  (0, 4725)\t0.07710619316689983\n",
      "  (0, 2927)\t0.06808186871502951\n",
      "  (0, 1923)\t0.1008249855090022\n",
      "  (0, 4294)\t0.06698089637298846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.08774747, 0.04602346, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.08424142, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# 입력이 텍스트여야함.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer = \"word\", sublinear_tf=True,\n",
    "                           ngram_range=(1,3), max_features=5000,stop_words = 'english')\n",
    "\n",
    "# min_df : 설정값보다 특정 토큰의 df(document Frequency)가 적으면 벡터화에서 제거\n",
    "# analyzer : word/char 2가지 : word는 단위 : 단어 / char : 단위 : char \n",
    "# sublinear_tf : term frequency에 대한 smoothing 여부\n",
    "# ngram_range = n-gram 의 범위 : 분석기에 의해 설정값을 사용하여 ngram자동 생성\n",
    "# max_features = 벡터의 최대 길이, \n",
    "\n",
    "tfidf_train = vectorizer.fit_transform(list(df['review']))\n",
    "tfidf_train\n",
    "print(type(tfidf_train))\n",
    "print(tfidf_train.shape)\n",
    "print(tfidf_train[0]) \n",
    "# 5000개의 단어 각각에 대한 tf-idf Weight를 의미함\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## TF \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000)\n",
    "\n",
    "count_train = vectorizer.fit_transform(list(df['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :  %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# Word2vec 입력은 단어로 표현된 리스트를 입력값으로 받음\n",
    "# n-gram으로 만들어서 넣을 수도 있지만 여기에서는 단순히 split만해서 넣는 것으로 함\n",
    "\n",
    "sentences = []\n",
    "for review in list(df['review']) :\n",
    "    sentences.append(review.split())\n",
    "\n",
    "# print(sentences[0])\n",
    "\n",
    "# 하이퍼파라미터\n",
    "num_features = 1000 # word2vec 특징 수\n",
    "min_word_count =20 \n",
    "num_workers = 6\n",
    "context =10 # Word2vec 수행을 위한 컨텍스트 윈도 크기\n",
    "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-13%EC%9D%BC%EC%B0%A8-word2vec-3c82ec870426\n",
    "downsampling = 1e-3 #Word2vec 빠른 학습을 위해 정답 단어 라벨에 대한 다운 샘플링, 보통 0.001이 좋은 성능\n",
    "#Downsampling of frequent words # 자주 나오는 단어에 대해서는 0.001 만큼 다운 샘플링하여 시간을 아낌\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 07:13:24,384 : INFO :  collecting all words and their counts\n",
      "2020-10-23 07:13:24,384 : INFO :  PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 07:13:24,702 : INFO :  PROGRESS: at sentence #10000, processed 2398843 words, keeping 64085 word types\n",
      "2020-10-23 07:13:25,025 : INFO :  PROGRESS: at sentence #20000, processed 4773958 words, keeping 86259 word types\n",
      "2020-10-23 07:13:25,190 : INFO :  collected 94969 word types from a corpus of 5953723 raw words and 25000 sentences\n",
      "2020-10-23 07:13:25,191 : INFO :  Loading a fresh vocabulary\n",
      "2020-10-23 07:13:25,243 : INFO :  effective_min_count=20 retains 14661 unique words (15% of original 94969, drops 80308)\n",
      "2020-10-23 07:13:25,243 : INFO :  effective_min_count=20 leaves 5672944 word corpus (95% of original 5953723, drops 280779)\n",
      "2020-10-23 07:13:25,281 : INFO :  deleting the raw counts dictionary of 94969 items\n",
      "2020-10-23 07:13:25,282 : INFO :  sample=0.001 downsamples 47 most-common words\n",
      "2020-10-23 07:13:25,283 : INFO :  downsampling leaves estimated 4294841 word corpus (75.7% of prior 5672944)\n",
      "2020-10-23 07:13:25,312 : INFO :  estimated required memory for 14661 words and 1000 dimensions: 124618500 bytes\n",
      "2020-10-23 07:13:25,312 : INFO :  resetting layer weights\n",
      "2020-10-23 07:13:27,866 : INFO :  training model with 6 workers on 14661 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-10-23 07:13:28,877 : INFO :  EPOCH 1 - PROGRESS: at 12.16% examples, 520191 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:29,892 : INFO :  EPOCH 1 - PROGRESS: at 24.88% examples, 532026 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:30,900 : INFO :  EPOCH 1 - PROGRESS: at 36.74% examples, 527246 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:31,902 : INFO :  EPOCH 1 - PROGRESS: at 49.50% examples, 531281 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:32,912 : INFO :  EPOCH 1 - PROGRESS: at 62.35% examples, 534179 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:33,922 : INFO :  EPOCH 1 - PROGRESS: at 75.24% examples, 536445 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:34,942 : INFO :  EPOCH 1 - PROGRESS: at 88.06% examples, 537251 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:35,773 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:13:35,803 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:13:35,810 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:13:35,816 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:13:35,819 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:13:35,831 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:13:35,832 : INFO :  EPOCH - 1 : training on 5953723 raw words (4293471 effective words) took 8.0s, 539170 effective words/s\n",
      "2020-10-23 07:13:36,844 : INFO :  EPOCH 2 - PROGRESS: at 12.16% examples, 519607 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:37,854 : INFO :  EPOCH 2 - PROGRESS: at 25.06% examples, 536661 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:38,877 : INFO :  EPOCH 2 - PROGRESS: at 37.64% examples, 537036 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:39,885 : INFO :  EPOCH 2 - PROGRESS: at 50.81% examples, 543048 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:40,901 : INFO :  EPOCH 2 - PROGRESS: at 63.84% examples, 544525 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:41,920 : INFO :  EPOCH 2 - PROGRESS: at 76.25% examples, 540713 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:42,924 : INFO :  EPOCH 2 - PROGRESS: at 89.18% examples, 542241 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:43,675 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:13:43,713 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:13:43,723 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:13:43,724 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:13:43,725 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:13:43,727 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:13:43,727 : INFO :  EPOCH - 2 : training on 5953723 raw words (4295020 effective words) took 7.9s, 544180 effective words/s\n",
      "2020-10-23 07:13:44,755 : INFO :  EPOCH 3 - PROGRESS: at 12.16% examples, 510499 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:45,760 : INFO :  EPOCH 3 - PROGRESS: at 24.88% examples, 529993 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:46,794 : INFO :  EPOCH 3 - PROGRESS: at 37.64% examples, 532845 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:47,813 : INFO :  EPOCH 3 - PROGRESS: at 50.48% examples, 534917 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:48,845 : INFO :  EPOCH 3 - PROGRESS: at 63.51% examples, 536456 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:49,849 : INFO :  EPOCH 3 - PROGRESS: at 76.61% examples, 539852 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:50,885 : INFO :  EPOCH 3 - PROGRESS: at 89.72% examples, 540095 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:51,566 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:13:51,580 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:13:51,584 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:13:51,602 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:13:51,610 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:13:51,616 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:13:51,617 : INFO :  EPOCH - 3 : training on 5953723 raw words (4294343 effective words) took 7.9s, 544467 effective words/s\n",
      "2020-10-23 07:13:52,669 : INFO :  EPOCH 4 - PROGRESS: at 12.33% examples, 506402 words/s, in_qsize 10, out_qsize 1\n",
      "2020-10-23 07:13:53,689 : INFO :  EPOCH 4 - PROGRESS: at 25.17% examples, 526785 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:54,707 : INFO :  EPOCH 4 - PROGRESS: at 38.14% examples, 535816 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:55,727 : INFO :  EPOCH 4 - PROGRESS: at 51.15% examples, 538671 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:56,746 : INFO :  EPOCH 4 - PROGRESS: at 64.60% examples, 543569 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:57,756 : INFO :  EPOCH 4 - PROGRESS: at 77.26% examples, 542961 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:58,762 : INFO :  EPOCH 4 - PROGRESS: at 89.72% examples, 541155 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:13:59,462 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:13:59,474 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:13:59,478 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:13:59,481 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:13:59,499 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:13:59,501 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:13:59,501 : INFO :  EPOCH - 4 : training on 5953723 raw words (4295164 effective words) took 7.9s, 544927 effective words/s\n",
      "2020-10-23 07:14:00,523 : INFO :  EPOCH 5 - PROGRESS: at 12.33% examples, 520915 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:01,528 : INFO :  EPOCH 5 - PROGRESS: at 24.88% examples, 531704 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:02,539 : INFO :  EPOCH 5 - PROGRESS: at 37.30% examples, 533348 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:03,543 : INFO :  EPOCH 5 - PROGRESS: at 49.65% examples, 532384 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:04,542 : INFO :  EPOCH 5 - PROGRESS: at 62.20% examples, 533388 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:05,553 : INFO :  EPOCH 5 - PROGRESS: at 75.04% examples, 535746 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:06,582 : INFO :  EPOCH 5 - PROGRESS: at 88.06% examples, 536995 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:07,413 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:14:07,416 : INFO :  worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 07:14:07,432 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:14:07,441 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:14:07,443 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:14:07,450 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:14:07,451 : INFO :  EPOCH - 5 : training on 5953723 raw words (4294720 effective words) took 7.9s, 540369 effective words/s\n",
      "2020-10-23 07:14:08,472 : INFO :  EPOCH 6 - PROGRESS: at 12.30% examples, 521886 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:09,493 : INFO :  EPOCH 6 - PROGRESS: at 25.20% examples, 535090 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:10,493 : INFO :  EPOCH 6 - PROGRESS: at 37.69% examples, 537420 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:11,510 : INFO :  EPOCH 6 - PROGRESS: at 50.80% examples, 542356 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:12,541 : INFO :  EPOCH 6 - PROGRESS: at 63.68% examples, 540972 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:13,565 : INFO :  EPOCH 6 - PROGRESS: at 76.79% examples, 541912 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:14,581 : INFO :  EPOCH 6 - PROGRESS: at 89.36% examples, 540328 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:15,317 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:14:15,345 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:14:15,352 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:14:15,356 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:14:15,360 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:14:15,364 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:14:15,364 : INFO :  EPOCH - 6 : training on 5953723 raw words (4295425 effective words) took 7.9s, 542923 effective words/s\n",
      "2020-10-23 07:14:16,374 : INFO :  EPOCH 7 - PROGRESS: at 12.32% examples, 527063 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:17,389 : INFO :  EPOCH 7 - PROGRESS: at 24.88% examples, 532369 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:18,390 : INFO :  EPOCH 7 - PROGRESS: at 37.64% examples, 540237 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:19,396 : INFO :  EPOCH 7 - PROGRESS: at 50.30% examples, 540652 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:20,407 : INFO :  EPOCH 7 - PROGRESS: at 63.02% examples, 540472 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:21,455 : INFO :  EPOCH 7 - PROGRESS: at 76.44% examples, 541660 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:22,456 : INFO :  EPOCH 7 - PROGRESS: at 89.56% examples, 544299 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:23,205 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:14:23,207 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:14:23,225 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:14:23,228 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:14:23,233 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:14:23,239 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:14:23,240 : INFO :  EPOCH - 7 : training on 5953723 raw words (4295066 effective words) took 7.9s, 545508 effective words/s\n",
      "2020-10-23 07:14:24,245 : INFO :  EPOCH 8 - PROGRESS: at 12.16% examples, 523044 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:25,267 : INFO :  EPOCH 8 - PROGRESS: at 24.69% examples, 528145 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:26,282 : INFO :  EPOCH 8 - PROGRESS: at 37.29% examples, 532785 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:27,308 : INFO :  EPOCH 8 - PROGRESS: at 50.48% examples, 537607 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:28,317 : INFO :  EPOCH 8 - PROGRESS: at 63.51% examples, 541187 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:29,326 : INFO :  EPOCH 8 - PROGRESS: at 76.09% examples, 539757 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:30,352 : INFO :  EPOCH 8 - PROGRESS: at 88.80% examples, 538817 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:31,156 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:14:31,179 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:14:31,185 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:14:31,187 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:14:31,191 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:14:31,196 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:14:31,197 : INFO :  EPOCH - 8 : training on 5953723 raw words (4295296 effective words) took 8.0s, 540052 effective words/s\n",
      "2020-10-23 07:14:32,223 : INFO :  EPOCH 9 - PROGRESS: at 12.33% examples, 519672 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:33,236 : INFO :  EPOCH 9 - PROGRESS: at 25.36% examples, 539098 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:34,251 : INFO :  EPOCH 9 - PROGRESS: at 37.97% examples, 539972 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:35,264 : INFO :  EPOCH 9 - PROGRESS: at 50.98% examples, 542731 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:36,299 : INFO :  EPOCH 9 - PROGRESS: at 64.01% examples, 542438 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:37,306 : INFO :  EPOCH 9 - PROGRESS: at 76.79% examples, 542238 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:38,336 : INFO :  EPOCH 9 - PROGRESS: at 89.89% examples, 542591 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:39,048 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:14:39,076 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:14:39,080 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:14:39,080 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:14:39,087 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:14:39,093 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:14:39,093 : INFO :  EPOCH - 9 : training on 5953723 raw words (4295179 effective words) took 7.9s, 544093 effective words/s\n",
      "2020-10-23 07:14:40,125 : INFO :  EPOCH 10 - PROGRESS: at 12.48% examples, 523874 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:41,129 : INFO :  EPOCH 10 - PROGRESS: at 25.20% examples, 536880 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:42,131 : INFO :  EPOCH 10 - PROGRESS: at 37.64% examples, 538313 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:43,140 : INFO :  EPOCH 10 - PROGRESS: at 50.48% examples, 540566 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:44,150 : INFO :  EPOCH 10 - PROGRESS: at 63.51% examples, 543355 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:45,185 : INFO :  EPOCH 10 - PROGRESS: at 76.98% examples, 545095 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:46,210 : INFO :  EPOCH 10 - PROGRESS: at 89.89% examples, 544412 words/s, in_qsize 11, out_qsize 0\n",
      "2020-10-23 07:14:46,907 : INFO :  worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-23 07:14:46,929 : INFO :  worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-23 07:14:46,940 : INFO :  worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-23 07:14:46,943 : INFO :  worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-23 07:14:46,949 : INFO :  worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-23 07:14:46,951 : INFO :  worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-23 07:14:46,952 : INFO :  EPOCH - 10 : training on 5953723 raw words (4295735 effective words) took 7.9s, 546789 effective words/s\n",
      "2020-10-23 07:14:46,952 : INFO :  training on a 59537230 raw words (42949419 effective words) took 79.1s, 543078 effective words/s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# https://wikidocs.net/50739\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers = num_workers,\n",
    "                          size = num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window =  context,\n",
    "                          sample = downsampling,\n",
    "                          iter = 10,\n",
    "                          sg =0 # sg =0 CBOW, 1 : skip-gram\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 07:14:46,960 : INFO :  precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6242783665657043),\n",
       " ('lady', 0.5922756791114807),\n",
       " ('soldier', 0.5731916427612305),\n",
       " ('doctor', 0.529197633266449),\n",
       " ('guy', 0.513803243637085),\n",
       " ('boy', 0.5031958222389221),\n",
       " ('priest', 0.5020657181739807),\n",
       " ('businessman', 0.4863041043281555),\n",
       " ('politician', 0.48207157850265503),\n",
       " ('person', 0.47985774278640747)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# man과 가장 유사한 단어 골라내기 \n",
    "model.wv.most_similar(\"man\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# word2vec 은 단어 하나하나가 벡터로 표현되어 있다.\n",
    "# Review 데이터는 단어들의 조합이기에 Review를 벡터로 표현하기 위해\n",
    "# Review에 포함된 단어 벡터들의 평균값을 만든다.\n",
    "# 다른 방법으로는 Doc2vec, average of word2vec vectors with TF-IDF\n",
    "# Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector.\n",
    " # 단어 벡터에 TF-IDF를 곱해서 평균 내는 방법\n",
    "    \n",
    "# https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence\n",
    "\n",
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype = np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘 사전\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words +=1\n",
    "            #사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            \n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            # model은 단어들에 대한 vector를 다 가지고 있음\n",
    "            # num_features 만큼 이미 학습할때 정의해서 만들어놓음 \n",
    "            \n",
    "    feature_vector = np.divide(feature_vector,num_words)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    \n",
    "    for s in reviews :\n",
    "        dataset.append(get_features(s,model,num_features))\n",
    "    \n",
    "    reviewFeaturevecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeaturevecs\n",
    "\n",
    "word2vec_train = get_dataset(sentences,model,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08623709,  0.08298736,  0.1976788 , -0.46267185, -0.22685285,\n",
       "        0.28637964,  0.29247442, -0.11936648,  0.17388092, -0.2671336 ,\n",
       "       -0.11850826, -0.01741083, -0.02960839, -0.17735995, -0.0865237 ,\n",
       "       -0.15344039,  0.01326852,  0.27622244,  0.27462026, -0.15838276,\n",
       "       -0.0265446 ,  0.13750008, -0.11071409,  0.17433603, -0.02349639,\n",
       "        0.17754598,  0.07311293,  0.19679257, -0.08381309, -0.1311007 ,\n",
       "        0.0724788 , -0.02999954,  0.11642356, -0.08174769, -0.0082254 ,\n",
       "        0.03706746,  0.14136219,  0.05021876,  0.20813496,  0.1075661 ,\n",
       "       -0.20862824, -0.12613824,  0.08138913, -0.14903179,  0.11974633,\n",
       "       -0.05858384, -0.00186476, -0.01194559,  0.04782065,  0.02600222,\n",
       "       -0.10956445, -0.07753213,  0.07053878, -0.05319669,  0.03127013,\n",
       "       -0.06025062,  0.07060022, -0.04415061,  0.12495424,  0.17038663,\n",
       "       -0.03587907, -0.17412184,  0.1170035 , -0.31121588, -0.03671926,\n",
       "        0.13918217, -0.07422502,  0.18445762,  0.1150818 , -0.05063861,\n",
       "        0.0325054 ,  0.08214165, -0.28847373, -0.13639516, -0.0227275 ,\n",
       "        0.04867829, -0.24613777,  0.26546925,  0.05332164,  0.07415614,\n",
       "        0.02877375, -0.20918605, -0.01826388, -0.11192386,  0.07770182,\n",
       "        0.08060272,  0.13133013,  0.12269501, -0.2311782 , -0.28291926,\n",
       "       -0.16700985, -0.05092347,  0.04887203,  0.17812955,  0.07395507,\n",
       "        0.01255805,  0.06732454, -0.29641494, -0.0760631 ,  0.13247888,\n",
       "       -0.67423964,  0.11858469, -0.12411258,  0.3905095 , -0.41964984,\n",
       "        0.02630826, -0.14357704,  0.1322613 ,  0.06490055, -0.02207357,\n",
       "        0.21929224,  0.18413815, -0.15291499, -0.19171983, -0.0316102 ,\n",
       "        0.04074301, -0.02331392,  0.18534678,  0.15363213, -0.06339917,\n",
       "       -0.08299612,  0.1626426 ,  0.00360518, -0.11971626, -0.19898981,\n",
       "       -0.07703594, -0.02615113,  0.13042083,  0.03491133,  0.05995761,\n",
       "        0.09961284, -0.28134972, -0.13129577, -0.05420845, -0.09124035,\n",
       "       -0.2625412 ,  0.12078639, -0.0556482 , -0.19100349, -0.17205574,\n",
       "        0.13659494, -0.19282888,  0.00447235, -0.03596274,  0.11746368,\n",
       "        0.12970014, -0.09275876, -0.04245208, -0.27515444,  0.2583582 ,\n",
       "       -0.04918179,  0.18228838, -0.07463706,  0.05371695,  0.13636184,\n",
       "        0.09492862, -0.01181533,  0.09497291, -0.08394934, -0.01153478,\n",
       "       -0.1278295 ,  0.23896459,  0.11050133,  0.34254166, -0.01485824,\n",
       "       -0.12787259,  0.23825729, -0.09496082, -0.07913233, -0.12116245,\n",
       "        0.03031388, -0.12143862, -0.14478473, -0.10219383,  0.19920625,\n",
       "        0.13612707,  0.27603838,  0.1672522 , -0.07340292,  0.06009932,\n",
       "        0.136682  , -0.05939733,  0.01592431, -0.39776108, -0.39123607,\n",
       "       -0.08117466,  0.20118824,  0.07809224,  0.13855001,  0.0850934 ,\n",
       "       -0.14723408, -0.3234723 ,  0.226591  ,  0.0641342 ,  0.15040702,\n",
       "       -0.23337372, -0.22610667,  0.06950687,  0.05718916, -0.02747323,\n",
       "        0.05883062, -0.21919158, -0.04849564, -0.00098852,  0.11632611,\n",
       "       -0.1368186 ,  0.09753491, -0.19641131, -0.06936795, -0.12353325,\n",
       "        0.03546605,  0.13734163,  0.15193462,  0.12912866, -0.05826343,\n",
       "        0.0052506 , -0.21157046,  0.03129972,  0.06733929, -0.07684063,\n",
       "       -0.2260222 , -0.03339154, -0.03591666, -0.05004458,  0.05224723,\n",
       "        0.2567083 , -0.09330831, -0.12667523,  0.05285642,  0.05097831,\n",
       "        0.09298852, -0.13274913, -0.34746328,  0.2694975 ,  0.05247895,\n",
       "        0.05501852, -0.11019018,  0.03778548, -0.08889247,  0.06716818,\n",
       "        0.01687707, -0.18565448,  0.01492857, -0.05306707,  0.07723694,\n",
       "       -0.01253384,  0.05937907,  0.01803169,  0.02476687, -0.02409958,\n",
       "        0.09784666, -0.05362971,  0.06352758, -0.01144638,  0.05949693,\n",
       "       -0.01976617, -0.14606936,  0.22051392,  0.16816081, -0.07218758,\n",
       "        0.27036646, -0.13809249, -0.2045347 ,  0.11415669, -0.08947607,\n",
       "        0.04462481,  0.04467927,  0.25596008,  0.04698569, -0.06998623,\n",
       "       -0.0580841 , -0.15446673,  0.10133222,  0.18825889,  0.20182087,\n",
       "        0.19631435, -0.21412903,  0.05529428,  0.16799718, -0.22920796,\n",
       "       -0.00651389,  0.18263592, -0.2059012 ,  0.06052139,  0.091775  ,\n",
       "       -0.08828656, -0.15573834,  0.01805285,  0.06452841, -0.02262957,\n",
       "       -0.16145621,  0.16056345, -0.17905721,  0.07550638, -0.08064353,\n",
       "        0.00803603,  0.07352827, -0.12897971, -0.02527766, -0.09422984,\n",
       "        0.01329091, -0.07063981, -0.2059711 , -0.03533123, -0.3251497 ,\n",
       "       -0.05827264,  0.05294212, -0.12425201, -0.02127947,  0.0348005 ,\n",
       "        0.12690863,  0.03151915,  0.00597011, -0.04486833,  0.01468234,\n",
       "        0.01808995, -0.10741177, -0.01129285,  0.15952665, -0.00499635,\n",
       "        0.00916196,  0.14512777,  0.1601646 , -0.00149871, -0.1043959 ,\n",
       "       -0.10431883,  0.05722943,  0.09451868, -0.04920617, -0.08366408,\n",
       "       -0.10317816,  0.04569799,  0.08964321, -0.07926179,  0.15842387,\n",
       "        0.09333654,  0.0942516 ,  0.17664546,  0.16773729, -0.07366542,\n",
       "       -0.09022283, -0.10240217,  0.22556126, -0.11239281, -0.08775976,\n",
       "        0.11416148, -0.04217237,  0.09178568, -0.04260738, -0.23371851,\n",
       "        0.3786607 , -0.25331616,  0.32102388,  0.14695771, -0.06010474,\n",
       "        0.19521932, -0.02643364,  0.09570714,  0.00981286,  0.11506733,\n",
       "        0.0144936 ,  0.0940582 , -0.10385544, -0.0767836 , -0.01676755,\n",
       "       -0.12997876,  0.05000052,  0.18671678,  0.02921269, -0.09327324,\n",
       "        0.08416028, -0.10269672, -0.07040279, -0.10829744,  0.02067637,\n",
       "       -0.11953679, -0.18379267,  0.06676541,  0.30515042, -0.1928588 ,\n",
       "        0.2239172 ,  0.15910731,  0.25362825,  0.00952047, -0.01125583,\n",
       "       -0.0014339 , -0.05299989,  0.02137697, -0.28218043,  0.10032818,\n",
       "        0.07365061,  0.06749885, -0.30383176,  0.12177149, -0.07424462,\n",
       "        0.0818767 , -0.11662818,  0.3169934 , -0.06486286, -0.07145834,\n",
       "        0.38399675,  0.04378048,  0.35302418,  0.15778628, -0.24495876,\n",
       "        0.14942496, -0.04262495, -0.04776967,  0.27512226,  0.26991245,\n",
       "        0.05855549, -0.2200158 ,  0.14980896, -0.29411313,  0.24852972,\n",
       "       -0.02564205,  0.19903702,  0.08241631, -0.13257216,  0.21757588,\n",
       "        0.02700074,  0.04152222,  0.05986985,  0.16602479,  0.00800216,\n",
       "       -0.02662587, -0.04226052, -0.11039764, -0.05478546,  0.08941911,\n",
       "       -0.06191042,  0.05181781,  0.14611138,  0.05542187, -0.01550956,\n",
       "       -0.12319846,  0.03593044,  0.00336838,  0.04117575,  0.12387508,\n",
       "       -0.0487327 , -0.10134026,  0.03328237, -0.06308915, -0.04610866,\n",
       "       -0.03288184,  0.01727351,  0.08189313,  0.00991659, -0.08332091,\n",
       "       -0.2012419 , -0.08965498, -0.00577693, -0.16101982, -0.06693684,\n",
       "       -0.06288336,  0.02774787, -0.05051384, -0.02487267, -0.04575231,\n",
       "        0.0117736 , -0.07082473,  0.02789154, -0.1473498 , -0.07799685,\n",
       "       -0.00220175,  0.01166647, -0.11021381,  0.00511919,  0.06319644,\n",
       "        0.06782041, -0.09083469, -0.11031185, -0.06975407,  0.00443748,\n",
       "        0.2565617 , -0.09375642, -0.21217199, -0.08646923, -0.22000892,\n",
       "       -0.11333927, -0.05524851,  0.08098786,  0.05087947,  0.08396195,\n",
       "       -0.06026706, -0.09458615,  0.0646418 ,  0.11723921, -0.21482973,\n",
       "        0.07672296,  0.10584211, -0.05560281, -0.18697008, -0.0871099 ,\n",
       "       -0.0800782 , -0.00931447,  0.29620168,  0.1013907 , -0.1990802 ,\n",
       "        0.18057604,  0.16184726, -0.36921176, -0.20597859, -0.27379888,\n",
       "        0.3240839 ,  0.15811591, -0.02577394,  0.17333509, -0.05869562,\n",
       "        0.0091944 , -0.26458302,  0.20363167,  0.00865002,  0.07886166,\n",
       "        0.0078613 , -0.08279981,  0.05735841, -0.06842241,  0.01473784,\n",
       "        0.03633526, -0.1114125 ,  0.05711012, -0.313844  ,  0.09535292,\n",
       "        0.32115206,  0.26312432,  0.3617127 ,  0.35399693,  0.10031445,\n",
       "        0.18145597, -0.09909438, -0.03710937,  0.01177567, -0.12659669,\n",
       "       -0.15048401, -0.04894135, -0.05243617,  0.15348515, -0.01359014,\n",
       "        0.047924  ,  0.21744718, -0.00940563, -0.3081253 ,  0.00188404,\n",
       "        0.07349455,  0.07719915, -0.29238573, -0.09361331, -0.08460142,\n",
       "       -0.0281272 , -0.08558325, -0.15781733,  0.15325761,  0.19473882,\n",
       "        0.05267635, -0.01477908,  0.11945279, -0.05281521, -0.00703746,\n",
       "        0.1159275 ,  0.07019555, -0.10787417,  0.20316428, -0.04734851,\n",
       "        0.04742569, -0.12772115, -0.05435761, -0.02745849,  0.28507677,\n",
       "        0.24866812,  0.13245577,  0.3270153 ,  0.11087153, -0.09029285,\n",
       "       -0.12504238, -0.16554348, -0.33289757,  0.14239377, -0.08681017,\n",
       "       -0.21163294, -0.00547219,  0.00130009, -0.07713205, -0.07720346,\n",
       "        0.01812722,  0.23078223, -0.21208824,  0.01207349,  0.09540717,\n",
       "        0.02754889, -0.19877927, -0.0287571 , -0.01093136, -0.00851642,\n",
       "       -0.01168033, -0.09109572,  0.03861251,  0.00815766, -0.01003847,\n",
       "       -0.00831866,  0.01017451,  0.01396473, -0.02134183, -0.02471127,\n",
       "        0.10749117, -0.01369647,  0.04591516, -0.13263679, -0.10892487,\n",
       "        0.07608631, -0.13635659, -0.10587275, -0.15594766,  0.04083061,\n",
       "        0.00356329,  0.20006143,  0.04988224,  0.09623191,  0.02682562,\n",
       "        0.03108248,  0.06100238,  0.05738084,  0.03245422,  0.0837885 ,\n",
       "        0.01170997,  0.05859432, -0.01063106,  0.1049678 ,  0.05164509,\n",
       "        0.15866703, -0.10590298, -0.02271907, -0.050802  , -0.06922697,\n",
       "        0.01911167, -0.1051523 , -0.03523533, -0.14687584,  0.08983192,\n",
       "       -0.04079492, -0.02209903, -0.01175786,  0.09416191,  0.11987146,\n",
       "       -0.09149187,  0.06882475,  0.06284136,  0.02097965,  0.19112033,\n",
       "        0.01176367, -0.15221785,  0.01595219,  0.01747239,  0.10653684,\n",
       "        0.0131232 , -0.01177662, -0.02038188,  0.02976296,  0.05479661,\n",
       "        0.01050063, -0.21585673, -0.09425801, -0.05664758, -0.25559178,\n",
       "       -0.03686656,  0.06765512,  0.05979982,  0.05256042, -0.0054099 ,\n",
       "        0.11452212,  0.13206828,  0.00581597, -0.30868077,  0.22267304,\n",
       "       -0.26817966,  0.2812709 ,  0.16047485,  0.02579228, -0.00246229,\n",
       "       -0.06708506,  0.09734918,  0.13433248,  0.01241207,  0.09849533,\n",
       "       -0.14864519,  0.03235563, -0.10023621,  0.09622537,  0.07338039,\n",
       "        0.06773586, -0.08005773, -0.04814368, -0.00643665,  0.01030588,\n",
       "        0.02574923,  0.08415043, -0.00896245, -0.09148653, -0.12243476,\n",
       "        0.03636906, -0.1382505 , -0.00732631,  0.04622538, -0.04952165,\n",
       "        0.05056306,  0.08415934, -0.03185102,  0.09055086,  0.05454824,\n",
       "       -0.09179702, -0.12363017, -0.0681349 , -0.04010608,  0.08304734,\n",
       "       -0.07524106, -0.03286866,  0.11502375,  0.1446718 ,  0.00086729,\n",
       "        0.02841636, -0.09573556, -0.01312324, -0.06290048,  0.09732623,\n",
       "        0.15949503,  0.17693347,  0.07080404, -0.06083358,  0.01673755,\n",
       "       -0.03874297, -0.05958109, -0.07625878, -0.08102663, -0.21537624,\n",
       "        0.29454964, -0.10817713, -0.2870457 ,  0.09855048, -0.06384614,\n",
       "       -0.05726004,  0.2373021 , -0.10985256, -0.17832047, -0.14988816,\n",
       "        0.09881704, -0.17214492, -0.03637572, -0.02292773,  0.18490939,\n",
       "       -0.065477  ,  0.17665488, -0.21982364, -0.02786514,  0.00665762,\n",
       "       -0.02931736, -0.13890655,  0.17702764,  0.07363442,  0.01786517,\n",
       "       -0.07729235,  0.06864395,  0.21958247,  0.06389803, -0.07925232,\n",
       "       -0.1398601 , -0.08651448,  0.06903598,  0.08719382, -0.05336871,\n",
       "        0.10156456, -0.07610005,  0.06467235,  0.11092118,  0.00303973,\n",
       "       -0.1444816 , -0.0542782 ,  0.1874001 , -0.3907008 ,  0.34789577,\n",
       "       -0.16661695,  0.2290526 ,  0.07035401,  0.15848154,  0.26473007,\n",
       "       -0.03921561,  0.01067316, -0.10454842, -0.12081932, -0.02402079,\n",
       "       -0.11725134, -0.1142006 , -0.08940049, -0.03433722, -0.00521823,\n",
       "        0.07702228,  0.06353857,  0.01491127,  0.10152415,  0.12278329,\n",
       "       -0.07236578, -0.09845204, -0.01193965, -0.0171238 , -0.07012737,\n",
       "        0.15428612,  0.13184142,  0.0354441 , -0.16015498, -0.04380811,\n",
       "        0.05719582,  0.0849121 , -0.10565129,  0.07289309, -0.06572108,\n",
       "       -0.02516226, -0.17317608, -0.2615973 ,  0.12774576, -0.44428444,\n",
       "       -0.16558988, -0.09931111,  0.3211675 ,  0.05974272, -0.1309248 ,\n",
       "       -0.03589776, -0.05739779, -0.13395715, -0.12520823, -0.00156842,\n",
       "        0.12047272,  0.21146668, -0.02181401, -0.00142452, -0.1103382 ,\n",
       "        0.02800917, -0.01978339,  0.08089863, -0.01006652,  0.14187884,\n",
       "       -0.00267821,  0.05502172, -0.05427837,  0.13439915,  0.05295749,\n",
       "        0.08005802, -0.15755014,  0.09634067, -0.16158018, -0.03052664,\n",
       "       -0.03179743, -0.09912062, -0.04219475,  0.02860405,  0.10195208,\n",
       "        0.11595131,  0.10877413,  0.5072772 , -0.22784907,  0.1414254 ,\n",
       "       -0.04086646, -0.0804973 , -0.02680252, -0.38499194, -0.09294412,\n",
       "        0.19690979, -0.17993233,  0.10440965, -0.3720109 , -0.00871269,\n",
       "       -0.07999122,  0.29956117,  0.05046212, -0.15336408,  0.06982648,\n",
       "       -0.03715784,  0.10752152,  0.07877661, -0.16874392, -0.02346426,\n",
       "        0.19792068,  0.08166873, -0.01983603,  0.07336   ,  0.08066265,\n",
       "       -0.01657814, -0.0163694 , -0.02321394,  0.03198652,  0.03479271,\n",
       "       -0.0421093 ,  0.07497512,  0.0191279 , -0.09006792,  0.03698945,\n",
       "       -0.14350192, -0.00824504,  0.2684819 , -0.07708256, -0.0086699 ,\n",
       "       -0.0895825 , -0.05445696, -0.02684769, -0.09076339, -0.09772987,\n",
       "        0.00991806,  0.03795102, -0.0257716 , -0.03224561,  0.03896412,\n",
       "        0.18211459, -0.08203968, -0.04797604, -0.17114137, -0.13413097,\n",
       "       -0.05598063, -0.21176289, -0.09313905,  0.07634514, -0.02582995,\n",
       "       -0.0068901 , -0.14061622, -0.24722596,  0.03839374,  0.00197467,\n",
       "        0.07686055,  0.00972489,  0.0624345 , -0.06346645, -0.02163298,\n",
       "       -0.01648959,  0.16357496,  0.09620055,  0.02936627, -0.06702776,\n",
       "       -0.01298325, -0.05889781, -0.11539122, -0.0927414 ,  0.13818838,\n",
       "        0.11870614, -0.00366111, -0.18480007, -0.06620397,  0.02106424,\n",
       "       -0.02147379, -0.06331614, -0.03181415, -0.11168683, -0.06717176,\n",
       "       -0.11137662, -0.06875937, -0.06058919, -0.08739321, -0.0666339 ,\n",
       "       -0.03728685,  0.10682367, -0.04630019, -0.15415575,  0.07513709,\n",
       "        0.06296052, -0.14536075,  0.00271572,  0.19183083,  0.17897117,\n",
       "       -0.06956903,  0.08758635,  0.0109977 , -0.09130223,  0.09712457,\n",
       "       -0.08393981,  0.05476685,  0.14137295, -0.08732172,  0.05136954,\n",
       "        0.02477238,  0.32642516,  0.5754318 , -0.23467465,  0.12457909,\n",
       "       -0.24480355,  0.04982103, -0.01123025, -0.17152095,  0.32539397,\n",
       "       -0.36368313,  0.07064225, -0.19327243, -0.0014593 ,  0.1267516 ,\n",
       "       -0.04255982,  0.0969758 ,  0.25140688, -0.06631877,  0.29279009,\n",
       "        0.34282228,  0.05793244,  0.13534792,  0.0222409 ,  0.2813122 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.841800\n"
     ]
    }
   ],
   "source": [
    "# Count_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(count_train.toarray(),Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.847200\n"
     ]
    }
   ],
   "source": [
    "# tfidf_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(tfidf_train.toarray(),Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.829400\n"
     ]
    }
   ],
   "source": [
    "# word2vec_train\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(word2vec_train,Y_train,test_size =0.2, random_state =99)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100)\n",
    "RF.fit(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy : %f\" % RF.score(x_val,y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "tf.random.set_seed(99)\n",
    "BATCH_SIZE = 128\n",
    "epochs = 10\n",
    "valid = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts 20 minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/labeledTrainData.tsv',header =0, delimiter = '\\t', quoting =3)\n",
    "\n",
    "def preprocessing(review, remove_stopwords = False):\n",
    "    review_text = BeautifulSoup(review,'html.parser').get_text()\n",
    "    \n",
    "    # 특수문자 제거 # 영문자,숫자를 제외한 문자를 모드 변환 띄어쓰기로\n",
    "    review_text = re.sub(\"\\W\",\" \",review_text)    \n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        clean_review = ' '.join(words)\n",
    "        \n",
    "    else :\n",
    "        clean_review = ' '.join(words)\n",
    "    \n",
    "    return clean_review\n",
    "\n",
    "clean_train_reviews = []\n",
    "\n",
    "for review in train_data['review']:\n",
    "    clean_train_reviews.append(preprocessing(review, remove_stopwords=True))\n",
    "\n",
    "clean_train_reviews[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[410, 71, 425, 8957, 512, 2483, 116, 54, 882, 522, 179, 18946, 179, 11409, 167, 79, 14, 669, 2484, 118, 93, 10, 505, 4131, 167, 22, 213, 589, 2361, 1205, 11409, 72, 4896, 72, 641, 2, 257, 71, 11, 306, 1683, 492, 1157, 3309, 8957, 417, 800, 3387, 17, 447, 607, 1516, 15, 4528, 1875, 1010, 148, 348, 1455, 750, 2452, 4, 8957, 424, 71, 643, 70, 241, 95, 547, 8957, 26374, 26375, 121, 1, 8957, 327, 8, 47, 20, 327, 169, 10, 210, 638, 641, 2, 117, 295, 388, 733, 124, 15761, 3361, 1517, 582, 741, 10167, 934, 11746, 829, 1251, 1423, 366, 8957, 225, 15, 584, 8957, 22505, 2300, 13630, 741, 10167, 27, 28950, 346, 16, 41, 18947, 1516, 394, 11410, 167, 4018, 8957, 116, 633, 505, 80, 4, 8957, 1444, 386, 2193, 115, 1943, 2529, 582, 17, 60, 101, 4947, 5239, 264, 1280, 26376, 15, 582, 498, 751, 643, 637, 3, 400, 166, 452, 115, 622, 3310, 1172, 690, 48, 1190, 228, 1, 16, 4, 8957, 3, 513, 62, 25, 16, 646, 135, 235, 96, 7553, 607, 3486, 8957, 37737, 1888, 1, 130, 348, 1455, 251, 3, 874, 16, 42, 1502, 1009, 2361, 12, 555, 392, 723, 7037, 12, 41, 16, 160, 368, 4456, 3432, 41, 88, 229, 444, 210, 258, 118, 3, 18948, 18949, 320, 1372]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_reviews)\n",
    "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
    "print(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = tokenizer.word_index\n",
    "word_vocab[\"<PAD>\"]=0 \n",
    "# print(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75684"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_configs = {}\n",
    "data_configs['vocab']= word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab)+1\n",
    "data_configs['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 174)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 174\n",
    "\n",
    "train_inputs =  pad_sequences(text_sequences,maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "print(train_inputs.shape)\n",
    "train_labels = np.array(train_data['sentiment'])\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(text_sequences[2]).shape\n",
    "# text_sequences[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75684"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_configs['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  800,  3387,    17, ..., 18949,   320,  1372],\n",
       "       [  236,   206,  3085, ...,     0,     0,     0],\n",
       "       [37741,  3511,  1410, ...,   708,  1190,  5398],\n",
       "       ...,\n",
       "       [  118,  3143,    14, ...,     0,     0,     0],\n",
       "       [  832,   645,   522, ...,     0,     0,     0],\n",
       "       [  110,     1,   354, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tensorflow 2.0 모델 구축 방법\n",
    "1. Sequential API : tf.keras.Sequential, model.add()\n",
    "2. Functional API : Input - layers \n",
    "3. Custom layer : layers. layer 상속 : 여러 레이어를 하나로 묶은 레이어 구현 용이\n",
    "4. Subclassing : tf.keras.Model  : 자유도가 가장 높아서 자주 사용. \n",
    "\n",
    "'''\n",
    "\n",
    "# tf.keras.Model을 학습받아 클래스로 구현 \n",
    "# Input은 \n",
    "# Word2 vec!!!!\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__() #  부모 클래스에 있는__init__ 함수 호출 \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim= data_configs['vocab_size'], # 이전에 임베딩된 5000개 Feature 를 넣겠다\n",
    "                                          output_dim= 300 # Dense 임베딩 결과shape                                         \n",
    "                                         )\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(150, #  lstm_shape : units = dimensionality of the output shape 의미\n",
    "                                          return_sequences = True)\n",
    "        self.lstm_2 = tf.keras.layers.LSTM(lstm_shape)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc1 = tf.keras.layers.Dense(units = 150, activation = tf.keras.activations.tanh)\n",
    "        self.fc2 = tf.keras.layers.Dense(units = 1, activation = tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm_1(x)\n",
    "        x = self.lstm_2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "# input_shape = (batch_size,tfidf_train.toarray().shape[1])\n",
    "input_shape = (batch_size, train_inputs.shape[1])\n",
    "emb_shape =500\n",
    "lstm_shape = 150\n",
    "\n",
    "model = RNN()\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(1e-4),\n",
    "             loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics= [tf.keras.metrics.BinaryAccuracy(name = \"accuracy\")]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_out/rnn_classifier - exists\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# earlystopping \n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy', min_delta=0.0001, patience=2)\n",
    "model_name = 'rnn_classifier'\n",
    "checkpoint_path = './data_out/{}/weight.h5'.format(model_name)\n",
    "checkpoint_dir =  os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} - exists\".format(checkpoint_dir))\n",
    "else :\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} - complete\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback= ModelCheckpoint(\n",
    "    checkpoint_path, monitor ='val_accuracy', verbose =1, save_best_only =True, save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5082\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50560, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 31s 2ms/sample - loss: 0.6931 - accuracy: 0.5086 - val_loss: 0.6924 - val_accuracy: 0.5056\n",
      "Epoch 2/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.5357\n",
      "Epoch 00002: val_accuracy improved from 0.50560 to 0.70720, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.6835 - accuracy: 0.5364 - val_loss: 0.6191 - val_accuracy: 0.7072\n",
      "Epoch 3/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.3262 - accuracy: 0.8681\n",
      "Epoch 00003: val_accuracy improved from 0.70720 to 0.88480, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.3260 - accuracy: 0.8684 - val_loss: 0.2857 - val_accuracy: 0.8848\n",
      "Epoch 4/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.1656 - accuracy: 0.9432\n",
      "Epoch 00004: val_accuracy improved from 0.88480 to 0.88600, saving model to ./data_out/rnn_classifier/weight.h5\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.1654 - accuracy: 0.9431 - val_loss: 0.2879 - val_accuracy: 0.8860\n",
      "Epoch 5/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.0950 - accuracy: 0.9709\n",
      "Epoch 00005: val_accuracy did not improve from 0.88600\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.0949 - accuracy: 0.9709 - val_loss: 0.3286 - val_accuracy: 0.8782\n",
      "Epoch 6/10\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9830\n",
      "Epoch 00006: val_accuracy did not improve from 0.88600\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.0620 - accuracy: 0.9830 - val_loss: 0.4345 - val_accuracy: 0.8726\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_inputs, train_labels, batch_size = batch_size, epochs =  epochs,\n",
    "                    validation_split = 0.2 , callbacks = [earlystop, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_graphs' from 'tensorflow.keras.utils' (C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\keras\\api\\_v2\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-f09b396cd587>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_graphs' from 'tensorflow.keras.utils' (C:\\Users\\yseon\\Anaconda3\\envs\\popcorn\\lib\\site-packages\\tensorflow_core\\python\\keras\\api\\_v2\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_graphs\n",
    "plot_graphs(history,'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 37s 1ms/sample - loss: 0.6927 - accuracy: 0.5119\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.5268 - accuracy: 0.6941\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.2215 - accuracy: 0.9182\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.1368 - accuracy: 0.9539\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.0843 - accuracy: 0.9749\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.0552 - accuracy: 0.9850\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.0441 - accuracy: 0.9886\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.0340 - accuracy: 0.9920\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.0296 - accuracy: 0.9930\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.0245 - accuracy: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x262f6330e48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popcorn",
   "language": "python",
   "name": "popcorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
