{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "#   try:\n",
    "#     tf.config.experimental.set_virtual_device_configuration(\n",
    "#         gpus[0],\n",
    "#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Virtual devices must be set before GPUs have been initialized\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n데이터 Q,A, Label\\nLabel = 0 : 일상 대화\\nLabel = 1 : 긍정\\nLabel = 2 : 부정 주제\\nLabel은 감정 분석에 활용할 수 있겠음.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "데이터 Q,A, Label\n",
    "Label = 0 : 일상 대화\n",
    "Label = 1 : 긍정\n",
    "Label = 2 : 부정 주제\n",
    "Label은 감정 분석에 활용할 수 있겠음.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n인코더 Input : 최대 길이만큼 <PAD>\\n디코더 Input : 시작을 알리는 <SOS>\\n디코더 타겟 : 끝을 알리는 <END>\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''전처리 결과'''\n",
    "seed = 99\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# 인코더의 입력값\n",
    "index_inputs = np.load(open('data_in/train_inputs.npy','rb'), allow_pickle=True)\n",
    "# 디코더의 입력값\n",
    "index_outputs = np.load(open('data_in/train_outputs.npy','rb'), allow_pickle=True)\n",
    "# 디코더의 타깃값\n",
    "index_targets = np.load(open('data_in/train_targets.npy','rb'), allow_pickle=True)\n",
    "# dictonary\n",
    "prepro_configs = json.load(open('data_in/data_configs.json'))\n",
    "\n",
    "'''\n",
    "인코더 Input : 최대 길이만큼 <PAD>\n",
    "디코더 Input : 시작을 알리는 <SOS>\n",
    "디코더 타겟 : 끝을 알리는 <END>\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2  # set을 키워보자 -> NoneType 에러가 발생한다. - 메모리이슈\n",
    "MAX_SEQUENCE =25\n",
    "EPOCH =30\n",
    "UNITS =1024\n",
    "EMBEDDING_DIM = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "char2idx = prepro_configs['char2idx']\n",
    "idx2char = prepro_configs['idx2char']\n",
    "std_index = prepro_configs['std_symbol']\n",
    "end_index = prepro_configs['end_symbol']\n",
    "vocab_size = prepro_configs['vocab_size']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Encoder '''\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                         return_sequences= True,\n",
    "                                         return_state= True,\n",
    "                                         # Xavier 초기화 = Glorot 초기화 방법\n",
    "                                         # 이전 노드와 다음 노드의 개수에 의존하여 초기화 하는 방법\n",
    "                                         recurrent_initializer= 'glorot_uniform'\n",
    "                                        )\n",
    "    def call(self,x,hidden): # 입력값 X와 은닉 상태 Hidden을 받는다.\n",
    "        x = self.embedding(x)\n",
    "        output,state = self.gru(x, initial_state = hidden)\n",
    "\n",
    "        return output, state\n",
    "\n",
    "    #초기에 사용될 Hidden state를 만듦\n",
    "    def initialize_hidden_state(self, inp):\n",
    "        return tf.zeros((tf.shape(inp)[0],self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BandanauAttention : Attention 가중치도 같이 학습 시키는 것\n",
    "class BandanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units): # 출력 벡터의 크기를 인자로 받음 \n",
    "        super(BandanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, query, values): # 인코더 Hidden(query) + encoder output(values) -- 기존 Context + Hidden\n",
    "        # query와 w2를 행렬곱 할 수 있도록 shape을 바꿈\n",
    "        hidden_with_time_axis =  tf.expand_dims(query,1)\n",
    "        # W1,W2의 결과를 더하여 activation function을 취함\n",
    "        # Query와 value 에 가중치를 곱함\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                                self.W1(values)+self.W2(hidden_with_time_axis)\n",
    "                ))\n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis =1) # 행단위로 Sum 하는 것\n",
    "        \n",
    "        return context_vector, attention_weights        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Decoder '''\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units =  dec_units\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences = True,\n",
    "                                        return_state = True,\n",
    "                                        recurrent_initializer = 'glorot_uniform'\n",
    "                                       )\n",
    "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self.attention = BandanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # 디코더의 입력값 x, 인코더의 은닉 상태값 hidden, 인코더의 결과값 enc_output\n",
    "        # 인코딩이 Query, attention이 key, 인코더 결과가 values?\n",
    "        \n",
    "        context_vector,attention_weights = self.attention(hidden, enc_output) \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x], axis =-1)  #axis -1\n",
    "        # concat 한 결과를 LSTM 하는 것\n",
    "        output,state = self.gru(x)\n",
    "#         print(output.shape)\n",
    "#         output = tf.concat([tf.expand_dims(context_vector,1),output], axis =2) \n",
    "#         print(output.shape)\n",
    "        output = tf.reshape(output, (-1,output.shape[2]))\n",
    "#         print(output.shape)\n",
    "        x = self.fc(output)\n",
    "#         print(1234, x)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#크로스 엔트로피 손실값 측정\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction= 'none')\n",
    "#정확도 측정 객체\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')\n",
    "\n",
    "def loss(real, pred):# real 값 중에서 0 인 <PAD> 값 제거하기 위한 함수    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0)) # True 1 , <PAD> 제외한 나머지는 0 \n",
    "    loss_ = loss_object(real,pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask # 요소간의 곱을 하면 <PAD>는 loss 계산에서 제외됨. True만 남고 나머지는 다 0으로 바뀌네\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype = pred.dtype), axis = -1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Main Class : encoding+decoding'''\n",
    "class seq2seq(tf.keras.Model):\n",
    "    def __init__(self,vocab_size, embedding_dim, enc_units, dec_units, batch_size, end_token_idx = 2):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_size)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_size)\n",
    "        \n",
    "    def call(self,x): # x는 인코더, 디코더 입력값을 포함 함\n",
    "#         print(x[1])\n",
    "        inp, tar = x\n",
    "#         print(inp, tar, x)\n",
    "        \n",
    "        # Encoder의 Hidden vector를 초기화 하여 encoding\n",
    "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
    "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        # 반복적으로 state 별로 attention 결과를 받아와서 Decoding\n",
    "        predict_tokens  = list()\n",
    "        for t in range(0, tar.shape[1]):\n",
    "#             print(t, tar.shape)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:,t],1),tf.float32) #특정 state 디코더 입력값\n",
    "            \n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "#             print(predictions)\n",
    "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
    "#             print(predict_tokens)\n",
    "#         print(predict_tokens)\n",
    "        result = tf.stack(predict_tokens, axis = 1)\n",
    "#         print(1111)\n",
    "#         print(np.array(result))\n",
    "#         print(222)\n",
    "        return result\n",
    "        \n",
    "    def inference(self, x): #모델의 결과값을 확인하기 위함, Test 목적\n",
    "        inp = x\n",
    "#         print(111)\n",
    "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
    "        enc_output,enc_hidden = self.encoder(inp,enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        dec_input = tf.expand_dims([char2idx[std_index]],1)  #end \n",
    "        \n",
    "        predict_tokens = list()\n",
    "        for t in range(0, MAX_SEQUENCE):\n",
    "            predictions,dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            predict_token = tf.argmax(predictions[0])\n",
    "            \n",
    "            if predict_token == self.end_token_idx : # 끝을 만나면 종료\n",
    "                break\n",
    "            predict_tokens.append(predict_token)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token],0),tf.float32)\n",
    "        \n",
    "        return tf.stack(predict_tokens, axis =0).numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model 생성'''\n",
    "model = seq2seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS,BATCH_SIZE, char2idx[end_index])\n",
    "model.compile(loss = loss, optimizer= tf.keras.optimizers.Adam(1e-3), metrics =  [accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9458 samples, validate on 2365 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "path = 'data_out/seq2seq_ban'\n",
    "if not(os.path.isdir(path)):\n",
    "    os.makedirs(os.path.join(path))\n",
    "    \n",
    "chk_path = path + '/weights.h5'\n",
    "callback = ModelCheckpoint( chk_path, monitor = 'val_accuracy', verbose =1, save_best_only= True,\n",
    "                            save_weights_only =True)\n",
    "earlystop = EarlyStopping(monitor ='val_accuracy', min_delta = 0.001, patience =10)\n",
    "\n",
    "history = model.fit([index_inputs, index_outputs], index_targets,\n",
    "                   batch_size =BATCH_SIZE,\n",
    "                   epochs = EPOCH,\n",
    "                   validation_split= 0.2, # set이 너무 작아서 valloss 계산이 안되는 거일수도 있다.\n",
    "                   callbacks = [earlystop, callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string],'')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string,'val_'+string])\n",
    "    plt.show()\n",
    "plot_graphs(history,'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history,'loss')\n",
    "# 뭔가 잘못 됐다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FILE_NM = \"weights.h5\"\n",
    "model.load_weights(os.path.join('data_out/seq2seq_ban/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"남자친구 승진 선물로 뭐가 좋을까\"\n",
    "\n",
    "test_index_inputs , _ = enc_processing([query],char2idx)\n",
    "predict_tokens =  model.inference(test_index_inputs)\n",
    "print(' '.join([idx2word[t] for  t in predict_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popcorn",
   "language": "python",
   "name": "popcorn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
