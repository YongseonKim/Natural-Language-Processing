{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n인코더 Input : 최대 길이만큼 <PAD>\\n디코더 Input : 시작을 알리는 <SOS>\\n디코더 타겟 : 끝을 알리는 <END>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''전처리 결과'''\n",
    "seed = 99\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# 인코더의 입력값\n",
    "index_inputs = np.load(open('data_in/train_inputs.npy','rb'), allow_pickle=True)\n",
    "# 디코더의 입력값\n",
    "index_outputs = np.load(open('data_in/train_outputs.npy','rb'), allow_pickle=True)\n",
    "# 디코더의 타깃값\n",
    "index_targets = np.load(open('data_in/train_targets.npy','rb'), allow_pickle=True)\n",
    "# dictonary\n",
    "prepro_configs = json.load(open('data_in/data_configs.json'))\n",
    "\n",
    "'''\n",
    "인코더 Input : 최대 길이만큼 <PAD>\n",
    "디코더 Input : 시작을 알리는 <SOS>\n",
    "디코더 타겟 : 끝을 알리는 <END>\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2  # set을 키워보자 -> NoneType 에러가 발생한다. - 메모리이슈\n",
    "MAX_SEQUENCE =25\n",
    "EPOCH =30\n",
    "UNITS =1024\n",
    "EMBEDDING_DIM = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "char2idx = prepro_configs['char2idx']\n",
    "idx2char = prepro_configs['idx2char']\n",
    "std_index = prepro_configs['std_symbol']\n",
    "end_index = prepro_configs['end_symbol']\n",
    "vocab_size = prepro_configs['vocab_size']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "스케일 내적 어텐션\n",
    "softmax(Q,k/크기)*value\n",
    "Scaling을 해주는 이유는 query, value를 이용해 내적한 값이 벡터 차원이 커지면\n",
    "학습이 잘 안될 수도 있기 때문에 벡터 크기에 따라 값이 반비례하도록 크기를 조정함\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask= 1 - tf.linalg.band_part(tf.ones((size,size)),-1,0)\n",
    "    return mask\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    matmul_qk = tf.matmul(q,k,transpose_b = True)\n",
    "    dk = tf.cast(tf.shape(k)[-1],tf.float32) # Type을 변환함\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "#         print(scaled_attention_logits)\n",
    "#         print(mask)\n",
    "        # scaled_attention_logits 이 상삼각행렬이 0인가?\n",
    "        scaled_attention_logits += (mask * -1e9) # 마스킹 대상에  모두 작은 음수값을 넣는 것  매우 작아지는구나\n",
    "#         print(scaled_attention_logits)\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)\n",
    "    output = tf.matmul(attention_weights,v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       " array([[1.      , 2.      , 3.      ],\n",
       "        [2.999998, 3.999998, 4.999998],\n",
       "        [5.      , 6.      , 7.      ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       " array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "        [9.5992834e-07, 9.9999905e-01, 0.0000000e+00],\n",
       "        [8.8453794e-19, 9.4049879e-10, 1.0000000e+00]], dtype=float32)>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "x=[[1.,2.,3.],[3.,4.,5],[5.,6.,7.]]\n",
    "mask = create_look_ahead_mask(3)\n",
    "scaled_dot_product_attention(x,x,x,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "멀티 헤드 어텐션 \n",
    "'''\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,**kargs):\n",
    "        super(MultiHeadAttention,sefl).__init__()\n",
    "        self.num_heads = kargs['num_heads']\n",
    "        self.d_model = kargs['d_model']\n",
    "        \n",
    "        # d_model : Q,K,V 차원을 결정하는 Parameter\n",
    "        # num_heads : 어텐션 head 수를 결정하는 parameter\n",
    "        # assert는 아래 조건에 해당하지 않으면 에러를 발생시킴\n",
    "        assert self.d_model % self.num_heads ==0 # 나머지가 없어야한다.\n",
    "        \n",
    "        self.depth = self.d_model // self.num_heads #각 Head에 입력될 벡터 차원 수\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        \n",
    "    def split_heads(self, x, batch_size): # 학습 중에 배치 크기가 바뀔 수 있음.\n",
    "        # (batch, sequence, feature) → (batch, head, sequence, feature)\n",
    "        x = tf.reshape(x, (batch_size,-1,self.num_heads,self.depth))\n",
    "        return tf.transpose(x, perm = [0,2,1,3]) # Sequence, head 차원을 바꿈\n",
    "    \n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q= self.wq(q)\n",
    "        k= self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q,batch_size)\n",
    "        k = self.split_heads(k,batch_size)\n",
    "        v = self.split_heads(v,batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q,k,v,mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0,2,1,3]) # (batch, seq, feature) 차원\n",
    "        # feature \n",
    "        concat_attention = tf.reshape(scaled_attention,(batch_size,-1, self.d_model))\n",
    "        \n",
    "        output = self.dense(concat_attention) # 멀티헤드 어텐션 벡터 \n",
    "        \n",
    "        return output, attention_weights\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Position-wise FFNN\n",
    "'''\n",
    "def point_wise_feed_forward_network(d_model,dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff,activation = 'relu'),\n",
    "        tf.keras.layers.Dense(d_model)        \n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Positional Encoding\n",
    "'''\n",
    "\n",
    "def get_angles(pos,i,d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2*i) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    # 포지션과 차원별로 각기 다른 값을 순차적으로 할당\n",
    "    angle_rads = get_angles(np.arange(position)[:,np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis,:],\n",
    "                            d_model\n",
    "                           )\n",
    "    \n",
    "    # 짝수차원에는 사인 함수, 홀수 차원에는 코사인 함수를 적용 \n",
    "    angle_rads[:,0::2] = np.sin(angle_rads[:,0::2])\n",
    "    angle_rads[:,1::2] = np.cos(angle_rads[:,1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype= tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "인코더\n",
    "'''\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(**kargs)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        attn_output, _ = self.mha(x,x,x,mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # x를 그대로 더해줌 Residual Connection\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output) \n",
    "        \n",
    "        return out2\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(kargs['input_vocab_size'], self.d_model)\n",
    "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], self.d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(**kargs) for _ in range(self.num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self,x,mask=None):\n",
    "        seq_len = tf.shape(x)[1] # 포지션 임베딩을 위함\n",
    "        # word embedding은  입력 길이가 가변적이고 포지션 임베딩인 고정이기 때문\n",
    "        \n",
    "        x= self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model,tf.float32)) # 임베딩에 대한 스케일을 맞추는 것\n",
    "        # 임베딩 차원의 제곱근 만큼 가중치 곱함\n",
    "        x += self.pos_encoding[:,:seq_len,:]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x,mask)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decoder\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
